[4;33mReloaded modules[24m: Network.lstm_network, study_folder, prepare_data02, params[0m
96
64
./Results\LSTMStudy_20250119_163659
./Results\LSTMStudy_20250119_163659\Train
Config file saved to: ./Results\LSTMStudy_20250119_163659\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([35387, 96, 11])
Train labels tensor shape: torch.Size([35387, 1])
Val data tensor shape: torch.Size([3705, 96, 11])
Val labels tensor shape: torch.Size([3705, 1])
Test data tensor shape: torch.Size([11310, 96, 11])
Test labels tensor shape: torch.Size([11310, 1])
Epoch [1/200], Loss: 0.0808
Epoch [2/200], Loss: 0.0673
Epoch [3/200], Loss: 0.0663
Epoch [4/200], Loss: 0.0652
Epoch [5/200], Loss: 0.0642
-- Evaluation after Epoch [5/200], Test Loss: 0.1027
New best test loss: 0.1027. Saving model.
Epoch [6/200], Loss: 0.0633
Epoch [7/200], Loss: 0.0620
Epoch [8/200], Loss: 0.0609
Epoch [9/200], Loss: 0.0601
Epoch [10/200], Loss: 0.0599
-- Evaluation after Epoch [10/200], Test Loss: 0.0985
New best test loss: 0.0985. Saving model.
Epoch [11/200], Loss: 0.0592
Epoch [12/200], Loss: 0.0586
Epoch [13/200], Loss: 0.0581
Epoch [14/200], Loss: 0.0580
Epoch [15/200], Loss: 0.0578
-- Evaluation after Epoch [15/200], Test Loss: 0.0964
New best test loss: 0.0964. Saving model.
Epoch [16/200], Loss: 0.0572
Epoch [17/200], Loss: 0.0568
Epoch [18/200], Loss: 0.0564
Epoch [19/200], Loss: 0.0564
Epoch [20/200], Loss: 0.0563
-- Evaluation after Epoch [20/200], Test Loss: 0.0967
Epoch [21/200], Loss: 0.0559
Epoch [22/200], Loss: 0.0566
Epoch [23/200], Loss: 0.0554
Epoch [24/200], Loss: 0.0555
Epoch [25/200], Loss: 0.0553
-- Evaluation after Epoch [25/200], Test Loss: 0.0861
New best test loss: 0.0861. Saving model.
Epoch [26/200], Loss: 0.0546
Epoch [27/200], Loss: 0.0541
Epoch [28/200], Loss: 0.0541
Epoch [29/200], Loss: 0.0532
Epoch [30/200], Loss: 0.0526
-- Evaluation after Epoch [30/200], Test Loss: 0.0785
New best test loss: 0.0785. Saving model.
Epoch [31/200], Loss: 0.0529
Epoch [32/200], Loss: 0.0527
Epoch [33/200], Loss: 0.0519
Epoch [34/200], Loss: 0.0531
Epoch [35/200], Loss: 0.0529
-- Evaluation after Epoch [35/200], Test Loss: 0.0785
Epoch [36/200], Loss: 0.0524
Epoch [37/200], Loss: 0.0512
Epoch [38/200], Loss: 0.0515
Epoch [39/200], Loss: 0.0503
Epoch [40/200], Loss: 0.0493
-- Evaluation after Epoch [40/200], Test Loss: 0.0814
Epoch [41/200], Loss: 0.0498
Epoch [42/200], Loss: 0.0495
Epoch [43/200], Loss: 0.0501
Epoch [44/200], Loss: 0.0502
Epoch [45/200], Loss: 0.0497
-- Evaluation after Epoch [45/200], Test Loss: 0.0783
New best test loss: 0.0783. Saving model.
Epoch [46/200], Loss: 0.0478
Epoch [47/200], Loss: 0.0487
Epoch [48/200], Loss: 0.0472
Epoch [49/200], Loss: 0.0464
Epoch [50/200], Loss: 0.0463
-- Evaluation after Epoch [50/200], Test Loss: 0.0785
Epoch [51/200], Loss: 0.0452
Epoch [52/200], Loss: 0.0477
Epoch [53/200], Loss: 0.0466
Epoch [54/200], Loss: 0.0463
Epoch [55/200], Loss: 0.0458
-- Evaluation after Epoch [55/200], Test Loss: 0.0883
Epoch [56/200], Loss: 0.0446
Epoch [57/200], Loss: 0.0444
Epoch [58/200], Loss: 0.0451
Epoch [59/200], Loss: 0.0434
Epoch [60/200], Loss: 0.0425
-- Evaluation after Epoch [60/200], Test Loss: 0.0875
Epoch [61/200], Loss: 0.0422
Epoch [62/200], Loss: 0.0425
Epoch [63/200], Loss: 0.0424
Epoch [64/200], Loss: 0.0420
Epoch [65/200], Loss: 0.0429
-- Evaluation after Epoch [65/200], Test Loss: 0.0908
Epoch [66/200], Loss: 0.0426
Epoch [67/200], Loss: 0.0421
Epoch [68/200], Loss: 0.0408
Epoch [69/200], Loss: 0.0402
Epoch [70/200], Loss: 0.0402
-- Evaluation after Epoch [70/200], Test Loss: 0.0821
Epoch [71/200], Loss: 0.0395
Epoch [72/200], Loss: 0.0398
Epoch [73/200], Loss: 0.0397
Epoch [74/200], Loss: 0.0408
Epoch [75/200], Loss: 0.0389
-- Evaluation after Epoch [75/200], Test Loss: 0.0858
Epoch [76/200], Loss: 0.0392
Epoch [77/200], Loss: 0.0387
Epoch [78/200], Loss: 0.0383
Epoch [79/200], Loss: 0.0383
Epoch [80/200], Loss: 0.0380
-- Evaluation after Epoch [80/200], Test Loss: 0.0838
Epoch [81/200], Loss: 0.0377
Epoch [82/200], Loss: 0.0373
Epoch [83/200], Loss: 0.0372
Epoch [84/200], Loss: 0.0370
Epoch [85/200], Loss: 0.0385
-- Evaluation after Epoch [85/200], Test Loss: 0.0741
New best test loss: 0.0741. Saving model.
Epoch [86/200], Loss: 0.0378
Epoch [87/200], Loss: 0.0369
Epoch [88/200], Loss: 0.0372
Epoch [89/200], Loss: 0.0367
Epoch [90/200], Loss: 0.0373
-- Evaluation after Epoch [90/200], Test Loss: 0.0754
Epoch [91/200], Loss: 0.0381
Epoch [92/200], Loss: 0.0371
Epoch [93/200], Loss: 0.0392
Epoch [94/200], Loss: 0.0370
Epoch [95/200], Loss: 0.0367
-- Evaluation after Epoch [95/200], Test Loss: 0.0645
New best test loss: 0.0645. Saving model.
Epoch [96/200], Loss: 0.0363
Epoch [97/200], Loss: 0.0359
Epoch [98/200], Loss: 0.0368
Epoch [99/200], Loss: 0.0362
Epoch [100/200], Loss: 0.0360
-- Evaluation after Epoch [100/200], Test Loss: 0.0625
New best test loss: 0.0625. Saving model.
Epoch [101/200], Loss: 0.0353
Epoch [102/200], Loss: 0.0361
Epoch [103/200], Loss: 0.0349
Epoch [104/200], Loss: 0.0376
Epoch [105/200], Loss: 0.0378
-- Evaluation after Epoch [105/200], Test Loss: 0.0667
Epoch [106/200], Loss: 0.0369
Epoch [107/200], Loss: 0.0357
Epoch [108/200], Loss: 0.0365
Epoch [109/200], Loss: 0.0362
Epoch [110/200], Loss: 0.0352
-- Evaluation after Epoch [110/200], Test Loss: 0.0658
Epoch [111/200], Loss: 0.0357
Epoch [112/200], Loss: 0.0371
Epoch [113/200], Loss: 0.0364
Epoch [114/200], Loss: 0.0350
Epoch [115/200], Loss: 0.0353
-- Evaluation after Epoch [115/200], Test Loss: 0.0642
Epoch [116/200], Loss: 0.0353
Epoch [117/200], Loss: 0.0337
Epoch [118/200], Loss: 0.0347
Epoch [119/200], Loss: 0.0344
Epoch [120/200], Loss: 0.0347
-- Evaluation after Epoch [120/200], Test Loss: 0.0686
Epoch [121/200], Loss: 0.0344
Epoch [122/200], Loss: 0.0349
Epoch [123/200], Loss: 0.0345
Epoch [124/200], Loss: 0.0344
Epoch [125/200], Loss: 0.0335
-- Evaluation after Epoch [125/200], Test Loss: 0.0621
New best test loss: 0.0621. Saving model.
Epoch [126/200], Loss: 0.0344
Epoch [127/200], Loss: 0.0339
Epoch [128/200], Loss: 0.0340
Epoch [129/200], Loss: 0.0340
Epoch [130/200], Loss: 0.0331
-- Evaluation after Epoch [130/200], Test Loss: 0.0818
Epoch [131/200], Loss: 0.0342
Epoch [132/200], Loss: 0.0359
Epoch [133/200], Loss: 0.0354
Epoch [134/200], Loss: 0.0348
Epoch [135/200], Loss: 0.0345
-- Evaluation after Epoch [135/200], Test Loss: 0.0738
Epoch [136/200], Loss: 0.0345
Epoch [137/200], Loss: 0.0340
Epoch [138/200], Loss: 0.0338
Epoch [139/200], Loss: 0.0337
Epoch [140/200], Loss: 0.0327
-- Evaluation after Epoch [140/200], Test Loss: 0.0695
Epoch [141/200], Loss: 0.0331
Epoch [142/200], Loss: 0.0344
Epoch [143/200], Loss: 0.0348
Epoch [144/200], Loss: 0.0352
Epoch [145/200], Loss: 0.0340
-- Evaluation after Epoch [145/200], Test Loss: 0.0854
Epoch [146/200], Loss: 0.0331
Epoch [147/200], Loss: 0.0317
Epoch [148/200], Loss: 0.0323
Epoch [149/200], Loss: 0.0315
Epoch [150/200], Loss: 0.0308
-- Evaluation after Epoch [150/200], Test Loss: 0.0867
Epoch [151/200], Loss: 0.0306
Epoch [152/200], Loss: 0.0312
Epoch [153/200], Loss: 0.0315
Epoch [154/200], Loss: 0.0318
Epoch [155/200], Loss: 0.0317
-- Evaluation after Epoch [155/200], Test Loss: 0.0816
Epoch [156/200], Loss: 0.0313
Epoch [157/200], Loss: 0.0312
Epoch [158/200], Loss: 0.0319
Epoch [159/200], Loss: 0.0313
Epoch [160/200], Loss: 0.0326
-- Evaluation after Epoch [160/200], Test Loss: 0.0719
Epoch [161/200], Loss: 0.0332
Epoch [162/200], Loss: 0.0314
Epoch [163/200], Loss: 0.0303
Epoch [164/200], Loss: 0.0302
Epoch [165/200], Loss: 0.0313
-- Evaluation after Epoch [165/200], Test Loss: 0.0688
Epoch [166/200], Loss: 0.0301
Epoch [167/200], Loss: 0.0302
Epoch [168/200], Loss: 0.0304
Epoch [169/200], Loss: 0.0309
Epoch [170/200], Loss: 0.0325
-- Evaluation after Epoch [170/200], Test Loss: 0.0677
Epoch [171/200], Loss: 0.0326
Epoch [172/200], Loss: 0.0325
Epoch [173/200], Loss: 0.0349
Epoch [174/200], Loss: 0.0346
Epoch [175/200], Loss: 0.0330
-- Evaluation after Epoch [175/200], Test Loss: 0.0688
Epoch [176/200], Loss: 0.0319
Epoch [177/200], Loss: 0.0306
Epoch [178/200], Loss: 0.0294
Epoch [179/200], Loss: 0.0287
Epoch [180/200], Loss: 0.0297
-- Evaluation after Epoch [180/200], Test Loss: 0.0683
Epoch [181/200], Loss: 0.0290
Epoch [182/200], Loss: 0.0285
Epoch [183/200], Loss: 0.0286
Epoch [184/200], Loss: 0.0297
Epoch [185/200], Loss: 0.0288
-- Evaluation after Epoch [185/200], Test Loss: 0.0676
Epoch [186/200], Loss: 0.0286
Epoch [187/200], Loss: 0.0289
Epoch [188/200], Loss: 0.0296
Epoch [189/200], Loss: 0.0297
Epoch [190/200], Loss: 0.0320
-- Evaluation after Epoch [190/200], Test Loss: 0.0658
Epoch [191/200], Loss: 0.0326
Epoch [192/200], Loss: 0.0300
Epoch [193/200], Loss: 0.0296
Epoch [194/200], Loss: 0.0297
Epoch [195/200], Loss: 0.0305
-- Evaluation after Epoch [195/200], Test Loss: 0.0755
Epoch [196/200], Loss: 0.0306
Epoch [197/200], Loss: 0.0298
Epoch [198/200], Loss: 0.0301
Epoch [199/200], Loss: 0.0294
Epoch [200/200], Loss: 0.0306
-- Evaluation after Epoch [200/200], Test Loss: 0.0762
Loading the best model from checkpoint with test loss: 0.0621
Validation Loss (RMSE): 0.0409
Validation results saved to: ./Results\LSTMStudy_20250119_163659\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0621
Avg Test Loss after reverse scaling (RMSE): 49.9164
Test results saved to: ./Results\LSTMStudy_20250119_163659\Test\test_results.txt
Exception in comm_msg for 8a434a8fd67911efbcf56c6a77a1cb50
Traceback (most recent call last):
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\commbase.py", line 347, in _handle_remote_call
    self._set_call_return_value(msg_dict, return_value)
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\commbase.py", line 384, in _set_call_return_value
    self._send_message('remote_call_reply', content=content, data=data,
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\frontendcomm.py", line 113, in _send_message
    return super(FrontendComm, self)._send_message(*args, **kwargs)
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\commbase.py", line 238, in _send_message
    raise CommError("The comm is not connected.")
spyder_kernels.comms.commbase.CommError: The comm is not connected.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\comm\base_comm.py", line 296, in comm_msg
    comm.handle_msg(msg)
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\frontendcomm.py", line 260, in handle_msg
    comm._msg_callback(msg)
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\commbase.py", line 333, in _comm_message
    self._message_handlers[spyder_msg_type](
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\commbase.py", line 351, in _handle_remote_call
    self._set_call_return_value(msg_dict, exc_infos, is_error=True)
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\commbase.py", line 384, in _set_call_return_value
    self._send_message('remote_call_reply', content=content, data=data,
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\frontendcomm.py", line 113, in _send_message
    return super(FrontendComm, self)._send_message(*args, **kwargs)
  File "C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\spyder_kernels\comms\commbase.py", line 238, in _send_message
    raise CommError("The comm is not connected.")
spyder_kernels.comms.commbase.CommError: The comm is not connected.
