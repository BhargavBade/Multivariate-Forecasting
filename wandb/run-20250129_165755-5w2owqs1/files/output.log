Epoch [1/300], Loss: 0.2240
Epoch [2/300], Loss: 0.1107
Epoch [3/300], Loss: 0.0990
Epoch [4/300], Loss: 0.0940
Epoch [5/300], Loss: 0.0912
-- Evaluation after Epoch [5/300], Test Loss: 0.0735
New best test loss: 0.0735. Saving model.
Epoch [6/300], Loss: 0.0900
Epoch [7/300], Loss: 0.0893
Epoch [8/300], Loss: 0.0890
Epoch [9/300], Loss: 0.0889
Epoch [10/300], Loss: 0.0889
-- Evaluation after Epoch [10/300], Test Loss: 0.0739
Epoch [11/300], Loss: 0.0888
Epoch [12/300], Loss: 0.0888
Epoch [13/300], Loss: 0.0888
Epoch [14/300], Loss: 0.0889
Epoch [15/300], Loss: 0.0888
-- Evaluation after Epoch [15/300], Test Loss: 0.0738
Epoch [16/300], Loss: 0.0889
Epoch [17/300], Loss: 0.0888
Epoch [18/300], Loss: 0.0888
Epoch [19/300], Loss: 0.0889
Epoch [20/300], Loss: 0.0888
-- Evaluation after Epoch [20/300], Test Loss: 0.0738
Epoch [21/300], Loss: 0.0888
Epoch [22/300], Loss: 0.0888
Epoch [23/300], Loss: 0.0889
Epoch [24/300], Loss: 0.0889
Epoch [25/300], Loss: 0.0889
-- Evaluation after Epoch [25/300], Test Loss: 0.0739
Epoch [26/300], Loss: 0.0889
Epoch [27/300], Loss: 0.0889
Epoch [28/300], Loss: 0.0889
Epoch [29/300], Loss: 0.0889
Epoch [30/300], Loss: 0.0889
-- Evaluation after Epoch [30/300], Test Loss: 0.0739
Epoch [31/300], Loss: 0.0889
Epoch [32/300], Loss: 0.0889
Epoch [33/300], Loss: 0.0889
Epoch [34/300], Loss: 0.0889
Epoch [35/300], Loss: 0.0889
-- Evaluation after Epoch [35/300], Test Loss: 0.0739
Epoch [36/300], Loss: 0.0889
Epoch [37/300], Loss: 0.0889
Epoch [38/300], Loss: 0.0889
Epoch [39/300], Loss: 0.0889
Epoch [40/300], Loss: 0.0889
-- Evaluation after Epoch [40/300], Test Loss: 0.0738
Epoch [41/300], Loss: 0.0889
Epoch [42/300], Loss: 0.0889
Epoch [43/300], Loss: 0.0889
Epoch [44/300], Loss: 0.0889
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, prepare_data02, params[0m
196
64
./Results\LSTM_Study_20250129_165837
./Results\LSTM_Study_20250129_165837\Train
Config file saved to: ./Results\LSTM_Study_20250129_165837\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([840, 196, 11])
Train labels tensor shape: torch.Size([840, 48])
Val data tensor shape: torch.Size([186, 196, 11])
Val labels tensor shape: torch.Size([186, 48])
Test data tensor shape: torch.Size([186, 196, 11])
Test labels tensor shape: torch.Size([186, 48])
Epoch [1/300], Loss: 0.2086
Epoch [2/300], Loss: 0.1158
Epoch [3/300], Loss: 0.1025
Epoch [4/300], Loss: 0.0976
Epoch [5/300], Loss: 0.0937
-- Evaluation after Epoch [5/300], Test Loss: 0.0740
New best test loss: 0.0740. Saving model.
Epoch [6/300], Loss: 0.0915
Epoch [7/300], Loss: 0.0902
Epoch [8/300], Loss: 0.0897
Epoch [9/300], Loss: 0.0892
Epoch [10/300], Loss: 0.0889
-- Evaluation after Epoch [10/300], Test Loss: 0.0730
New best test loss: 0.0730. Saving model.
Epoch [11/300], Loss: 0.0885
Epoch [12/300], Loss: 0.0884
Epoch [13/300], Loss: 0.0883
Epoch [14/300], Loss: 0.0882
Epoch [15/300], Loss: 0.0878
-- Evaluation after Epoch [15/300], Test Loss: 0.0714
New best test loss: 0.0714. Saving model.
Epoch [16/300], Loss: 0.0877
Epoch [17/300], Loss: 0.0878
Epoch [18/300], Loss: 0.0874
Epoch [19/300], Loss: 0.0874
Epoch [20/300], Loss: 0.0871
-- Evaluation after Epoch [20/300], Test Loss: 0.0702
New best test loss: 0.0702. Saving model.
Epoch [21/300], Loss: 0.0869
Epoch [22/300], Loss: 0.0869
Epoch [23/300], Loss: 0.0866
Epoch [24/300], Loss: 0.0863
Epoch [25/300], Loss: 0.0860
-- Evaluation after Epoch [25/300], Test Loss: 0.0684
New best test loss: 0.0684. Saving model.
Epoch [26/300], Loss: 0.0858
Epoch [27/300], Loss: 0.0858
Epoch [28/300], Loss: 0.0855
Epoch [29/300], Loss: 0.0854
Epoch [30/300], Loss: 0.0855
-- Evaluation after Epoch [30/300], Test Loss: 0.0685
Epoch [31/300], Loss: 0.0846
Epoch [32/300], Loss: 0.0845
Epoch [33/300], Loss: 0.0838
Epoch [34/300], Loss: 0.0836
Epoch [35/300], Loss: 0.0837
-- Evaluation after Epoch [35/300], Test Loss: 0.0692
Epoch [36/300], Loss: 0.0832
Epoch [37/300], Loss: 0.0826
Epoch [38/300], Loss: 0.0825
Epoch [39/300], Loss: 0.0821
Epoch [40/300], Loss: 0.0818
-- Evaluation after Epoch [40/300], Test Loss: 0.0680
New best test loss: 0.0680. Saving model.
Epoch [41/300], Loss: 0.0825
Epoch [42/300], Loss: 0.0819
Epoch [43/300], Loss: 0.0814
Epoch [44/300], Loss: 0.0819
Epoch [45/300], Loss: 0.0814
-- Evaluation after Epoch [45/300], Test Loss: 0.0703
Epoch [46/300], Loss: 0.0815
Epoch [47/300], Loss: 0.0805
Epoch [48/300], Loss: 0.0822
Epoch [49/300], Loss: 0.0820
Epoch [50/300], Loss: 0.0812
-- Evaluation after Epoch [50/300], Test Loss: 0.0659
New best test loss: 0.0659. Saving model.
Epoch [51/300], Loss: 0.0804
Epoch [52/300], Loss: 0.0799
Epoch [53/300], Loss: 0.0796
Epoch [54/300], Loss: 0.0797
Epoch [55/300], Loss: 0.0794
-- Evaluation after Epoch [55/300], Test Loss: 0.0659
Epoch [56/300], Loss: 0.0802
Epoch [57/300], Loss: 0.0799
Epoch [58/300], Loss: 0.0790
Epoch [59/300], Loss: 0.0791
Epoch [60/300], Loss: 0.0787
-- Evaluation after Epoch [60/300], Test Loss: 0.0674
Epoch [61/300], Loss: 0.0805
Epoch [62/300], Loss: 0.0802
Epoch [63/300], Loss: 0.0804
Epoch [64/300], Loss: 0.0805
Epoch [65/300], Loss: 0.0789
-- Evaluation after Epoch [65/300], Test Loss: 0.0639
New best test loss: 0.0639. Saving model.
Epoch [66/300], Loss: 0.0790
Epoch [67/300], Loss: 0.0786
Epoch [68/300], Loss: 0.0778
Epoch [69/300], Loss: 0.0776
Epoch [70/300], Loss: 0.0774
-- Evaluation after Epoch [70/300], Test Loss: 0.0647
Epoch [71/300], Loss: 0.0769
Epoch [72/300], Loss: 0.0766
Epoch [73/300], Loss: 0.0768
Epoch [74/300], Loss: 0.0785
Epoch [75/300], Loss: 0.0803
-- Evaluation after Epoch [75/300], Test Loss: 0.0666
Epoch [76/300], Loss: 0.0802
Epoch [77/300], Loss: 0.0806
Epoch [78/300], Loss: 0.0794
Epoch [79/300], Loss: 0.0770
Epoch [80/300], Loss: 0.0766
-- Evaluation after Epoch [80/300], Test Loss: 0.0645
Epoch [81/300], Loss: 0.0760
Epoch [82/300], Loss: 0.0764
Epoch [83/300], Loss: 0.0760
Epoch [84/300], Loss: 0.0760
Epoch [85/300], Loss: 0.0757
-- Evaluation after Epoch [85/300], Test Loss: 0.0655
Epoch [86/300], Loss: 0.0745
Epoch [87/300], Loss: 0.0750
Epoch [88/300], Loss: 0.0750
Epoch [89/300], Loss: 0.0765
Epoch [90/300], Loss: 0.0762
-- Evaluation after Epoch [90/300], Test Loss: 0.0669
Epoch [91/300], Loss: 0.0761
Epoch [92/300], Loss: 0.0759
Epoch [93/300], Loss: 0.0782
Epoch [94/300], Loss: 0.0750
Epoch [95/300], Loss: 0.0751
-- Evaluation after Epoch [95/300], Test Loss: 0.0701
Epoch [96/300], Loss: 0.0736
Epoch [97/300], Loss: 0.0736
Epoch [98/300], Loss: 0.0740
Epoch [99/300], Loss: 0.0743
Epoch [100/300], Loss: 0.0734
-- Evaluation after Epoch [100/300], Test Loss: 0.0676
Epoch [101/300], Loss: 0.0727
Epoch [102/300], Loss: 0.0721
Epoch [103/300], Loss: 0.0719
Epoch [104/300], Loss: 0.0708
Epoch [105/300], Loss: 0.0708
-- Evaluation after Epoch [105/300], Test Loss: 0.0653
Epoch [106/300], Loss: 0.0709
Epoch [107/300], Loss: 0.0710
Epoch [108/300], Loss: 0.0706
Epoch [109/300], Loss: 0.0708
Epoch [110/300], Loss: 0.0700
-- Evaluation after Epoch [110/300], Test Loss: 0.0682
Epoch [111/300], Loss: 0.0699
Epoch [112/300], Loss: 0.0693
Epoch [113/300], Loss: 0.0697
Epoch [114/300], Loss: 0.0715
Epoch [115/300], Loss: 0.0730
-- Evaluation after Epoch [115/300], Test Loss: 0.0692
Epoch [116/300], Loss: 0.0721
Epoch [117/300], Loss: 0.0709
Epoch [118/300], Loss: 0.0703
Epoch [119/300], Loss: 0.0700
Epoch [120/300], Loss: 0.0688
-- Evaluation after Epoch [120/300], Test Loss: 0.0678
Epoch [121/300], Loss: 0.0694
Epoch [122/300], Loss: 0.0683
Epoch [123/300], Loss: 0.0681
Epoch [124/300], Loss: 0.0674
Epoch [125/300], Loss: 0.0674
-- Evaluation after Epoch [125/300], Test Loss: 0.0702
Epoch [126/300], Loss: 0.0687
Epoch [127/300], Loss: 0.0735
Epoch [128/300], Loss: 0.0705
Epoch [129/300], Loss: 0.0688
Epoch [130/300], Loss: 0.0683
-- Evaluation after Epoch [130/300], Test Loss: 0.0689
Epoch [131/300], Loss: 0.0684
Epoch [132/300], Loss: 0.0677
Epoch [133/300], Loss: 0.0667
Epoch [134/300], Loss: 0.0656
Epoch [135/300], Loss: 0.0652
-- Evaluation after Epoch [135/300], Test Loss: 0.0698
Epoch [136/300], Loss: 0.0650
Epoch [137/300], Loss: 0.0646
Epoch [138/300], Loss: 0.0641
Epoch [139/300], Loss: 0.0642
Epoch [140/300], Loss: 0.0634
-- Evaluation after Epoch [140/300], Test Loss: 0.0711
Epoch [141/300], Loss: 0.0633
Epoch [142/300], Loss: 0.0643
Epoch [143/300], Loss: 0.0644
Epoch [144/300], Loss: 0.0640
Epoch [145/300], Loss: 0.0631
-- Evaluation after Epoch [145/300], Test Loss: 0.0683
Epoch [146/300], Loss: 0.0631
Epoch [147/300], Loss: 0.0627
Epoch [148/300], Loss: 0.0620
Epoch [149/300], Loss: 0.0620
Epoch [150/300], Loss: 0.0643
-- Evaluation after Epoch [150/300], Test Loss: 0.0672
Epoch [151/300], Loss: 0.0632
Epoch [152/300], Loss: 0.0620
Epoch [153/300], Loss: 0.0616
Epoch [154/300], Loss: 0.0633
Epoch [155/300], Loss: 0.0617
-- Evaluation after Epoch [155/300], Test Loss: 0.0669
Epoch [156/300], Loss: 0.0607
Epoch [157/300], Loss: 0.0617
Epoch [158/300], Loss: 0.0615
Epoch [159/300], Loss: 0.0604
Epoch [160/300], Loss: 0.0600
-- Evaluation after Epoch [160/300], Test Loss: 0.0649
Epoch [161/300], Loss: 0.0585
Epoch [162/300], Loss: 0.0591
Epoch [163/300], Loss: 0.0597
Epoch [164/300], Loss: 0.0599
Epoch [165/300], Loss: 0.0588
-- Evaluation after Epoch [165/300], Test Loss: 0.0671
Epoch [166/300], Loss: 0.0581
Epoch [167/300], Loss: 0.0596
Epoch [168/300], Loss: 0.0589
Epoch [169/300], Loss: 0.0597
Epoch [170/300], Loss: 0.0585
-- Evaluation after Epoch [170/300], Test Loss: 0.0682
Epoch [171/300], Loss: 0.0587
Epoch [172/300], Loss: 0.0589
Epoch [173/300], Loss: 0.0583
Epoch [174/300], Loss: 0.0605
Epoch [175/300], Loss: 0.0605
-- Evaluation after Epoch [175/300], Test Loss: 0.0690
Epoch [176/300], Loss: 0.0640
Epoch [177/300], Loss: 0.0679
Epoch [178/300], Loss: 0.0685
Epoch [179/300], Loss: 0.0668
Epoch [180/300], Loss: 0.0628
-- Evaluation after Epoch [180/300], Test Loss: 0.0680
Epoch [181/300], Loss: 0.0611
Epoch [182/300], Loss: 0.0605
Epoch [183/300], Loss: 0.0584
Epoch [184/300], Loss: 0.0584
Epoch [185/300], Loss: 0.0578
-- Evaluation after Epoch [185/300], Test Loss: 0.0740
Epoch [186/300], Loss: 0.0575
Epoch [187/300], Loss: 0.0582
Epoch [188/300], Loss: 0.0570
Epoch [189/300], Loss: 0.0566
Epoch [190/300], Loss: 0.0559
-- Evaluation after Epoch [190/300], Test Loss: 0.0721
Epoch [191/300], Loss: 0.0561
Epoch [192/300], Loss: 0.0558
Epoch [193/300], Loss: 0.0548
Epoch [194/300], Loss: 0.0551
Epoch [195/300], Loss: 0.0553
-- Evaluation after Epoch [195/300], Test Loss: 0.0675
Epoch [196/300], Loss: 0.0544
Epoch [197/300], Loss: 0.0555
Epoch [198/300], Loss: 0.0550
Epoch [199/300], Loss: 0.0561
Epoch [200/300], Loss: 0.0562
-- Evaluation after Epoch [200/300], Test Loss: 0.0682
Epoch [201/300], Loss: 0.0550
Epoch [202/300], Loss: 0.0567
Epoch [203/300], Loss: 0.0570
Epoch [204/300], Loss: 0.0580
Epoch [205/300], Loss: 0.0590
-- Evaluation after Epoch [205/300], Test Loss: 0.0669
Epoch [206/300], Loss: 0.0573
Epoch [207/300], Loss: 0.0582
Epoch [208/300], Loss: 0.0583
Epoch [209/300], Loss: 0.0594
Epoch [210/300], Loss: 0.0597
-- Evaluation after Epoch [210/300], Test Loss: 0.0793
Epoch [211/300], Loss: 0.0562
Epoch [212/300], Loss: 0.0548
Epoch [213/300], Loss: 0.0553
Epoch [214/300], Loss: 0.0545
Epoch [215/300], Loss: 0.0533
-- Evaluation after Epoch [215/300], Test Loss: 0.0686
Epoch [216/300], Loss: 0.0540
Epoch [217/300], Loss: 0.0537
Epoch [218/300], Loss: 0.0537
Epoch [219/300], Loss: 0.0543
Epoch [220/300], Loss: 0.0551
-- Evaluation after Epoch [220/300], Test Loss: 0.0729
Epoch [221/300], Loss: 0.0534
Epoch [222/300], Loss: 0.0543
Epoch [223/300], Loss: 0.0547
Epoch [224/300], Loss: 0.0536
Epoch [225/300], Loss: 0.0532
-- Evaluation after Epoch [225/300], Test Loss: 0.0675
Epoch [226/300], Loss: 0.0523
Epoch [227/300], Loss: 0.0521
Epoch [228/300], Loss: 0.0541
Epoch [229/300], Loss: 0.0543
Epoch [230/300], Loss: 0.0517
-- Evaluation after Epoch [230/300], Test Loss: 0.0661
Epoch [231/300], Loss: 0.0510
Epoch [232/300], Loss: 0.0504
Epoch [233/300], Loss: 0.0503
Epoch [234/300], Loss: 0.0512
Epoch [235/300], Loss: 0.0510
-- Evaluation after Epoch [235/300], Test Loss: 0.0732
Epoch [236/300], Loss: 0.0512
Epoch [237/300], Loss: 0.0500
Epoch [238/300], Loss: 0.0505
Epoch [239/300], Loss: 0.0493
Epoch [240/300], Loss: 0.0493
-- Evaluation after Epoch [240/300], Test Loss: 0.0688
Epoch [241/300], Loss: 0.0496
Epoch [242/300], Loss: 0.0509
Epoch [243/300], Loss: 0.0514
Epoch [244/300], Loss: 0.0511
Epoch [245/300], Loss: 0.0501
-- Evaluation after Epoch [245/300], Test Loss: 0.0687
Epoch [246/300], Loss: 0.0494
Epoch [247/300], Loss: 0.0500
Epoch [248/300], Loss: 0.0498
Epoch [249/300], Loss: 0.0488
Epoch [250/300], Loss: 0.0486
-- Evaluation after Epoch [250/300], Test Loss: 0.0744
Epoch [251/300], Loss: 0.0485
Epoch [252/300], Loss: 0.0486
Epoch [253/300], Loss: 0.0491
Epoch [254/300], Loss: 0.0491
Epoch [255/300], Loss: 0.0487
-- Evaluation after Epoch [255/300], Test Loss: 0.0748
Epoch [256/300], Loss: 0.0488
Epoch [257/300], Loss: 0.0493
Epoch [258/300], Loss: 0.0490
Epoch [259/300], Loss: 0.0507
Epoch [260/300], Loss: 0.0516
-- Evaluation after Epoch [260/300], Test Loss: 0.0703
Epoch [261/300], Loss: 0.0529
Epoch [262/300], Loss: 0.0532
Epoch [263/300], Loss: 0.0507
Epoch [264/300], Loss: 0.0506
Epoch [265/300], Loss: 0.0491
-- Evaluation after Epoch [265/300], Test Loss: 0.0745
Epoch [266/300], Loss: 0.0499
Epoch [267/300], Loss: 0.0497
Epoch [268/300], Loss: 0.0492
Epoch [269/300], Loss: 0.0492
Epoch [270/300], Loss: 0.0484
-- Evaluation after Epoch [270/300], Test Loss: 0.0705
Epoch [271/300], Loss: 0.0482
Epoch [272/300], Loss: 0.0478
Epoch [273/300], Loss: 0.0483
Epoch [274/300], Loss: 0.0476
Epoch [275/300], Loss: 0.0470
-- Evaluation after Epoch [275/300], Test Loss: 0.0710
Epoch [276/300], Loss: 0.0475
Epoch [277/300], Loss: 0.0481
Epoch [278/300], Loss: 0.0472
Epoch [279/300], Loss: 0.0477
Epoch [280/300], Loss: 0.0472
-- Evaluation after Epoch [280/300], Test Loss: 0.0721
Epoch [281/300], Loss: 0.0463
Epoch [282/300], Loss: 0.0461
Epoch [283/300], Loss: 0.0467
Epoch [284/300], Loss: 0.0463
Epoch [285/300], Loss: 0.0470
-- Evaluation after Epoch [285/300], Test Loss: 0.0692
Epoch [286/300], Loss: 0.0467
Epoch [287/300], Loss: 0.0464
Epoch [288/300], Loss: 0.0464
Epoch [289/300], Loss: 0.0457
Epoch [290/300], Loss: 0.0461
-- Evaluation after Epoch [290/300], Test Loss: 0.0724
Epoch [291/300], Loss: 0.0460
Epoch [292/300], Loss: 0.0453
Epoch [293/300], Loss: 0.0454
Epoch [294/300], Loss: 0.0452
Epoch [295/300], Loss: 0.0454
-- Evaluation after Epoch [295/300], Test Loss: 0.0710
Epoch [296/300], Loss: 0.0466
Epoch [297/300], Loss: 0.0487
Epoch [298/300], Loss: 0.0493
Epoch [299/300], Loss: 0.0465
Epoch [300/300], Loss: 0.0469
-- Evaluation after Epoch [300/300], Test Loss: 0.0702
Loading the best model from checkpoint with test loss: 0.0639
Validation Loss (RMSE): 0.0639
Validation results saved to: ./Results\LSTM_Study_20250129_165837\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0639
Avg Test Loss after reverse scaling (RMSE): 48.9525
Test results saved to: ./Results\LSTM_Study_20250129_165837\Test\test_results.txt
