Epoch [1/20], Loss: 0.2906
Epoch [2/20], Loss: 0.1667
Epoch [3/20], Loss: 0.1165
Epoch [4/20], Loss: 0.1052
Epoch [5/20], Loss: 0.1016
-- Evaluation after Epoch [5/20], Test Loss: 0.1131
New best test loss: 0.1131. Saving model.
Epoch [6/20], Loss: 0.0981
Epoch [7/20], Loss: 0.0949
Epoch [8/20], Loss: 0.0929
Epoch [9/20], Loss: 0.0915
Epoch [10/20], Loss: 0.0905
-- Evaluation after Epoch [10/20], Test Loss: 0.1115
New best test loss: 0.1115. Saving model.
Epoch [11/20], Loss: 0.0897
Epoch [12/20], Loss: 0.0895
Epoch [13/20], Loss: 0.0894
Epoch [14/20], Loss: 0.0892
Epoch [15/20], Loss: 0.0893
-- Evaluation after Epoch [15/20], Test Loss: 0.1115
Epoch [16/20], Loss: 0.0892
Epoch [17/20], Loss: 0.0892
Epoch [18/20], Loss: 0.0892
Epoch [19/20], Loss: 0.0891
Epoch [20/20], Loss: 0.0892
-- Evaluation after Epoch [20/20], Test Loss: 0.1115
Loading the best model from checkpoint with test loss: 0.1115
Validation Loss (RMSE): 0.1115
Validation results saved to: ./Results\LSTM_Study_20250129_222420\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1115
Avg Test Loss after reverse scaling (RMSE): 85.6655
Test results saved to: ./Results\LSTM_Study_20250129_222420\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
> [1;32mc:\users\bhargav bade\multivar lstm\data\prepare_data02.py[0m(2)[0;36m<module>[1;34m()[0m
[1;32m      1 [1;33m[1;31m# prepare_data.py[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m----> 2 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      3 [1;33m[1;32mimport[0m [0mpandas[0m [1;32mas[0m [0mpd[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      4 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;32mimport[0m [0mmath[0m[1;33m[0m[1;33m[0m[0m
[0m
[4;33mReloaded modules[24m: params[0m
> [1;32mc:\users\bhargav bade\multivar lstm\train_test_sng_step.py[0m(6)[0;36m<module>[1;34m()[0m
[1;32m      4 [1;33m[1;31m# In[34]:[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;33m[0m[0m
[0m[1;32m----> 6 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      7 [1;33m[1;32mimport[0m [0mtorch[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      8 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m
96
64
./Results\LSTM_Study_20250129_222920
./Results\LSTM_Study_20250129_222920\Train
Config file saved to: ./Results\LSTM_Study_20250129_222920\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([321, 96, 11])
Train labels tensor shape: torch.Size([321, 24])
Val data tensor shape: torch.Size([69, 96, 11])
Val labels tensor shape: torch.Size([69, 24])
Test data tensor shape: torch.Size([69, 96, 11])
Test labels tensor shape: torch.Size([69, 24])
Epoch [1/20], Loss: 0.2706
Epoch [2/20], Loss: 0.1535
Epoch [3/20], Loss: 0.1093
Epoch [4/20], Loss: 0.1043
Epoch [5/20], Loss: 0.1001
-- Evaluation after Epoch [5/20], Test Loss: 0.1118
New best test loss: 0.1118. Saving model.
Epoch [6/20], Loss: 0.0958
Epoch [7/20], Loss: 0.0943
Epoch [8/20], Loss: 0.0935
Epoch [9/20], Loss: 0.0914
Epoch [10/20], Loss: 0.0906
-- Evaluation after Epoch [10/20], Test Loss: 0.1113
New best test loss: 0.1113. Saving model.
Epoch [11/20], Loss: 0.0904
Epoch [12/20], Loss: 0.0895
Epoch [13/20], Loss: 0.0896
Epoch [14/20], Loss: 0.0894
Epoch [15/20], Loss: 0.0894
-- Evaluation after Epoch [15/20], Test Loss: 0.1117
Epoch [16/20], Loss: 0.0894
Epoch [17/20], Loss: 0.0893
Epoch [18/20], Loss: 0.0892
Epoch [19/20], Loss: 0.0891
Epoch [20/20], Loss: 0.0892
-- Evaluation after Epoch [20/20], Test Loss: 0.1115
Loading the best model from checkpoint with test loss: 0.1113
Validation Loss (RMSE): 0.1113
Validation results saved to: ./Results\LSTM_Study_20250129_222920\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1113
Avg Test Loss after reverse scaling (RMSE): 85.4615
Test results saved to: ./Results\LSTM_Study_20250129_222920\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
> [1;32mc:\users\bhargav bade\multivar lstm\data\prepare_data02.py[0m(2)[0;36m<module>[1;34m()[0m
[1;32m      1 [1;33m[1;31m# prepare_data.py[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m----> 2 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      3 [1;33m[1;32mimport[0m [0mpandas[0m [1;32mas[0m [0mpd[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      4 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;32mimport[0m [0mmath[0m[1;33m[0m[1;33m[0m[0m
[0m
[4;33mReloaded modules[24m: params[0m
> [1;32mc:\users\bhargav bade\multivar lstm\train_test_sng_step.py[0m(6)[0;36m<module>[1;34m()[0m
[1;32m      4 [1;33m[1;31m# In[34]:[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;33m[0m[0m
[0m[1;32m----> 6 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      7 [1;33m[1;32mimport[0m [0mtorch[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      8 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m
96
64
./Results\LSTM_Study_20250129_224451
./Results\LSTM_Study_20250129_224451\Train
Config file saved to: ./Results\LSTM_Study_20250129_224451\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([321, 96, 11])
Train labels tensor shape: torch.Size([321, 24])
Val data tensor shape: torch.Size([69, 96, 11])
Val labels tensor shape: torch.Size([69, 24])
Test data tensor shape: torch.Size([69, 96, 11])
Test labels tensor shape: torch.Size([69, 24])
Epoch [1/20], Loss: 0.3346
Epoch [2/20], Loss: 0.2111
Epoch [3/20], Loss: 0.1457
Epoch [4/20], Loss: 0.1146
Epoch [5/20], Loss: 0.1065
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
> [1;32mc:\users\bhargav bade\multivar lstm\train_test_sng_step.py[0m(6)[0;36m<module>[1;34m()[0m
[1;32m      4 [1;33m[1;31m# In[34]:[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;33m[0m[0m
[0m[1;32m----> 6 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      7 [1;33m[1;32mimport[0m [0mtorch[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      8 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m
96
64
./Results\LSTM_Study_20250129_230659
./Results\LSTM_Study_20250129_230659\Train
Config file saved to: ./Results\LSTM_Study_20250129_230659\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([321, 96, 11])
Train labels tensor shape: torch.Size([321, 24])
Val data tensor shape: torch.Size([69, 96, 11])
Val labels tensor shape: torch.Size([69, 24])
Test data tensor shape: torch.Size([69, 96, 11])
Test labels tensor shape: torch.Size([69, 24])
Epoch [1/20], Loss: 0.2504
Epoch [2/20], Loss: 0.1346
Epoch [3/20], Loss: 0.1027
Epoch [4/20], Loss: 0.0995
Epoch [5/20], Loss: 0.0964
-- Evaluation after Epoch [5/20], Test Loss: 0.1121
New best test loss: 0.1121. Saving model.
Epoch [6/20], Loss: 0.0948
Epoch [7/20], Loss: 0.0930
Epoch [8/20], Loss: 0.0914
Epoch [9/20], Loss: 0.0907
Epoch [10/20], Loss: 0.0898
-- Evaluation after Epoch [10/20], Test Loss: 0.1116
New best test loss: 0.1116. Saving model.
Epoch [11/20], Loss: 0.0897
Epoch [12/20], Loss: 0.0897
Epoch [13/20], Loss: 0.0893
Epoch [14/20], Loss: 0.0893
Epoch [15/20], Loss: 0.0891
-- Evaluation after Epoch [15/20], Test Loss: 0.1116
New best test loss: 0.1116. Saving model.
Epoch [16/20], Loss: 0.0891
Epoch [17/20], Loss: 0.0892
Epoch [18/20], Loss: 0.0891
Epoch [19/20], Loss: 0.0891
Epoch [20/20], Loss: 0.0891
-- Evaluation after Epoch [20/20], Test Loss: 0.1115
New best test loss: 0.1115. Saving model.
Loading the best model from checkpoint with test loss: 0.1115
Validation Loss (RMSE): 0.1115
Validation results saved to: ./Results\LSTM_Study_20250129_230659\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1115
Avg Test Loss after reverse scaling (RMSE): 85.9458
Test results saved to: ./Results\LSTM_Study_20250129_230659\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
> [1;32mc:\users\bhargav bade\multivar lstm\train_test_sng_step.py[0m(6)[0;36m<module>[1;34m()[0m
[1;32m      4 [1;33m[1;31m# In[34]:[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;33m[0m[0m
[0m[1;32m----> 6 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      7 [1;33m[1;32mimport[0m [0mtorch[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      8 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m
96
64
./Results\LSTM_Study_20250129_233256
./Results\LSTM_Study_20250129_233256\Train
Config file saved to: ./Results\LSTM_Study_20250129_233256\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([321, 96, 11])
Train labels tensor shape: torch.Size([321, 24])
Val data tensor shape: torch.Size([69, 96, 11])
Val labels tensor shape: torch.Size([69, 24])
Test data tensor shape: torch.Size([69, 96, 11])
Test labels tensor shape: torch.Size([69, 24])
Epoch [1/20], Loss: 0.2935
Epoch [2/20], Loss: 0.1768
Epoch [3/20], Loss: 0.1171
Epoch [4/20], Loss: 0.1005
Epoch [5/20], Loss: 0.0985
-- Evaluation after Epoch [5/20], Test Loss: 0.1137
New best test loss: 0.1137. Saving model.
Epoch [6/20], Loss: 0.0955
Epoch [7/20], Loss: 0.0940
Epoch [8/20], Loss: 0.0922
Epoch [9/20], Loss: 0.0910
Epoch [10/20], Loss: 0.0911
-- Evaluation after Epoch [10/20], Test Loss: 0.1113
New best test loss: 0.1113. Saving model.
Epoch [11/20], Loss: 0.0903
Epoch [12/20], Loss: 0.0901
Epoch [13/20], Loss: 0.0900
Epoch [14/20], Loss: 0.0897
Epoch [15/20], Loss: 0.0894
-- Evaluation after Epoch [15/20], Test Loss: 0.1116
Epoch [16/20], Loss: 0.0894
Epoch [17/20], Loss: 0.0892
Epoch [18/20], Loss: 0.0894
Epoch [19/20], Loss: 0.0892
Epoch [20/20], Loss: 0.0893
-- Evaluation after Epoch [20/20], Test Loss: 0.1115
Loading the best model from checkpoint with test loss: 0.1113
Validation Loss (RMSE): 0.1113
Validation results saved to: ./Results\LSTM_Study_20250129_233256\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1113
Avg Test Loss after reverse scaling (RMSE): 85.7064
Test results saved to: ./Results\LSTM_Study_20250129_233256\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
96
64
./Results\LSTM_Study_20250130_001010
./Results\LSTM_Study_20250130_001010\Train
Config file saved to: ./Results\LSTM_Study_20250130_001010\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([321, 96, 11])
Train labels tensor shape: torch.Size([321, 24])
Val data tensor shape: torch.Size([69, 96, 11])
Val labels tensor shape: torch.Size([69, 24])
Test data tensor shape: torch.Size([69, 96, 11])
Test labels tensor shape: torch.Size([69, 24])
Epoch [1/20], Loss: 0.2410
Epoch [2/20], Loss: 0.1331
Epoch [3/20], Loss: 0.1021
Epoch [4/20], Loss: 0.1031
Epoch [5/20], Loss: 0.0985
-- Evaluation after Epoch [5/20], Test Loss: 0.1115
New best test loss: 0.1115. Saving model.
Epoch [6/20], Loss: 0.0960
Epoch [7/20], Loss: 0.0947
Epoch [8/20], Loss: 0.0934
Epoch [9/20], Loss: 0.0929
Epoch [10/20], Loss: 0.0915
-- Evaluation after Epoch [10/20], Test Loss: 0.1114
New best test loss: 0.1114. Saving model.
Epoch [11/20], Loss: 0.0911
Epoch [12/20], Loss: 0.0906
Epoch [13/20], Loss: 0.0902
Epoch [14/20], Loss: 0.0898
Epoch [15/20], Loss: 0.0898
-- Evaluation after Epoch [15/20], Test Loss: 0.1118
Epoch [16/20], Loss: 0.0895
Epoch [17/20], Loss: 0.0892
Epoch [18/20], Loss: 0.0892
Epoch [19/20], Loss: 0.0893
Epoch [20/20], Loss: 0.0893
-- Evaluation after Epoch [20/20], Test Loss: 0.1115
Loading the best model from checkpoint with test loss: 0.1114
Validation Loss (RMSE): 0.1114
Validation results saved to: ./Results\LSTM_Study_20250130_001010\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1114
Avg Test Loss after reverse scaling (RMSE): 85.6349
Test results saved to: ./Results\LSTM_Study_20250130_001010\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
96
64
./Results\LSTM_Study_20250130_002149
./Results\LSTM_Study_20250130_002149\Train
Config file saved to: ./Results\LSTM_Study_20250130_002149\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([321, 96, 11])
Train labels tensor shape: torch.Size([321, 24])
Val data tensor shape: torch.Size([69, 96, 11])
Val labels tensor shape: torch.Size([69, 24])
Test data tensor shape: torch.Size([69, 96, 11])
Test labels tensor shape: torch.Size([69, 24])
Epoch [1/20], Loss: 0.2171
Epoch [2/20], Loss: 0.1115
Epoch [3/20], Loss: 0.1007
Epoch [4/20], Loss: 0.0981
Epoch [5/20], Loss: 0.0940
-- Evaluation after Epoch [5/20], Test Loss: 0.1117
New best test loss: 0.1117. Saving model.
Epoch [6/20], Loss: 0.0928
Epoch [7/20], Loss: 0.0914
Epoch [8/20], Loss: 0.0910
Epoch [9/20], Loss: 0.0902
Epoch [10/20], Loss: 0.0898
-- Evaluation after Epoch [10/20], Test Loss: 0.1117
New best test loss: 0.1117. Saving model.
Epoch [11/20], Loss: 0.0897
Epoch [12/20], Loss: 0.0892
Epoch [13/20], Loss: 0.0895
Epoch [14/20], Loss: 0.0893
Epoch [15/20], Loss: 0.0894
-- Evaluation after Epoch [15/20], Test Loss: 0.1114
New best test loss: 0.1114. Saving model.
Epoch [16/20], Loss: 0.0891
Epoch [17/20], Loss: 0.0891
Epoch [18/20], Loss: 0.0892
Epoch [19/20], Loss: 0.0891
Epoch [20/20], Loss: 0.0891
-- Evaluation after Epoch [20/20], Test Loss: 0.1116
Loading the best model from checkpoint with test loss: 0.1114
Validation Loss (RMSE): 0.1114
Validation results saved to: ./Results\LSTM_Study_20250130_002149\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1114
Avg Test Loss after reverse scaling (RMSE): 85.7883
Test results saved to: ./Results\LSTM_Study_20250130_002149\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
96
64
./Results\LSTM_Study_20250130_002352
./Results\LSTM_Study_20250130_002352\Train
Config file saved to: ./Results\LSTM_Study_20250130_002352\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([1685, 96, 11])
Train labels tensor shape: torch.Size([1685, 24])
Val data tensor shape: torch.Size([376, 96, 11])
Val labels tensor shape: torch.Size([376, 24])
Test data tensor shape: torch.Size([376, 96, 11])
Test labels tensor shape: torch.Size([376, 24])
Epoch [1/200], Loss: 0.1372
Epoch [2/200], Loss: 0.0914
Epoch [3/200], Loss: 0.0885
Epoch [4/200], Loss: 0.0878
Epoch [5/200], Loss: 0.0877
-- Evaluation after Epoch [5/200], Test Loss: 0.0771
New best test loss: 0.0771. Saving model.
Epoch [6/200], Loss: 0.0876
Epoch [7/200], Loss: 0.0876
Epoch [8/200], Loss: 0.0876
Epoch [9/200], Loss: 0.0876
Epoch [10/200], Loss: 0.0871
-- Evaluation after Epoch [10/200], Test Loss: 0.0742
New best test loss: 0.0742. Saving model.
Epoch [11/200], Loss: 0.0859
Epoch [12/200], Loss: 0.0847
Epoch [13/200], Loss: 0.0837
Epoch [14/200], Loss: 0.0829
Epoch [15/200], Loss: 0.0828
-- Evaluation after Epoch [15/200], Test Loss: 0.0700
New best test loss: 0.0700. Saving model.
Epoch [16/200], Loss: 0.0823
Epoch [17/200], Loss: 0.0825
Epoch [18/200], Loss: 0.0813
Epoch [19/200], Loss: 0.0810
Epoch [20/200], Loss: 0.0810
-- Evaluation after Epoch [20/200], Test Loss: 0.0688
New best test loss: 0.0688. Saving model.
Epoch [21/200], Loss: 0.0813
Epoch [22/200], Loss: 0.0799
Epoch [23/200], Loss: 0.0800
Epoch [24/200], Loss: 0.0802
Epoch [25/200], Loss: 0.0795
-- Evaluation after Epoch [25/200], Test Loss: 0.0676
New best test loss: 0.0676. Saving model.
Epoch [26/200], Loss: 0.0797
Epoch [27/200], Loss: 0.0789
Epoch [28/200], Loss: 0.0789
Epoch [29/200], Loss: 0.0785
Epoch [30/200], Loss: 0.0782
-- Evaluation after Epoch [30/200], Test Loss: 0.0662
New best test loss: 0.0662. Saving model.
Epoch [31/200], Loss: 0.0785
Epoch [32/200], Loss: 0.0787
Epoch [33/200], Loss: 0.0782
Epoch [34/200], Loss: 0.0771
Epoch [35/200], Loss: 0.0776
-- Evaluation after Epoch [35/200], Test Loss: 0.0663
Epoch [36/200], Loss: 0.0766
Epoch [37/200], Loss: 0.0774
Epoch [38/200], Loss: 0.0769
Epoch [39/200], Loss: 0.0765
Epoch [40/200], Loss: 0.0760
-- Evaluation after Epoch [40/200], Test Loss: 0.0656
New best test loss: 0.0656. Saving model.
Epoch [41/200], Loss: 0.0759
Epoch [42/200], Loss: 0.0764
Epoch [43/200], Loss: 0.0750
Epoch [44/200], Loss: 0.0760
Epoch [45/200], Loss: 0.0759
-- Evaluation after Epoch [45/200], Test Loss: 0.0653
New best test loss: 0.0653. Saving model.
Epoch [46/200], Loss: 0.0760
Epoch [47/200], Loss: 0.0750
Epoch [48/200], Loss: 0.0762
Epoch [49/200], Loss: 0.0755
Epoch [50/200], Loss: 0.0742
-- Evaluation after Epoch [50/200], Test Loss: 0.0646
New best test loss: 0.0646. Saving model.
Epoch [51/200], Loss: 0.0746
Epoch [52/200], Loss: 0.0745
Epoch [53/200], Loss: 0.0736
Epoch [54/200], Loss: 0.0741
Epoch [55/200], Loss: 0.0735
-- Evaluation after Epoch [55/200], Test Loss: 0.0652
Epoch [56/200], Loss: 0.0736
Epoch [57/200], Loss: 0.0739
Epoch [58/200], Loss: 0.0728
Epoch [59/200], Loss: 0.0736
Epoch [60/200], Loss: 0.0728
-- Evaluation after Epoch [60/200], Test Loss: 0.0649
Epoch [61/200], Loss: 0.0729
Epoch [62/200], Loss: 0.0729
Epoch [63/200], Loss: 0.0726
Epoch [64/200], Loss: 0.0720
Epoch [65/200], Loss: 0.0721
-- Evaluation after Epoch [65/200], Test Loss: 0.0651
Epoch [66/200], Loss: 0.0722
Epoch [67/200], Loss: 0.0733
Epoch [68/200], Loss: 0.0732
Epoch [69/200], Loss: 0.0715
Epoch [70/200], Loss: 0.0731
-- Evaluation after Epoch [70/200], Test Loss: 0.0641
New best test loss: 0.0641. Saving model.
Epoch [71/200], Loss: 0.0722
Epoch [72/200], Loss: 0.0716
Epoch [73/200], Loss: 0.0709
Epoch [74/200], Loss: 0.0705
Epoch [75/200], Loss: 0.0703
-- Evaluation after Epoch [75/200], Test Loss: 0.0655
Epoch [76/200], Loss: 0.0701
Epoch [77/200], Loss: 0.0700
Epoch [78/200], Loss: 0.0693
Epoch [79/200], Loss: 0.0700
Epoch [80/200], Loss: 0.0713
-- Evaluation after Epoch [80/200], Test Loss: 0.0648
Epoch [81/200], Loss: 0.0704
Epoch [82/200], Loss: 0.0690
Epoch [83/200], Loss: 0.0689
Epoch [84/200], Loss: 0.0706
Epoch [85/200], Loss: 0.0692
-- Evaluation after Epoch [85/200], Test Loss: 0.0649
Epoch [86/200], Loss: 0.0700
Epoch [87/200], Loss: 0.0694
Epoch [88/200], Loss: 0.0706
Epoch [89/200], Loss: 0.0685
Epoch [90/200], Loss: 0.0682
-- Evaluation after Epoch [90/200], Test Loss: 0.0674
Epoch [91/200], Loss: 0.0673
Epoch [92/200], Loss: 0.0691
Epoch [93/200], Loss: 0.0690
Epoch [94/200], Loss: 0.0670
Epoch [95/200], Loss: 0.0677
-- Evaluation after Epoch [95/200], Test Loss: 0.0661
Epoch [96/200], Loss: 0.0669
Epoch [97/200], Loss: 0.0663
Epoch [98/200], Loss: 0.0656
Epoch [99/200], Loss: 0.0662
Epoch [100/200], Loss: 0.0659
-- Evaluation after Epoch [100/200], Test Loss: 0.0666
Epoch [101/200], Loss: 0.0656
Epoch [102/200], Loss: 0.0660
Epoch [103/200], Loss: 0.0669
Epoch [104/200], Loss: 0.0643
Epoch [105/200], Loss: 0.0654
-- Evaluation after Epoch [105/200], Test Loss: 0.0688
Epoch [106/200], Loss: 0.0647
Epoch [107/200], Loss: 0.0641
Epoch [108/200], Loss: 0.0665
Epoch [109/200], Loss: 0.0657
Epoch [110/200], Loss: 0.0646
-- Evaluation after Epoch [110/200], Test Loss: 0.0677
Epoch [111/200], Loss: 0.0632
Epoch [112/200], Loss: 0.0631
Epoch [113/200], Loss: 0.0632
Epoch [114/200], Loss: 0.0637
Epoch [115/200], Loss: 0.0642
-- Evaluation after Epoch [115/200], Test Loss: 0.0653
Epoch [116/200], Loss: 0.0643
Epoch [117/200], Loss: 0.0636
Epoch [118/200], Loss: 0.0646
Epoch [119/200], Loss: 0.0644
Epoch [120/200], Loss: 0.0617
-- Evaluation after Epoch [120/200], Test Loss: 0.0691
Epoch [121/200], Loss: 0.0614
Epoch [122/200], Loss: 0.0606
Epoch [123/200], Loss: 0.0627
Epoch [124/200], Loss: 0.0662
Epoch [125/200], Loss: 0.0643
-- Evaluation after Epoch [125/200], Test Loss: 0.0688
Epoch [126/200], Loss: 0.0625
Epoch [127/200], Loss: 0.0610
Epoch [128/200], Loss: 0.0624
Epoch [129/200], Loss: 0.0602
Epoch [130/200], Loss: 0.0603
-- Evaluation after Epoch [130/200], Test Loss: 0.0689
Epoch [131/200], Loss: 0.0595
Epoch [132/200], Loss: 0.0604
Epoch [133/200], Loss: 0.0603
Epoch [134/200], Loss: 0.0622
Epoch [135/200], Loss: 0.0598
-- Evaluation after Epoch [135/200], Test Loss: 0.0670
Epoch [136/200], Loss: 0.0598
Epoch [137/200], Loss: 0.0620
Epoch [138/200], Loss: 0.0635
Epoch [139/200], Loss: 0.0590
Epoch [140/200], Loss: 0.0599
-- Evaluation after Epoch [140/200], Test Loss: 0.0694
Epoch [141/200], Loss: 0.0614
Epoch [142/200], Loss: 0.0604
Epoch [143/200], Loss: 0.0573
Epoch [144/200], Loss: 0.0587
Epoch [145/200], Loss: 0.0618
-- Evaluation after Epoch [145/200], Test Loss: 0.0688
Epoch [146/200], Loss: 0.0595
Epoch [147/200], Loss: 0.0590
Epoch [148/200], Loss: 0.0580
Epoch [149/200], Loss: 0.0570
Epoch [150/200], Loss: 0.0575
-- Evaluation after Epoch [150/200], Test Loss: 0.0703
Epoch [151/200], Loss: 0.0561
Epoch [152/200], Loss: 0.0571
Epoch [153/200], Loss: 0.0565
Epoch [154/200], Loss: 0.0572
Epoch [155/200], Loss: 0.0552
-- Evaluation after Epoch [155/200], Test Loss: 0.0743
Epoch [156/200], Loss: 0.0563
Epoch [157/200], Loss: 0.0565
Epoch [158/200], Loss: 0.0572
Epoch [159/200], Loss: 0.0563
Epoch [160/200], Loss: 0.0585
-- Evaluation after Epoch [160/200], Test Loss: 0.0707
Epoch [161/200], Loss: 0.0563
Epoch [162/200], Loss: 0.0561
Epoch [163/200], Loss: 0.0571
Epoch [164/200], Loss: 0.0566
Epoch [165/200], Loss: 0.0540
-- Evaluation after Epoch [165/200], Test Loss: 0.0735
Epoch [166/200], Loss: 0.0537
Epoch [167/200], Loss: 0.0531
Epoch [168/200], Loss: 0.0532
Epoch [169/200], Loss: 0.0555
Epoch [170/200], Loss: 0.0588
-- Evaluation after Epoch [170/200], Test Loss: 0.0716
Epoch [171/200], Loss: 0.0562
Epoch [172/200], Loss: 0.0542
Epoch [173/200], Loss: 0.0553
Epoch [174/200], Loss: 0.0554
Epoch [175/200], Loss: 0.0531
-- Evaluation after Epoch [175/200], Test Loss: 0.0701
Epoch [176/200], Loss: 0.0555
Epoch [177/200], Loss: 0.0532
Epoch [178/200], Loss: 0.0528
Epoch [179/200], Loss: 0.0533
Epoch [180/200], Loss: 0.0541
-- Evaluation after Epoch [180/200], Test Loss: 0.0777
Epoch [181/200], Loss: 0.0576
Epoch [182/200], Loss: 0.0523
Epoch [183/200], Loss: 0.0517
Epoch [184/200], Loss: 0.0523
Epoch [185/200], Loss: 0.0503
-- Evaluation after Epoch [185/200], Test Loss: 0.0734
Epoch [186/200], Loss: 0.0504
Epoch [187/200], Loss: 0.0510
Epoch [188/200], Loss: 0.0500
Epoch [189/200], Loss: 0.0521
Epoch [190/200], Loss: 0.0533
-- Evaluation after Epoch [190/200], Test Loss: 0.0733
Epoch [191/200], Loss: 0.0513
Epoch [192/200], Loss: 0.0506
Epoch [193/200], Loss: 0.0505
Epoch [194/200], Loss: 0.0498
Epoch [195/200], Loss: 0.0502
-- Evaluation after Epoch [195/200], Test Loss: 0.0764
Epoch [196/200], Loss: 0.0516
Epoch [197/200], Loss: 0.0529
Epoch [198/200], Loss: 0.0525
Epoch [199/200], Loss: 0.0502
Epoch [200/200], Loss: 0.0497
-- Evaluation after Epoch [200/200], Test Loss: 0.0719
Loading the best model from checkpoint with test loss: 0.0641
Validation Loss (RMSE): 0.0641
Validation results saved to: ./Results\LSTM_Study_20250130_002352\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0641
Avg Test Loss after reverse scaling (RMSE): 45.7724
Test results saved to: ./Results\LSTM_Study_20250130_002352\Test\test_results.txt
