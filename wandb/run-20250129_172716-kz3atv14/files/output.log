[4;33mReloaded modules[24m: study_folder, params, prepare_data02, Network.lstm_network[0m
96
64
./Results\LSTM_Study_20250129_172858
./Results\LSTM_Study_20250129_172858\Train
Config file saved to: ./Results\LSTM_Study_20250129_172858\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([1685, 96, 11])
Train labels tensor shape: torch.Size([1685, 24])
Val data tensor shape: torch.Size([376, 96, 11])
Val labels tensor shape: torch.Size([376, 24])
Test data tensor shape: torch.Size([376, 96, 11])
Test labels tensor shape: torch.Size([376, 24])
C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\torch\nn\modules\loss.py:538: UserWarning: Using a target size (torch.Size([32, 24])) that is different to the input size (torch.Size([32, 48])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, prepare_data02, params[0m
96
64
./Results\LSTM_Study_20250129_173126
./Results\LSTM_Study_20250129_173126\Train
Config file saved to: ./Results\LSTM_Study_20250129_173126\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([1685, 96, 11])
Train labels tensor shape: torch.Size([1685, 24])
Val data tensor shape: torch.Size([376, 96, 11])
Val labels tensor shape: torch.Size([376, 24])
Test data tensor shape: torch.Size([376, 96, 11])
Test labels tensor shape: torch.Size([376, 24])
Epoch [1/300], Loss: 0.1194
Epoch [2/300], Loss: 0.0951
Epoch [3/300], Loss: 0.0913
Epoch [4/300], Loss: 0.0889
Epoch [5/300], Loss: 0.0867
-- Evaluation after Epoch [5/300], Test Loss: 0.0733
New best test loss: 0.0733. Saving model.
Epoch [6/300], Loss: 0.0849
Epoch [7/300], Loss: 0.0835
Epoch [8/300], Loss: 0.0825
Epoch [9/300], Loss: 0.0817
Epoch [10/300], Loss: 0.0807
-- Evaluation after Epoch [10/300], Test Loss: 0.0696
New best test loss: 0.0696. Saving model.
Epoch [11/300], Loss: 0.0798
Epoch [12/300], Loss: 0.0789
Epoch [13/300], Loss: 0.0796
Epoch [14/300], Loss: 0.0796
Epoch [15/300], Loss: 0.0794
-- Evaluation after Epoch [15/300], Test Loss: 0.0699
Epoch [16/300], Loss: 0.0795
Epoch [17/300], Loss: 0.0790
Epoch [18/300], Loss: 0.0784
Epoch [19/300], Loss: 0.0781
Epoch [20/300], Loss: 0.0784
-- Evaluation after Epoch [20/300], Test Loss: 0.0686
New best test loss: 0.0686. Saving model.
Epoch [21/300], Loss: 0.0787
Epoch [22/300], Loss: 0.0780
Epoch [23/300], Loss: 0.0776
Epoch [24/300], Loss: 0.0775
Epoch [25/300], Loss: 0.0765
-- Evaluation after Epoch [25/300], Test Loss: 0.0677
New best test loss: 0.0677. Saving model.
Epoch [26/300], Loss: 0.0756
Epoch [27/300], Loss: 0.0756
Epoch [28/300], Loss: 0.0761
Epoch [29/300], Loss: 0.0780
Epoch [30/300], Loss: 0.0749
-- Evaluation after Epoch [30/300], Test Loss: 0.0673
New best test loss: 0.0673. Saving model.
Epoch [31/300], Loss: 0.0748
Epoch [32/300], Loss: 0.0747
Epoch [33/300], Loss: 0.0752
Epoch [34/300], Loss: 0.0749
Epoch [35/300], Loss: 0.0745
-- Evaluation after Epoch [35/300], Test Loss: 0.0671
New best test loss: 0.0671. Saving model.
Epoch [36/300], Loss: 0.0757
Epoch [37/300], Loss: 0.0737
Epoch [38/300], Loss: 0.0738
Epoch [39/300], Loss: 0.0741
Epoch [40/300], Loss: 0.0747
-- Evaluation after Epoch [40/300], Test Loss: 0.0679
Epoch [41/300], Loss: 0.0732
Epoch [42/300], Loss: 0.0733
Epoch [43/300], Loss: 0.0756
Epoch [44/300], Loss: 0.0746
Epoch [45/300], Loss: 0.0736
-- Evaluation after Epoch [45/300], Test Loss: 0.0668
New best test loss: 0.0668. Saving model.
Epoch [46/300], Loss: 0.0717
Epoch [47/300], Loss: 0.0726
Epoch [48/300], Loss: 0.0732
Epoch [49/300], Loss: 0.0719
Epoch [50/300], Loss: 0.0724
-- Evaluation after Epoch [50/300], Test Loss: 0.0645
New best test loss: 0.0645. Saving model.
Epoch [51/300], Loss: 0.0729
Epoch [52/300], Loss: 0.0726
Epoch [53/300], Loss: 0.0720
Epoch [54/300], Loss: 0.0730
Epoch [55/300], Loss: 0.0735
-- Evaluation after Epoch [55/300], Test Loss: 0.0647
Epoch [56/300], Loss: 0.0728
Epoch [57/300], Loss: 0.0726
Epoch [58/300], Loss: 0.0750
Epoch [59/300], Loss: 0.0726
Epoch [60/300], Loss: 0.0721
-- Evaluation after Epoch [60/300], Test Loss: 0.0662
Epoch [61/300], Loss: 0.0719
Epoch [62/300], Loss: 0.0704
Epoch [63/300], Loss: 0.0706
Epoch [64/300], Loss: 0.0705
Epoch [65/300], Loss: 0.0699
-- Evaluation after Epoch [65/300], Test Loss: 0.0672
Epoch [66/300], Loss: 0.0700
Epoch [67/300], Loss: 0.0702
Epoch [68/300], Loss: 0.0697
Epoch [69/300], Loss: 0.0703
Epoch [70/300], Loss: 0.0703
-- Evaluation after Epoch [70/300], Test Loss: 0.0664
Epoch [71/300], Loss: 0.0687
Epoch [72/300], Loss: 0.0696
Epoch [73/300], Loss: 0.0701
Epoch [74/300], Loss: 0.0696
Epoch [75/300], Loss: 0.0692
-- Evaluation after Epoch [75/300], Test Loss: 0.0659
Epoch [76/300], Loss: 0.0696
Epoch [77/300], Loss: 0.0684
Epoch [78/300], Loss: 0.0680
Epoch [79/300], Loss: 0.0692
Epoch [80/300], Loss: 0.0691
-- Evaluation after Epoch [80/300], Test Loss: 0.0666
Epoch [81/300], Loss: 0.0686
Epoch [82/300], Loss: 0.0692
Epoch [83/300], Loss: 0.0677
Epoch [84/300], Loss: 0.0679
Epoch [85/300], Loss: 0.0684
-- Evaluation after Epoch [85/300], Test Loss: 0.0700
Epoch [86/300], Loss: 0.0689
Epoch [87/300], Loss: 0.0677
Epoch [88/300], Loss: 0.0690
Epoch [89/300], Loss: 0.0679
Epoch [90/300], Loss: 0.0693
-- Evaluation after Epoch [90/300], Test Loss: 0.0695
Epoch [91/300], Loss: 0.0675
Epoch [92/300], Loss: 0.0659
Epoch [93/300], Loss: 0.0671
Epoch [94/300], Loss: 0.0681
Epoch [95/300], Loss: 0.0667
-- Evaluation after Epoch [95/300], Test Loss: 0.0693
Epoch [96/300], Loss: 0.0665
Epoch [97/300], Loss: 0.0666
Epoch [98/300], Loss: 0.0660
Epoch [99/300], Loss: 0.0655
Epoch [100/300], Loss: 0.0677
-- Evaluation after Epoch [100/300], Test Loss: 0.0688
Epoch [101/300], Loss: 0.0669
Epoch [102/300], Loss: 0.0679
Epoch [103/300], Loss: 0.0665
Epoch [104/300], Loss: 0.0662
Epoch [105/300], Loss: 0.0645
-- Evaluation after Epoch [105/300], Test Loss: 0.0706
Epoch [106/300], Loss: 0.0653
Epoch [107/300], Loss: 0.0648
Epoch [108/300], Loss: 0.0669
Epoch [109/300], Loss: 0.0650
Epoch [110/300], Loss: 0.0643
-- Evaluation after Epoch [110/300], Test Loss: 0.0673
Epoch [111/300], Loss: 0.0656
Epoch [112/300], Loss: 0.0641
Epoch [113/300], Loss: 0.0655
Epoch [114/300], Loss: 0.0636
Epoch [115/300], Loss: 0.0633
-- Evaluation after Epoch [115/300], Test Loss: 0.0728
Epoch [116/300], Loss: 0.0645
Epoch [117/300], Loss: 0.0640
Epoch [118/300], Loss: 0.0627
Epoch [119/300], Loss: 0.0631
Epoch [120/300], Loss: 0.0627
-- Evaluation after Epoch [120/300], Test Loss: 0.0710
Epoch [121/300], Loss: 0.0635
Epoch [122/300], Loss: 0.0628
Epoch [123/300], Loss: 0.0622
Epoch [124/300], Loss: 0.0623
Epoch [125/300], Loss: 0.0624
-- Evaluation after Epoch [125/300], Test Loss: 0.0716
Epoch [126/300], Loss: 0.0620
Epoch [127/300], Loss: 0.0616
Epoch [128/300], Loss: 0.0627
Epoch [129/300], Loss: 0.0622
Epoch [130/300], Loss: 0.0628
-- Evaluation after Epoch [130/300], Test Loss: 0.0772
Epoch [131/300], Loss: 0.0643
Epoch [132/300], Loss: 0.0633
Epoch [133/300], Loss: 0.0676
Epoch [134/300], Loss: 0.0645
Epoch [135/300], Loss: 0.0632
-- Evaluation after Epoch [135/300], Test Loss: 0.0691
Epoch [136/300], Loss: 0.0613
Epoch [137/300], Loss: 0.0619
Epoch [138/300], Loss: 0.0626
Epoch [139/300], Loss: 0.0602
Epoch [140/300], Loss: 0.0610
-- Evaluation after Epoch [140/300], Test Loss: 0.0730
Epoch [141/300], Loss: 0.0616
Epoch [142/300], Loss: 0.0610
Epoch [143/300], Loss: 0.0617
Epoch [144/300], Loss: 0.0598
Epoch [145/300], Loss: 0.0593
-- Evaluation after Epoch [145/300], Test Loss: 0.0677
Epoch [146/300], Loss: 0.0618
Epoch [147/300], Loss: 0.0634
Epoch [148/300], Loss: 0.0599
Epoch [149/300], Loss: 0.0598
Epoch [150/300], Loss: 0.0592
-- Evaluation after Epoch [150/300], Test Loss: 0.0721
Epoch [151/300], Loss: 0.0591
Epoch [152/300], Loss: 0.0585
Epoch [153/300], Loss: 0.0598
Epoch [154/300], Loss: 0.0580
Epoch [155/300], Loss: 0.0596
-- Evaluation after Epoch [155/300], Test Loss: 0.0720
Epoch [156/300], Loss: 0.0622
Epoch [157/300], Loss: 0.0619
Epoch [158/300], Loss: 0.0609
Epoch [159/300], Loss: 0.0580
Epoch [160/300], Loss: 0.0564
-- Evaluation after Epoch [160/300], Test Loss: 0.0718
Epoch [161/300], Loss: 0.0565
Epoch [162/300], Loss: 0.0571
Epoch [163/300], Loss: 0.0556
Epoch [164/300], Loss: 0.0562
Epoch [165/300], Loss: 0.0558
-- Evaluation after Epoch [165/300], Test Loss: 0.0722
Epoch [166/300], Loss: 0.0566
Epoch [167/300], Loss: 0.0580
Epoch [168/300], Loss: 0.0576
Epoch [169/300], Loss: 0.0566
Epoch [170/300], Loss: 0.0561
-- Evaluation after Epoch [170/300], Test Loss: 0.0709
Epoch [171/300], Loss: 0.0558
Epoch [172/300], Loss: 0.0553
Epoch [173/300], Loss: 0.0559
Epoch [174/300], Loss: 0.0551
Epoch [175/300], Loss: 0.0542
-- Evaluation after Epoch [175/300], Test Loss: 0.0772
Epoch [176/300], Loss: 0.0535
Epoch [177/300], Loss: 0.0541
Epoch [178/300], Loss: 0.0550
Epoch [179/300], Loss: 0.0539
Epoch [180/300], Loss: 0.0564
-- Evaluation after Epoch [180/300], Test Loss: 0.0735
Epoch [181/300], Loss: 0.0545
Epoch [182/300], Loss: 0.0532
Epoch [183/300], Loss: 0.0531
Epoch [184/300], Loss: 0.0535
Epoch [185/300], Loss: 0.0539
-- Evaluation after Epoch [185/300], Test Loss: 0.0717
Epoch [186/300], Loss: 0.0545
Epoch [187/300], Loss: 0.0538
Epoch [188/300], Loss: 0.0549
Epoch [189/300], Loss: 0.0538
Epoch [190/300], Loss: 0.0529
-- Evaluation after Epoch [190/300], Test Loss: 0.0816
Epoch [191/300], Loss: 0.0522
Epoch [192/300], Loss: 0.0520
Epoch [193/300], Loss: 0.0523
Epoch [194/300], Loss: 0.0516
Epoch [195/300], Loss: 0.0521
-- Evaluation after Epoch [195/300], Test Loss: 0.0714
Epoch [196/300], Loss: 0.0519
Epoch [197/300], Loss: 0.0539
Epoch [198/300], Loss: 0.0541
Epoch [199/300], Loss: 0.0520
Epoch [200/300], Loss: 0.0526
-- Evaluation after Epoch [200/300], Test Loss: 0.0766
Epoch [201/300], Loss: 0.0524
Epoch [202/300], Loss: 0.0515
Epoch [203/300], Loss: 0.0512
Epoch [204/300], Loss: 0.0513
Epoch [205/300], Loss: 0.0540
-- Evaluation after Epoch [205/300], Test Loss: 0.0750
Epoch [206/300], Loss: 0.0520
Epoch [207/300], Loss: 0.0514
Epoch [208/300], Loss: 0.0517
Epoch [209/300], Loss: 0.0502
Epoch [210/300], Loss: 0.0496
-- Evaluation after Epoch [210/300], Test Loss: 0.0756
Epoch [211/300], Loss: 0.0503
Epoch [212/300], Loss: 0.0507
Epoch [213/300], Loss: 0.0502
Epoch [214/300], Loss: 0.0503
Epoch [215/300], Loss: 0.0520
-- Evaluation after Epoch [215/300], Test Loss: 0.0808
Epoch [216/300], Loss: 0.0509
Epoch [217/300], Loss: 0.0558
Epoch [218/300], Loss: 0.0529
Epoch [219/300], Loss: 0.0532
Epoch [220/300], Loss: 0.0513
-- Evaluation after Epoch [220/300], Test Loss: 0.0761
Epoch [221/300], Loss: 0.0507
Epoch [222/300], Loss: 0.0515
Epoch [223/300], Loss: 0.0513
Epoch [224/300], Loss: 0.0501
Epoch [225/300], Loss: 0.0502
-- Evaluation after Epoch [225/300], Test Loss: 0.0796
Epoch [226/300], Loss: 0.0503
Epoch [227/300], Loss: 0.0492
Epoch [228/300], Loss: 0.0489
Epoch [229/300], Loss: 0.0493
Epoch [230/300], Loss: 0.0496
-- Evaluation after Epoch [230/300], Test Loss: 0.0751
Epoch [231/300], Loss: 0.0495
Epoch [232/300], Loss: 0.0510
Epoch [233/300], Loss: 0.0521
Epoch [234/300], Loss: 0.0506
Epoch [235/300], Loss: 0.0500
-- Evaluation after Epoch [235/300], Test Loss: 0.0759
Epoch [236/300], Loss: 0.0495
Epoch [237/300], Loss: 0.0487
Epoch [238/300], Loss: 0.0479
Epoch [239/300], Loss: 0.0474
Epoch [240/300], Loss: 0.0475
-- Evaluation after Epoch [240/300], Test Loss: 0.0772
Epoch [241/300], Loss: 0.0470
Epoch [242/300], Loss: 0.0482
Epoch [243/300], Loss: 0.0482
Epoch [244/300], Loss: 0.0482
Epoch [245/300], Loss: 0.0482
-- Evaluation after Epoch [245/300], Test Loss: 0.0743
Epoch [246/300], Loss: 0.0479
Epoch [247/300], Loss: 0.0475
Epoch [248/300], Loss: 0.0481
Epoch [249/300], Loss: 0.0473
Epoch [250/300], Loss: 0.0469
-- Evaluation after Epoch [250/300], Test Loss: 0.0742
Epoch [251/300], Loss: 0.0469
Epoch [252/300], Loss: 0.0478
Epoch [253/300], Loss: 0.0484
Epoch [254/300], Loss: 0.0471
Epoch [255/300], Loss: 0.0468
-- Evaluation after Epoch [255/300], Test Loss: 0.0765
Epoch [256/300], Loss: 0.0471
Epoch [257/300], Loss: 0.0461
Epoch [258/300], Loss: 0.0462
Epoch [259/300], Loss: 0.0462
Epoch [260/300], Loss: 0.0464
-- Evaluation after Epoch [260/300], Test Loss: 0.0780
Epoch [261/300], Loss: 0.0490
Epoch [262/300], Loss: 0.0457
Epoch [263/300], Loss: 0.0468
Epoch [264/300], Loss: 0.0468
Epoch [265/300], Loss: 0.0461
-- Evaluation after Epoch [265/300], Test Loss: 0.0751
Epoch [266/300], Loss: 0.0460
Epoch [267/300], Loss: 0.0461
Epoch [268/300], Loss: 0.0462
Epoch [269/300], Loss: 0.0456
Epoch [270/300], Loss: 0.0458
-- Evaluation after Epoch [270/300], Test Loss: 0.0769
Epoch [271/300], Loss: 0.0459
Epoch [272/300], Loss: 0.0456
Epoch [273/300], Loss: 0.0446
Epoch [274/300], Loss: 0.0452
Epoch [275/300], Loss: 0.0453
-- Evaluation after Epoch [275/300], Test Loss: 0.0741
Epoch [276/300], Loss: 0.0449
Epoch [277/300], Loss: 0.0458
Epoch [278/300], Loss: 0.0445
Epoch [279/300], Loss: 0.0458
Epoch [280/300], Loss: 0.0462
-- Evaluation after Epoch [280/300], Test Loss: 0.0771
Epoch [281/300], Loss: 0.0448
Epoch [282/300], Loss: 0.0450
Epoch [283/300], Loss: 0.0454
Epoch [284/300], Loss: 0.0483
Epoch [285/300], Loss: 0.0467
-- Evaluation after Epoch [285/300], Test Loss: 0.0750
Epoch [286/300], Loss: 0.0456
Epoch [287/300], Loss: 0.0460
Epoch [288/300], Loss: 0.0465
Epoch [289/300], Loss: 0.0443
Epoch [290/300], Loss: 0.0451
-- Evaluation after Epoch [290/300], Test Loss: 0.0741
Epoch [291/300], Loss: 0.0446
Epoch [292/300], Loss: 0.0457
Epoch [293/300], Loss: 0.0448
Epoch [294/300], Loss: 0.0453
Epoch [295/300], Loss: 0.0442
-- Evaluation after Epoch [295/300], Test Loss: 0.0735
Epoch [296/300], Loss: 0.0438
Epoch [297/300], Loss: 0.0438
Epoch [298/300], Loss: 0.0438
Epoch [299/300], Loss: 0.0435
Epoch [300/300], Loss: 0.0440
-- Evaluation after Epoch [300/300], Test Loss: 0.0714
Loading the best model from checkpoint with test loss: 0.0645
Validation Loss (RMSE): 0.0645
Validation results saved to: ./Results\LSTM_Study_20250129_173126\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0645
Avg Test Loss after reverse scaling (RMSE): 48.0762
Test results saved to: ./Results\LSTM_Study_20250129_173126\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
96
64
./Results\LSTM_Study_20250129_174333
./Results\LSTM_Study_20250129_174333\Train
Config file saved to: ./Results\LSTM_Study_20250129_174333\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([321, 96, 11])
Train labels tensor shape: torch.Size([321, 24])
Val data tensor shape: torch.Size([69, 96, 11])
Val labels tensor shape: torch.Size([69, 24])
Test data tensor shape: torch.Size([69, 96, 11])
Test labels tensor shape: torch.Size([69, 24])
Epoch [1/20], Loss: 0.1620
Epoch [2/20], Loss: 0.1335
Epoch [3/20], Loss: 0.1182
Epoch [4/20], Loss: 0.1092
Epoch [5/20], Loss: 0.1026
-- Evaluation after Epoch [5/20], Test Loss: 0.1064
New best test loss: 0.1064. Saving model.
Epoch [6/20], Loss: 0.1019
Epoch [7/20], Loss: 0.0987
Epoch [8/20], Loss: 0.0954
Epoch [9/20], Loss: 0.0960
Epoch [10/20], Loss: 0.0930
-- Evaluation after Epoch [10/20], Test Loss: 0.1036
New best test loss: 0.1036. Saving model.
Epoch [11/20], Loss: 0.0922
Epoch [12/20], Loss: 0.0921
Epoch [13/20], Loss: 0.0897
Epoch [14/20], Loss: 0.0911
Epoch [15/20], Loss: 0.0907
-- Evaluation after Epoch [15/20], Test Loss: 0.0997
New best test loss: 0.0997. Saving model.
Epoch [16/20], Loss: 0.0951
Epoch [17/20], Loss: 0.0895
Epoch [18/20], Loss: 0.0882
Epoch [19/20], Loss: 0.0880
Epoch [20/20], Loss: 0.0889
-- Evaluation after Epoch [20/20], Test Loss: 0.0987
New best test loss: 0.0987. Saving model.
Loading the best model from checkpoint with test loss: 0.0987
Validation Loss (RMSE): 0.0987
Validation results saved to: ./Results\LSTM_Study_20250129_174333\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0987
Avg Test Loss after reverse scaling (RMSE): 72.3200
Test results saved to: ./Results\LSTM_Study_20250129_174333\Test\test_results.txt
