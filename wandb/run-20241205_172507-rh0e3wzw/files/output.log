Epoch [1/200], Loss: 0.0955
Epoch [2/200], Loss: 0.0853
Epoch [3/200], Loss: 0.0845
Epoch [4/200], Loss: 0.0841
Epoch [5/200], Loss: 0.0837
-- Evaluation after Epoch [5/200], Test Loss: 0.0784
New best test loss: 0.0784. Saving model.
Epoch [6/200], Loss: 0.0834
Epoch [7/200], Loss: 0.0832
Epoch [8/200], Loss: 0.0830
Epoch [9/200], Loss: 0.0827
Epoch [10/200], Loss: 0.0826
-- Evaluation after Epoch [10/200], Test Loss: 0.0801
Epoch [11/200], Loss: 0.0824
Epoch [12/200], Loss: 0.0823
Epoch [13/200], Loss: 0.0822
Epoch [14/200], Loss: 0.0820
Epoch [15/200], Loss: 0.0820
-- Evaluation after Epoch [15/200], Test Loss: 0.0832
Epoch [16/200], Loss: 0.0819
Epoch [17/200], Loss: 0.0818
Epoch [18/200], Loss: 0.0817
Epoch [19/200], Loss: 0.0817
Epoch [20/200], Loss: 0.0817
-- Evaluation after Epoch [20/200], Test Loss: 0.0851
Epoch [21/200], Loss: 0.0816
Epoch [22/200], Loss: 0.0816
Epoch [23/200], Loss: 0.0816
Epoch [24/200], Loss: 0.0816
Epoch [25/200], Loss: 0.0815
-- Evaluation after Epoch [25/200], Test Loss: 0.0868
Epoch [26/200], Loss: 0.0815
Epoch [27/200], Loss: 0.0815
Epoch [28/200], Loss: 0.0814
Epoch [29/200], Loss: 0.0815
Epoch [30/200], Loss: 0.0814
-- Evaluation after Epoch [30/200], Test Loss: 0.0868
Epoch [31/200], Loss: 0.0814
Epoch [32/200], Loss: 0.0814
Epoch [33/200], Loss: 0.0813
Epoch [34/200], Loss: 0.0812
Epoch [35/200], Loss: 0.0812
-- Evaluation after Epoch [35/200], Test Loss: 0.0865
Epoch [36/200], Loss: 0.0811
Epoch [37/200], Loss: 0.0811
Epoch [38/200], Loss: 0.0810
Epoch [39/200], Loss: 0.0810
Epoch [40/200], Loss: 0.0809
-- Evaluation after Epoch [40/200], Test Loss: 0.0827
Epoch [41/200], Loss: 0.0809
Epoch [42/200], Loss: 0.0808
Epoch [43/200], Loss: 0.0808
Epoch [44/200], Loss: 0.0808
Epoch [45/200], Loss: 0.0807
-- Evaluation after Epoch [45/200], Test Loss: 0.0819
Epoch [46/200], Loss: 0.0807
Epoch [47/200], Loss: 0.0807
Epoch [48/200], Loss: 0.0807
Epoch [49/200], Loss: 0.0806
Epoch [50/200], Loss: 0.0806
-- Evaluation after Epoch [50/200], Test Loss: 0.0810
Epoch [51/200], Loss: 0.0806
Epoch [52/200], Loss: 0.0806
Epoch [53/200], Loss: 0.0806
Epoch [54/200], Loss: 0.0806
Epoch [55/200], Loss: 0.0806
-- Evaluation after Epoch [55/200], Test Loss: 0.0809
Epoch [56/200], Loss: 0.0806
Epoch [57/200], Loss: 0.0806
Epoch [58/200], Loss: 0.0806
Epoch [59/200], Loss: 0.0806
Epoch [60/200], Loss: 0.0806
-- Evaluation after Epoch [60/200], Test Loss: 0.0807
Epoch [61/200], Loss: 0.0806
Epoch [62/200], Loss: 0.0806
Epoch [63/200], Loss: 0.0806
Epoch [64/200], Loss: 0.0806
Epoch [65/200], Loss: 0.0806
-- Evaluation after Epoch [65/200], Test Loss: 0.0806
Epoch [66/200], Loss: 0.0806
Epoch [67/200], Loss: 0.0806
Epoch [68/200], Loss: 0.0806
Epoch [69/200], Loss: 0.0806
Epoch [70/200], Loss: 0.0806
-- Evaluation after Epoch [70/200], Test Loss: 0.0799
Epoch [71/200], Loss: 0.0806
Epoch [72/200], Loss: 0.0806
Epoch [73/200], Loss: 0.0806
Epoch [74/200], Loss: 0.0806
Epoch [75/200], Loss: 0.0805
-- Evaluation after Epoch [75/200], Test Loss: 0.0803
Epoch [76/200], Loss: 0.0805
Epoch [77/200], Loss: 0.0806
Epoch [78/200], Loss: 0.0806
Epoch [79/200], Loss: 0.0805
Epoch [80/200], Loss: 0.0805
-- Evaluation after Epoch [80/200], Test Loss: 0.0800
Epoch [81/200], Loss: 0.0805
Epoch [82/200], Loss: 0.0805
Epoch [83/200], Loss: 0.0805
Epoch [84/200], Loss: 0.0806
Epoch [85/200], Loss: 0.0806
-- Evaluation after Epoch [85/200], Test Loss: 0.0798
Epoch [86/200], Loss: 0.0805
Epoch [87/200], Loss: 0.0805
Epoch [88/200], Loss: 0.0805
Epoch [89/200], Loss: 0.0805
Epoch [90/200], Loss: 0.0805
-- Evaluation after Epoch [90/200], Test Loss: 0.0794
Epoch [91/200], Loss: 0.0805
Epoch [92/200], Loss: 0.0805
Epoch [93/200], Loss: 0.0805
Epoch [94/200], Loss: 0.0805
Epoch [95/200], Loss: 0.0805
-- Evaluation after Epoch [95/200], Test Loss: 0.0798
Epoch [96/200], Loss: 0.0805
Epoch [97/200], Loss: 0.0805
Epoch [98/200], Loss: 0.0806
Epoch [99/200], Loss: 0.0805
Epoch [100/200], Loss: 0.0805
-- Evaluation after Epoch [100/200], Test Loss: 0.0798
Epoch [101/200], Loss: 0.0805
Epoch [102/200], Loss: 0.0805
Epoch [103/200], Loss: 0.0805
Epoch [104/200], Loss: 0.0805
Epoch [105/200], Loss: 0.0805
-- Evaluation after Epoch [105/200], Test Loss: 0.0797
Epoch [106/200], Loss: 0.0805
Epoch [107/200], Loss: 0.0805
Epoch [108/200], Loss: 0.0805
Epoch [109/200], Loss: 0.0805
Epoch [110/200], Loss: 0.0805
-- Evaluation after Epoch [110/200], Test Loss: 0.0796
Epoch [111/200], Loss: 0.0805
Epoch [112/200], Loss: 0.0805
Epoch [113/200], Loss: 0.0805
Epoch [114/200], Loss: 0.0805
Epoch [115/200], Loss: 0.0805
-- Evaluation after Epoch [115/200], Test Loss: 0.0794
Epoch [116/200], Loss: 0.0805
Epoch [117/200], Loss: 0.0805
Epoch [118/200], Loss: 0.0805
Epoch [119/200], Loss: 0.0805
Epoch [120/200], Loss: 0.0805
-- Evaluation after Epoch [120/200], Test Loss: 0.0792
Epoch [121/200], Loss: 0.0804
Epoch [122/200], Loss: 0.0805
Epoch [123/200], Loss: 0.0805
Epoch [124/200], Loss: 0.0805
Epoch [125/200], Loss: 0.0805
-- Evaluation after Epoch [125/200], Test Loss: 0.0790
Epoch [126/200], Loss: 0.0804
Epoch [127/200], Loss: 0.0805
Epoch [128/200], Loss: 0.0804
Epoch [129/200], Loss: 0.0804
Epoch [130/200], Loss: 0.0804
-- Evaluation after Epoch [130/200], Test Loss: 0.0790
Epoch [131/200], Loss: 0.0804
Epoch [132/200], Loss: 0.0804
Epoch [133/200], Loss: 0.0804
Epoch [134/200], Loss: 0.0804
Epoch [135/200], Loss: 0.0804
-- Evaluation after Epoch [135/200], Test Loss: 0.0789
Epoch [136/200], Loss: 0.0804
Epoch [137/200], Loss: 0.0804
Epoch [138/200], Loss: 0.0804
Epoch [139/200], Loss: 0.0804
Epoch [140/200], Loss: 0.0805
-- Evaluation after Epoch [140/200], Test Loss: 0.0791
Epoch [141/200], Loss: 0.0804
Epoch [142/200], Loss: 0.0804
Epoch [143/200], Loss: 0.0804
Epoch [144/200], Loss: 0.0804
Epoch [145/200], Loss: 0.0804
-- Evaluation after Epoch [145/200], Test Loss: 0.0789
Epoch [146/200], Loss: 0.0804
Epoch [147/200], Loss: 0.0804
Epoch [148/200], Loss: 0.0804
Epoch [149/200], Loss: 0.0804
Epoch [150/200], Loss: 0.0804
-- Evaluation after Epoch [150/200], Test Loss: 0.0789
Epoch [151/200], Loss: 0.0804
Epoch [152/200], Loss: 0.0804
Epoch [153/200], Loss: 0.0804
Epoch [154/200], Loss: 0.0804
Epoch [155/200], Loss: 0.0804
-- Evaluation after Epoch [155/200], Test Loss: 0.0789
Epoch [156/200], Loss: 0.0804
Epoch [157/200], Loss: 0.0804
Epoch [158/200], Loss: 0.0804
Epoch [159/200], Loss: 0.0804
Epoch [160/200], Loss: 0.0804
-- Evaluation after Epoch [160/200], Test Loss: 0.0788
Epoch [161/200], Loss: 0.0804
Epoch [162/200], Loss: 0.0804
Epoch [163/200], Loss: 0.0804
Epoch [164/200], Loss: 0.0805
Epoch [165/200], Loss: 0.0804
-- Evaluation after Epoch [165/200], Test Loss: 0.0790
Epoch [166/200], Loss: 0.0804
Epoch [167/200], Loss: 0.0804
Epoch [168/200], Loss: 0.0804
Epoch [169/200], Loss: 0.0804
Epoch [170/200], Loss: 0.0804
-- Evaluation after Epoch [170/200], Test Loss: 0.0790
Epoch [171/200], Loss: 0.0804
Epoch [172/200], Loss: 0.0804
Epoch [173/200], Loss: 0.0804
Epoch [174/200], Loss: 0.0804
Epoch [175/200], Loss: 0.0804
-- Evaluation after Epoch [175/200], Test Loss: 0.0791
Epoch [176/200], Loss: 0.0804
Epoch [177/200], Loss: 0.0804
Epoch [178/200], Loss: 0.0804
Epoch [179/200], Loss: 0.0804
Epoch [180/200], Loss: 0.0804
-- Evaluation after Epoch [180/200], Test Loss: 0.0789
Epoch [181/200], Loss: 0.0804
Epoch [182/200], Loss: 0.0804
Epoch [183/200], Loss: 0.0804
Epoch [184/200], Loss: 0.0804
Epoch [185/200], Loss: 0.0804
-- Evaluation after Epoch [185/200], Test Loss: 0.0790
Epoch [186/200], Loss: 0.0804
Epoch [187/200], Loss: 0.0804
Epoch [188/200], Loss: 0.0804
Epoch [189/200], Loss: 0.0804
Epoch [190/200], Loss: 0.0804
-- Evaluation after Epoch [190/200], Test Loss: 0.0786
Epoch [191/200], Loss: 0.0805
Epoch [192/200], Loss: 0.0804
Epoch [193/200], Loss: 0.0804
Epoch [194/200], Loss: 0.0804
Epoch [195/200], Loss: 0.0804
-- Evaluation after Epoch [195/200], Test Loss: 0.0787
Epoch [196/200], Loss: 0.0804
Epoch [197/200], Loss: 0.0804
Epoch [198/200], Loss: 0.0804
Epoch [199/200], Loss: 0.0804
Epoch [200/200], Loss: 0.0804
-- Evaluation after Epoch [200/200], Test Loss: 0.0787
Loading the best model from checkpoint with test loss: 0.0784
Validation Loss (RMSE): 0.0511
Validation results saved to: ./Results\LSTMStudy_20241205_172406\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0784
Avg Test Loss after reverse scaling (RMSE): 67.1662
Test results saved to: ./Results\LSTMStudy_20241205_172406\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
96
64
./Results\LSTMStudy_20241205_174544
./Results\LSTMStudy_20241205_174544\Train
Config file saved to: ./Results\LSTMStudy_20241205_174544\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
PM index during scaling is: 1
Train data tensor shape: torch.Size([35387, 96, 1])
Train labels tensor shape: torch.Size([35387, 1])
Val data tensor shape: torch.Size([3705, 96, 1])
Val labels tensor shape: torch.Size([3705, 1])
Test data tensor shape: torch.Size([11310, 96, 1])
Test labels tensor shape: torch.Size([11310, 1])
Epoch [1/10], Loss: 0.0832
Epoch [2/10], Loss: 0.0741
Epoch [3/10], Loss: 0.0724
Epoch [4/10], Loss: 0.0712
Epoch [5/10], Loss: 0.0704
-- Evaluation after Epoch [5/10], Test Loss: 0.1075
New best test loss: 0.1075. Saving model.
Epoch [6/10], Loss: 0.0695
Epoch [7/10], Loss: 0.0687
Epoch [8/10], Loss: 0.0685
Epoch [9/10], Loss: 0.0685
Epoch [10/10], Loss: 0.0679
-- Evaluation after Epoch [10/10], Test Loss: 0.1236
Loading the best model from checkpoint with test loss: 0.1075
Validation Loss (RMSE): 0.1123
Validation results saved to: ./Results\LSTMStudy_20241205_174544\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1075
Avg Test Loss after reverse scaling (RMSE): 97.9323
Test results saved to: ./Results\LSTMStudy_20241205_174544\Test\test_results.txt
