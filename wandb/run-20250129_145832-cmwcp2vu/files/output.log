Epoch [1/10], Loss: 0.2890
Epoch [2/10], Loss: 0.2236
Epoch [3/10], Loss: 0.1741
Epoch [4/10], Loss: 0.1382
Epoch [5/10], Loss: 0.1144
-- Evaluation after Epoch [5/10], Test Loss: 0.0983
New best test loss: 0.0983. Saving model.
Epoch [6/10], Loss: 0.1025
Epoch [7/10], Loss: 0.0986
Epoch [8/10], Loss: 0.0986
Epoch [9/10], Loss: 0.0975
Epoch [10/10], Loss: 0.0973
-- Evaluation after Epoch [10/10], Test Loss: 0.0999
Loading the best model from checkpoint with test loss: 0.0983
Validation Loss (RMSE): 0.0983
Validation results saved to: ./Results\LSTM_Study_20250129_145812\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0983
Avg Test Loss after reverse scaling (RMSE): 72.1603
Test results saved to: ./Results\LSTM_Study_20250129_145812\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
> [1;32mc:\users\bhargav bade\multivar lstm\train_test_sng_step.py[0m(6)[0;36m<module>[1;34m()[0m
[1;32m      4 [1;33m[1;31m# In[34]:[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;33m[0m[0m
[0m[1;32m----> 6 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      7 [1;33m[1;32mimport[0m [0mtorch[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      8 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m
48
64
./Results\LSTM_Study_20250129_150126
./Results\LSTM_Study_20250129_150126\Train
Config file saved to: ./Results\LSTM_Study_20250129_150126\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
[4;33mReloaded modules[24m: params[0m
48
64
./Results\LSTM_Study_20250129_151215
./Results\LSTM_Study_20250129_151215\Train
Config file saved to: ./Results\LSTM_Study_20250129_151215\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.2853
Epoch [2/10], Loss: 0.2107
Epoch [3/10], Loss: 0.1507
Epoch [4/10], Loss: 0.1117
Epoch [5/10], Loss: 0.0979
-- Evaluation after Epoch [5/10], Test Loss: 0.1047
New best test loss: 0.1047. Saving model.
Epoch [6/10], Loss: 0.0987
Epoch [7/10], Loss: 0.0991
Epoch [8/10], Loss: 0.0975
Epoch [9/10], Loss: 0.0950
Epoch [10/10], Loss: 0.0929
-- Evaluation after Epoch [10/10], Test Loss: 0.0947
New best test loss: 0.0947. Saving model.
Loading the best model from checkpoint with test loss: 0.0947
Validation Loss (RMSE): 0.0947
Validation results saved to: ./Results\LSTM_Study_20250129_151215\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0947
Avg Test Loss after reverse scaling (RMSE): 72.4024
Test results saved to: ./Results\LSTM_Study_20250129_151215\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_151437
./Results\LSTM_Study_20250129_151437\Train
Config file saved to: ./Results\LSTM_Study_20250129_151437\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.3421
Epoch [2/10], Loss: 0.2596
Epoch [3/10], Loss: 0.1946
Epoch [4/10], Loss: 0.1478
Epoch [5/10], Loss: 0.1150
-- Evaluation after Epoch [5/10], Test Loss: 0.1011
New best test loss: 0.1011. Saving model.
Epoch [6/10], Loss: 0.1020
Epoch [7/10], Loss: 0.1014
Epoch [8/10], Loss: 0.1013
Epoch [9/10], Loss: 0.1015
Epoch [10/10], Loss: 0.0986
-- Evaluation after Epoch [10/10], Test Loss: 0.0983
New best test loss: 0.0983. Saving model.
Loading the best model from checkpoint with test loss: 0.0983
Validation Loss (RMSE): 0.0983
Validation results saved to: ./Results\LSTM_Study_20250129_151437\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0983
Avg Test Loss after reverse scaling (RMSE): 76.9409
Test results saved to: ./Results\LSTM_Study_20250129_151437\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_151646
./Results\LSTM_Study_20250129_151646\Train
Config file saved to: ./Results\LSTM_Study_20250129_151646\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.3383
Epoch [2/10], Loss: 0.2624
Epoch [3/10], Loss: 0.2010
Epoch [4/10], Loss: 0.1568
Epoch [5/10], Loss: 0.1288
-- Evaluation after Epoch [5/10], Test Loss: 0.1093
New best test loss: 0.1093. Saving model.
Epoch [6/10], Loss: 0.1089
Epoch [7/10], Loss: 0.0993
Epoch [8/10], Loss: 0.0973
Epoch [9/10], Loss: 0.0970
Epoch [10/10], Loss: 0.0967
-- Evaluation after Epoch [10/10], Test Loss: 0.0996
New best test loss: 0.0996. Saving model.
Loading the best model from checkpoint with test loss: 0.0996
Validation Loss (RMSE): 0.0996
Validation results saved to: ./Results\LSTM_Study_20250129_151646\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0996
Avg Test Loss after reverse scaling (RMSE): 78.6506
Test results saved to: ./Results\LSTM_Study_20250129_151646\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
> [1;32mc:\users\bhargav bade\multivar lstm\train_test_sng_step.py[0m(6)[0;36m<module>[1;34m()[0m
[1;32m      4 [1;33m[1;31m# In[34]:[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;33m[0m[0m
[0m[1;32m----> 6 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      7 [1;33m[1;32mimport[0m [0mtorch[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      8 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m
48
64
./Results\LSTM_Study_20250129_152130
./Results\LSTM_Study_20250129_152130\Train
Config file saved to: ./Results\LSTM_Study_20250129_152130\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.3143
Epoch [2/10], Loss: 0.2457
Epoch [3/10], Loss: 0.1886
Epoch [4/10], Loss: 0.1460
Epoch [5/10], Loss: 0.1191
-- Evaluation after Epoch [5/10], Test Loss: 0.1022
New best test loss: 0.1022. Saving model.
Epoch [6/10], Loss: 0.1044
Epoch [7/10], Loss: 0.1000
Epoch [8/10], Loss: 0.1007
Epoch [9/10], Loss: 0.0993
Epoch [10/10], Loss: 0.0988
-- Evaluation after Epoch [10/10], Test Loss: 0.0997
New best test loss: 0.0997. Saving model.
Loading the best model from checkpoint with test loss: 0.0997
Validation Loss (RMSE): 0.0997
Validation results saved to: ./Results\LSTM_Study_20250129_152130\Val\validation_results.txt

Program interrupted. (Use 'cont' to resume).
--Return--
Avg Test Loss before reverse scaling(RMSE): 0.0997
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
> [1;32mc:\users\bhargav bade\multivar lstm\train_test_sng_step.py[0m(6)[0;36m<module>[1;34m()[0m
[1;32m      4 [1;33m[1;31m# In[34]:[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      5 [1;33m[1;33m[0m[0m
[0m[1;32m----> 6 [1;33m[1;32mimport[0m [0mos[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      7 [1;33m[1;32mimport[0m [0mtorch[0m[1;33m[0m[1;33m[0m[0m
[0m[1;32m      8 [1;33m[1;32mimport[0m [0mnumpy[0m [1;32mas[0m [0mnp[0m[1;33m[0m[1;33m[0m[0m
[0m
48
64
./Results\LSTM_Study_20250129_152218
./Results\LSTM_Study_20250129_152218\Train
Config file saved to: ./Results\LSTM_Study_20250129_152218\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_153044
./Results\LSTM_Study_20250129_153044\Train
Config file saved to: ./Results\LSTM_Study_20250129_153044\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.3876
Epoch [2/10], Loss: 0.3060
Epoch [3/10], Loss: 0.2365
Epoch [4/10], Loss: 0.1797
Epoch [5/10], Loss: 0.1361
-- Evaluation after Epoch [5/10], Test Loss: 0.1102
New best test loss: 0.1102. Saving model.
Epoch [6/10], Loss: 0.1125
Epoch [7/10], Loss: 0.1016
Epoch [8/10], Loss: 0.1034
Epoch [9/10], Loss: 0.1024
Epoch [10/10], Loss: 0.1018
-- Evaluation after Epoch [10/10], Test Loss: 0.1008
New best test loss: 0.1008. Saving model.
Loading the best model from checkpoint with test loss: 0.1008
Validation Loss (RMSE): 0.1008
Validation results saved to: ./Results\LSTM_Study_20250129_153044\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1008
Avg Test Loss after reverse scaling (RMSE): 80.2646
Test results saved to: ./Results\LSTM_Study_20250129_153044\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_153435
./Results\LSTM_Study_20250129_153435\Train
Config file saved to: ./Results\LSTM_Study_20250129_153435\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.3819
Epoch [2/10], Loss: 0.3120
Epoch [3/10], Loss: 0.2577
Epoch [4/10], Loss: 0.2203
Epoch [5/10], Loss: 0.1893
-- Evaluation after Epoch [5/10], Test Loss: 0.1567
New best test loss: 0.1567. Saving model.
Epoch [6/10], Loss: 0.1646
Epoch [7/10], Loss: 0.1480
Epoch [8/10], Loss: 0.1383
Epoch [9/10], Loss: 0.1269
Epoch [10/10], Loss: 0.1197
-- Evaluation after Epoch [10/10], Test Loss: 0.1016
New best test loss: 0.1016. Saving model.
Loading the best model from checkpoint with test loss: 0.1016
Validation Loss (RMSE): 0.1016
Validation results saved to: ./Results\LSTM_Study_20250129_153435\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.1016
Avg Test Loss after reverse scaling (RMSE): 80.1269
Test results saved to: ./Results\LSTM_Study_20250129_153435\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_154235
./Results\LSTM_Study_20250129_154235\Train
Config file saved to: ./Results\LSTM_Study_20250129_154235\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.2830
Epoch [2/10], Loss: 0.2073
Epoch [3/10], Loss: 0.1543
Epoch [4/10], Loss: 0.1221
Epoch [5/10], Loss: 0.1028
-- Evaluation after Epoch [5/10], Test Loss: 0.0977
New best test loss: 0.0977. Saving model.
Epoch [6/10], Loss: 0.0956
Epoch [7/10], Loss: 0.0954
Epoch [8/10], Loss: 0.0969
Epoch [9/10], Loss: 0.0963
Epoch [10/10], Loss: 0.0931
-- Evaluation after Epoch [10/10], Test Loss: 0.0971
New best test loss: 0.0971. Saving model.
Loading the best model from checkpoint with test loss: 0.0971
Validation Loss (RMSE): 0.0971
Validation results saved to: ./Results\LSTM_Study_20250129_154235\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0971
Avg Test Loss after reverse scaling (RMSE): 76.0720
Test results saved to: ./Results\LSTM_Study_20250129_154235\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_160907
./Results\LSTM_Study_20250129_160907\Train
Config file saved to: ./Results\LSTM_Study_20250129_160907\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.3198
Epoch [2/10], Loss: 0.2444
Epoch [3/10], Loss: 0.1871
Epoch [4/10], Loss: 0.1462
Epoch [5/10], Loss: 0.1159
-- Evaluation after Epoch [5/10], Test Loss: 0.1010
New best test loss: 0.1010. Saving model.
Epoch [6/10], Loss: 0.0992
Epoch [7/10], Loss: 0.0952
Epoch [8/10], Loss: 0.0977
Epoch [9/10], Loss: 0.0974
Epoch [10/10], Loss: 0.0937
-- Evaluation after Epoch [10/10], Test Loss: 0.0980
New best test loss: 0.0980. Saving model.
Loading the best model from checkpoint with test loss: 0.0980
Validation Loss (RMSE): 0.0980
Validation results saved to: ./Results\LSTM_Study_20250129_160907\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0980
Avg Test Loss after reverse scaling (RMSE): 76.7324
Test results saved to: ./Results\LSTM_Study_20250129_160907\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_161327
./Results\LSTM_Study_20250129_161327\Train
Config file saved to: ./Results\LSTM_Study_20250129_161327\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.2778
Epoch [2/10], Loss: 0.2110
Epoch [3/10], Loss: 0.1640
Epoch [4/10], Loss: 0.1310
Epoch [5/10], Loss: 0.1116
-- Evaluation after Epoch [5/10], Test Loss: 0.1047
New best test loss: 0.1047. Saving model.
Epoch [6/10], Loss: 0.1027
Epoch [7/10], Loss: 0.1019
Epoch [8/10], Loss: 0.1012
Epoch [9/10], Loss: 0.0998
Epoch [10/10], Loss: 0.0966
-- Evaluation after Epoch [10/10], Test Loss: 0.0954
New best test loss: 0.0954. Saving model.
Loading the best model from checkpoint with test loss: 0.0954
Validation Loss (RMSE): 0.0954
Validation results saved to: ./Results\LSTM_Study_20250129_161327\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0954
Avg Test Loss after reverse scaling (RMSE): 73.6123
Test results saved to: ./Results\LSTM_Study_20250129_161327\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
48
64
./Results\LSTM_Study_20250129_161431
./Results\LSTM_Study_20250129_161431\Train
Config file saved to: ./Results\LSTM_Study_20250129_161431\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 979.0 (Min: 1.0, Max: 980.0)
Avg of PM values: 101.82727551960684
PM index during scaling is: 8
Train data tensor shape: torch.Size([282, 48, 11])
Train labels tensor shape: torch.Size([282, 24])
Val data tensor shape: torch.Size([95, 48, 11])
Val labels tensor shape: torch.Size([95, 24])
Test data tensor shape: torch.Size([95, 48, 11])
Test labels tensor shape: torch.Size([95, 24])
Epoch [1/10], Loss: 0.3152
Epoch [2/10], Loss: 0.2406
Epoch [3/10], Loss: 0.1823
Epoch [4/10], Loss: 0.1400
Epoch [5/10], Loss: 0.1154
-- Evaluation after Epoch [5/10], Test Loss: 0.0981
New best test loss: 0.0981. Saving model.
Epoch [6/10], Loss: 0.1030
Epoch [7/10], Loss: 0.1004
Epoch [8/10], Loss: 0.1017
Epoch [9/10], Loss: 0.1001
Epoch [10/10], Loss: 0.0964
-- Evaluation after Epoch [10/10], Test Loss: 0.0980
New best test loss: 0.0980. Saving model.
Loading the best model from checkpoint with test loss: 0.0980
Validation Loss (RMSE): 0.0980
Validation results saved to: ./Results\LSTM_Study_20250129_161431\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0980
Avg Test Loss after reverse scaling (RMSE): 76.9458
Test results saved to: ./Results\LSTM_Study_20250129_161431\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
196
64
./Results\LSTM_Study_20250129_161903
./Results\LSTM_Study_20250129_161903\Train
Config file saved to: ./Results\LSTM_Study_20250129_161903\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 96.99694239082635
PM index during scaling is: 8
Train data tensor shape: torch.Size([1163, 196, 11])
Train labels tensor shape: torch.Size([1163, 24])
Val data tensor shape: torch.Size([393, 196, 11])
Val labels tensor shape: torch.Size([393, 24])
Test data tensor shape: torch.Size([393, 196, 11])
Test labels tensor shape: torch.Size([393, 24])
Epoch [1/50], Loss: 0.1619
Epoch [2/50], Loss: 0.1006
Epoch [3/50], Loss: 0.0951
Epoch [4/50], Loss: 0.0930
Epoch [5/50], Loss: 0.0923
-- Evaluation after Epoch [5/50], Test Loss: 0.0832
New best test loss: 0.0832. Saving model.
Epoch [6/50], Loss: 0.0916
Epoch [7/50], Loss: 0.0900
Epoch [8/50], Loss: 0.0887
Epoch [9/50], Loss: 0.0874
Epoch [10/50], Loss: 0.0869
-- Evaluation after Epoch [10/50], Test Loss: 0.0788
New best test loss: 0.0788. Saving model.
Epoch [11/50], Loss: 0.0863
Epoch [12/50], Loss: 0.0857
Epoch [13/50], Loss: 0.0852
Epoch [14/50], Loss: 0.0852
Epoch [15/50], Loss: 0.0847
-- Evaluation after Epoch [15/50], Test Loss: 0.0771
New best test loss: 0.0771. Saving model.
Epoch [16/50], Loss: 0.0845
Epoch [17/50], Loss: 0.0849
Epoch [18/50], Loss: 0.0846
Epoch [19/50], Loss: 0.0844
Epoch [20/50], Loss: 0.0846
-- Evaluation after Epoch [20/50], Test Loss: 0.0770
New best test loss: 0.0770. Saving model.
Epoch [21/50], Loss: 0.0853
Epoch [22/50], Loss: 0.0848
Epoch [23/50], Loss: 0.0848
Epoch [24/50], Loss: 0.0841
Epoch [25/50], Loss: 0.0850
-- Evaluation after Epoch [25/50], Test Loss: 0.0737
New best test loss: 0.0737. Saving model.
Epoch [26/50], Loss: 0.0841
Epoch [27/50], Loss: 0.0850
Epoch [28/50], Loss: 0.0840
Epoch [29/50], Loss: 0.0845
Epoch [30/50], Loss: 0.0833
-- Evaluation after Epoch [30/50], Test Loss: 0.0737
Epoch [31/50], Loss: 0.0825
Epoch [32/50], Loss: 0.0825
Epoch [33/50], Loss: 0.0826
Epoch [34/50], Loss: 0.0822
Epoch [35/50], Loss: 0.0822
-- Evaluation after Epoch [35/50], Test Loss: 0.0728
New best test loss: 0.0728. Saving model.
Epoch [36/50], Loss: 0.0824
Epoch [37/50], Loss: 0.0820
Epoch [38/50], Loss: 0.0819
Epoch [39/50], Loss: 0.0801
Epoch [40/50], Loss: 0.0810
-- Evaluation after Epoch [40/50], Test Loss: 0.0716
New best test loss: 0.0716. Saving model.
Epoch [41/50], Loss: 0.0807
Epoch [42/50], Loss: 0.0812
Epoch [43/50], Loss: 0.0806
Epoch [44/50], Loss: 0.0798
Epoch [45/50], Loss: 0.0796
-- Evaluation after Epoch [45/50], Test Loss: 0.0722
Epoch [46/50], Loss: 0.0794
Epoch [47/50], Loss: 0.0792
Epoch [48/50], Loss: 0.0795
Epoch [49/50], Loss: 0.0798
Epoch [50/50], Loss: 0.0786
-- Evaluation after Epoch [50/50], Test Loss: 0.0714
New best test loss: 0.0714. Saving model.
Loading the best model from checkpoint with test loss: 0.0714
Validation Loss (RMSE): 0.0714
Validation results saved to: ./Results\LSTM_Study_20250129_161903\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0714
Avg Test Loss after reverse scaling (RMSE): 50.7392
Test results saved to: ./Results\LSTM_Study_20250129_161903\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
120
64
./Results\LSTM_Study_20250129_162504
./Results\LSTM_Study_20250129_162504\Train
Config file saved to: ./Results\LSTM_Study_20250129_162504\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([1473, 120, 11])
Train labels tensor shape: torch.Size([1473, 24])
Val data tensor shape: torch.Size([501, 120, 11])
Val labels tensor shape: torch.Size([501, 24])
Test data tensor shape: torch.Size([501, 120, 11])
Test labels tensor shape: torch.Size([501, 24])
Epoch [1/50], Loss: 0.3516
Epoch [2/50], Loss: 0.3075
Epoch [3/50], Loss: 0.2685
Epoch [4/50], Loss: 0.2330
Epoch [5/50], Loss: 0.2018
-- Evaluation after Epoch [5/50], Test Loss: 0.1735
New best test loss: 0.1735. Saving model.
Epoch [6/50], Loss: 0.1769
Epoch [7/50], Loss: 0.1559
Epoch [8/50], Loss: 0.1393
Epoch [9/50], Loss: 0.1258
Epoch [10/50], Loss: 0.1159
-- Evaluation after Epoch [10/50], Test Loss: 0.0921
New best test loss: 0.0921. Saving model.
Epoch [11/50], Loss: 0.1100
Epoch [12/50], Loss: 0.1053
Epoch [13/50], Loss: 0.1034
Epoch [14/50], Loss: 0.1021
Epoch [15/50], Loss: 0.1012
-- Evaluation after Epoch [15/50], Test Loss: 0.0812
New best test loss: 0.0812. Saving model.
Epoch [16/50], Loss: 0.1000
Epoch [17/50], Loss: 0.0991
Epoch [18/50], Loss: 0.0984
Epoch [19/50], Loss: 0.0978
Epoch [20/50], Loss: 0.0973
-- Evaluation after Epoch [20/50], Test Loss: 0.0807
New best test loss: 0.0807. Saving model.
Epoch [21/50], Loss: 0.0963
Epoch [22/50], Loss: 0.0957
Epoch [23/50], Loss: 0.0952
Epoch [24/50], Loss: 0.0949
Epoch [25/50], Loss: 0.0944
-- Evaluation after Epoch [25/50], Test Loss: 0.0806
New best test loss: 0.0806. Saving model.
Epoch [26/50], Loss: 0.0941
Epoch [27/50], Loss: 0.0934
Epoch [28/50], Loss: 0.0932
Epoch [29/50], Loss: 0.0929
Epoch [30/50], Loss: 0.0927
-- Evaluation after Epoch [30/50], Test Loss: 0.0805
New best test loss: 0.0805. Saving model.
Epoch [31/50], Loss: 0.0922
Epoch [32/50], Loss: 0.0920
Epoch [33/50], Loss: 0.0919
Epoch [34/50], Loss: 0.0915
Epoch [35/50], Loss: 0.0913
-- Evaluation after Epoch [35/50], Test Loss: 0.0805
New best test loss: 0.0805. Saving model.
Epoch [36/50], Loss: 0.0912
Epoch [37/50], Loss: 0.0911
Epoch [38/50], Loss: 0.0909
Epoch [39/50], Loss: 0.0905
Epoch [40/50], Loss: 0.0901
-- Evaluation after Epoch [40/50], Test Loss: 0.0792
New best test loss: 0.0792. Saving model.
Epoch [41/50], Loss: 0.0894
Epoch [42/50], Loss: 0.0886
Epoch [43/50], Loss: 0.0883
Epoch [44/50], Loss: 0.0881
Epoch [45/50], Loss: 0.0881
-- Evaluation after Epoch [45/50], Test Loss: 0.0767
New best test loss: 0.0767. Saving model.
Epoch [46/50], Loss: 0.0882
Epoch [47/50], Loss: 0.0877
Epoch [48/50], Loss: 0.0874
Epoch [49/50], Loss: 0.0871
Epoch [50/50], Loss: 0.0869
-- Evaluation after Epoch [50/50], Test Loss: 0.0754
New best test loss: 0.0754. Saving model.
Loading the best model from checkpoint with test loss: 0.0754
Validation Loss (RMSE): 0.0754
Validation results saved to: ./Results\LSTM_Study_20250129_162504\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0754
Avg Test Loss after reverse scaling (RMSE): 59.8008
Test results saved to: ./Results\LSTM_Study_20250129_162504\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, prepare_data02, params[0m
120
64
./Results\LSTM_Study_20250129_162653
./Results\LSTM_Study_20250129_162653\Train
Config file saved to: ./Results\LSTM_Study_20250129_162653\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([1473, 120, 11])
Train labels tensor shape: torch.Size([1473, 24])
Val data tensor shape: torch.Size([501, 120, 11])
Val labels tensor shape: torch.Size([501, 24])
Test data tensor shape: torch.Size([501, 120, 11])
Test labels tensor shape: torch.Size([501, 24])
Epoch [1/250], Loss: 0.2165
Epoch [2/250], Loss: 0.1073
Epoch [3/250], Loss: 0.0996
Epoch [4/250], Loss: 0.0957
Epoch [5/250], Loss: 0.0935
-- Evaluation after Epoch [5/250], Test Loss: 0.0787
New best test loss: 0.0787. Saving model.
Epoch [6/250], Loss: 0.0925
Epoch [7/250], Loss: 0.0919
Epoch [8/250], Loss: 0.0917
Epoch [9/250], Loss: 0.0916
Epoch [10/250], Loss: 0.0916
-- Evaluation after Epoch [10/250], Test Loss: 0.0786
New best test loss: 0.0786. Saving model.
Epoch [11/250], Loss: 0.0916
Epoch [12/250], Loss: 0.0916
Epoch [13/250], Loss: 0.0915
Epoch [14/250], Loss: 0.0915
Epoch [15/250], Loss: 0.0915
-- Evaluation after Epoch [15/250], Test Loss: 0.0784
New best test loss: 0.0784. Saving model.
Epoch [16/250], Loss: 0.0916
Epoch [17/250], Loss: 0.0916
Epoch [18/250], Loss: 0.0915
Epoch [19/250], Loss: 0.0915
Epoch [20/250], Loss: 0.0915
-- Evaluation after Epoch [20/250], Test Loss: 0.0783
New best test loss: 0.0783. Saving model.
Epoch [21/250], Loss: 0.0916
Epoch [22/250], Loss: 0.0916
Epoch [23/250], Loss: 0.0915
Epoch [24/250], Loss: 0.0915
Epoch [25/250], Loss: 0.0915
-- Evaluation after Epoch [25/250], Test Loss: 0.0783
New best test loss: 0.0783. Saving model.
Epoch [26/250], Loss: 0.0915
Epoch [27/250], Loss: 0.0915
Epoch [28/250], Loss: 0.0915
Epoch [29/250], Loss: 0.0915
Epoch [30/250], Loss: 0.0915
-- Evaluation after Epoch [30/250], Test Loss: 0.0783
New best test loss: 0.0783. Saving model.
Epoch [31/250], Loss: 0.0915
Epoch [32/250], Loss: 0.0915
Epoch [33/250], Loss: 0.0915
Epoch [34/250], Loss: 0.0915
Epoch [35/250], Loss: 0.0915
-- Evaluation after Epoch [35/250], Test Loss: 0.0783
Epoch [36/250], Loss: 0.0915
Epoch [37/250], Loss: 0.0915
Epoch [38/250], Loss: 0.0915
Epoch [39/250], Loss: 0.0915
Epoch [40/250], Loss: 0.0915
-- Evaluation after Epoch [40/250], Test Loss: 0.0783
Epoch [41/250], Loss: 0.0914
Epoch [42/250], Loss: 0.0915
Epoch [43/250], Loss: 0.0915
Epoch [44/250], Loss: 0.0914
Epoch [45/250], Loss: 0.0915
-- Evaluation after Epoch [45/250], Test Loss: 0.0782
New best test loss: 0.0782. Saving model.
Epoch [46/250], Loss: 0.0914
Epoch [47/250], Loss: 0.0915
Epoch [48/250], Loss: 0.0915
Epoch [49/250], Loss: 0.0914
Epoch [50/250], Loss: 0.0914
-- Evaluation after Epoch [50/250], Test Loss: 0.0782
New best test loss: 0.0782. Saving model.
Epoch [51/250], Loss: 0.0913
Epoch [52/250], Loss: 0.0902
Epoch [53/250], Loss: 0.0890
Epoch [54/250], Loss: 0.0882
Epoch [55/250], Loss: 0.0874
-- Evaluation after Epoch [55/250], Test Loss: 0.0740
New best test loss: 0.0740. Saving model.
Epoch [56/250], Loss: 0.0866
Epoch [57/250], Loss: 0.0875
Epoch [58/250], Loss: 0.0856
Epoch [59/250], Loss: 0.0852
Epoch [60/250], Loss: 0.0843
-- Evaluation after Epoch [60/250], Test Loss: 0.0730
New best test loss: 0.0730. Saving model.
Epoch [61/250], Loss: 0.0840
Epoch [62/250], Loss: 0.0830
Epoch [63/250], Loss: 0.0838
Epoch [64/250], Loss: 0.0830
Epoch [65/250], Loss: 0.0824
-- Evaluation after Epoch [65/250], Test Loss: 0.0726
New best test loss: 0.0726. Saving model.
Epoch [66/250], Loss: 0.0825
Epoch [67/250], Loss: 0.0824
Epoch [68/250], Loss: 0.0820
Epoch [69/250], Loss: 0.0819
Epoch [70/250], Loss: 0.0810
-- Evaluation after Epoch [70/250], Test Loss: 0.0725
New best test loss: 0.0725. Saving model.
Epoch [71/250], Loss: 0.0818
Epoch [72/250], Loss: 0.0810
Epoch [73/250], Loss: 0.0804
Epoch [74/250], Loss: 0.0806
Epoch [75/250], Loss: 0.0802
-- Evaluation after Epoch [75/250], Test Loss: 0.0718
New best test loss: 0.0718. Saving model.
Epoch [76/250], Loss: 0.0798
Epoch [77/250], Loss: 0.0799
Epoch [78/250], Loss: 0.0796
Epoch [79/250], Loss: 0.0794
Epoch [80/250], Loss: 0.0797
-- Evaluation after Epoch [80/250], Test Loss: 0.0717
New best test loss: 0.0717. Saving model.
Epoch [81/250], Loss: 0.0800
Epoch [82/250], Loss: 0.0799
Epoch [83/250], Loss: 0.0793
Epoch [84/250], Loss: 0.0793
Epoch [85/250], Loss: 0.0788
-- Evaluation after Epoch [85/250], Test Loss: 0.0720
Epoch [86/250], Loss: 0.0791
Epoch [87/250], Loss: 0.0788
Epoch [88/250], Loss: 0.0789
Epoch [89/250], Loss: 0.0785
Epoch [90/250], Loss: 0.0784
-- Evaluation after Epoch [90/250], Test Loss: 0.0715
New best test loss: 0.0715. Saving model.
Epoch [91/250], Loss: 0.0783
Epoch [92/250], Loss: 0.0770
Epoch [93/250], Loss: 0.0780
Epoch [94/250], Loss: 0.0778
Epoch [95/250], Loss: 0.0769
-- Evaluation after Epoch [95/250], Test Loss: 0.0721
Epoch [96/250], Loss: 0.0767
Epoch [97/250], Loss: 0.0778
Epoch [98/250], Loss: 0.0784
Epoch [99/250], Loss: 0.0772
Epoch [100/250], Loss: 0.0771
-- Evaluation after Epoch [100/250], Test Loss: 0.0718
Epoch [101/250], Loss: 0.0768
Epoch [102/250], Loss: 0.0759
Epoch [103/250], Loss: 0.0760
Epoch [104/250], Loss: 0.0757
Epoch [105/250], Loss: 0.0760
-- Evaluation after Epoch [105/250], Test Loss: 0.0706
New best test loss: 0.0706. Saving model.
Epoch [106/250], Loss: 0.0768
Epoch [107/250], Loss: 0.0762
Epoch [108/250], Loss: 0.0779
Epoch [109/250], Loss: 0.0766
Epoch [110/250], Loss: 0.0760
-- Evaluation after Epoch [110/250], Test Loss: 0.0729
Epoch [111/250], Loss: 0.0763
Epoch [112/250], Loss: 0.0758
Epoch [113/250], Loss: 0.0764
Epoch [114/250], Loss: 0.0755
Epoch [115/250], Loss: 0.0747
-- Evaluation after Epoch [115/250], Test Loss: 0.0737
Epoch [116/250], Loss: 0.0746
Epoch [117/250], Loss: 0.0742
Epoch [118/250], Loss: 0.0738
Epoch [119/250], Loss: 0.0735
Epoch [120/250], Loss: 0.0745
-- Evaluation after Epoch [120/250], Test Loss: 0.0745
Epoch [121/250], Loss: 0.0728
Epoch [122/250], Loss: 0.0743
Epoch [123/250], Loss: 0.0731
Epoch [124/250], Loss: 0.0735
Epoch [125/250], Loss: 0.0729
-- Evaluation after Epoch [125/250], Test Loss: 0.0755
Epoch [126/250], Loss: 0.0732
Epoch [127/250], Loss: 0.0722
Epoch [128/250], Loss: 0.0726
Epoch [129/250], Loss: 0.0725
Epoch [130/250], Loss: 0.0715
-- Evaluation after Epoch [130/250], Test Loss: 0.0730
Epoch [131/250], Loss: 0.0729
Epoch [132/250], Loss: 0.0730
Epoch [133/250], Loss: 0.0717
Epoch [134/250], Loss: 0.0710
Epoch [135/250], Loss: 0.0715
-- Evaluation after Epoch [135/250], Test Loss: 0.0743
Epoch [136/250], Loss: 0.0709
Epoch [137/250], Loss: 0.0710
Epoch [138/250], Loss: 0.0704
Epoch [139/250], Loss: 0.0701
Epoch [140/250], Loss: 0.0712
-- Evaluation after Epoch [140/250], Test Loss: 0.0748
Epoch [141/250], Loss: 0.0709
Epoch [142/250], Loss: 0.0715
Epoch [143/250], Loss: 0.0717
Epoch [144/250], Loss: 0.0707
Epoch [145/250], Loss: 0.0707
-- Evaluation after Epoch [145/250], Test Loss: 0.0740
Epoch [146/250], Loss: 0.0693
Epoch [147/250], Loss: 0.0720
Epoch [148/250], Loss: 0.0717
Epoch [149/250], Loss: 0.0715
Epoch [150/250], Loss: 0.0698
-- Evaluation after Epoch [150/250], Test Loss: 0.0743
Epoch [151/250], Loss: 0.0686
Epoch [152/250], Loss: 0.0686
Epoch [153/250], Loss: 0.0687
Epoch [154/250], Loss: 0.0668
Epoch [155/250], Loss: 0.0678
-- Evaluation after Epoch [155/250], Test Loss: 0.0744
Epoch [156/250], Loss: 0.0676
Epoch [157/250], Loss: 0.0669
Epoch [158/250], Loss: 0.0688
Epoch [159/250], Loss: 0.0669
Epoch [160/250], Loss: 0.0672
-- Evaluation after Epoch [160/250], Test Loss: 0.0740
Epoch [161/250], Loss: 0.0687
Epoch [162/250], Loss: 0.0727
Epoch [163/250], Loss: 0.0681
Epoch [164/250], Loss: 0.0672
Epoch [165/250], Loss: 0.0666
-- Evaluation after Epoch [165/250], Test Loss: 0.0750
Epoch [166/250], Loss: 0.0655
Epoch [167/250], Loss: 0.0640
Epoch [168/250], Loss: 0.0641
Epoch [169/250], Loss: 0.0638
Epoch [170/250], Loss: 0.0638
-- Evaluation after Epoch [170/250], Test Loss: 0.0759
Epoch [171/250], Loss: 0.0641
Epoch [172/250], Loss: 0.0642
Epoch [173/250], Loss: 0.0633
Epoch [174/250], Loss: 0.0637
Epoch [175/250], Loss: 0.0666
-- Evaluation after Epoch [175/250], Test Loss: 0.0747
Epoch [176/250], Loss: 0.0651
Epoch [177/250], Loss: 0.0638
Epoch [178/250], Loss: 0.0633
Epoch [179/250], Loss: 0.0630
Epoch [180/250], Loss: 0.0632
-- Evaluation after Epoch [180/250], Test Loss: 0.0769
Epoch [181/250], Loss: 0.0637
Epoch [182/250], Loss: 0.0640
Epoch [183/250], Loss: 0.0623
Epoch [184/250], Loss: 0.0609
Epoch [185/250], Loss: 0.0604
-- Evaluation after Epoch [185/250], Test Loss: 0.0771
Epoch [186/250], Loss: 0.0597
Epoch [187/250], Loss: 0.0591
Epoch [188/250], Loss: 0.0597
Epoch [189/250], Loss: 0.0602
Epoch [190/250], Loss: 0.0597
-- Evaluation after Epoch [190/250], Test Loss: 0.0811
Epoch [191/250], Loss: 0.0596
Epoch [192/250], Loss: 0.0716
Epoch [193/250], Loss: 0.0700
Epoch [194/250], Loss: 0.0628
Epoch [195/250], Loss: 0.0607
-- Evaluation after Epoch [195/250], Test Loss: 0.0727
Epoch [196/250], Loss: 0.0598
Epoch [197/250], Loss: 0.0592
Epoch [198/250], Loss: 0.0599
Epoch [199/250], Loss: 0.0596
Epoch [200/250], Loss: 0.0576
-- Evaluation after Epoch [200/250], Test Loss: 0.0793
Epoch [201/250], Loss: 0.0577
Epoch [202/250], Loss: 0.0575
Epoch [203/250], Loss: 0.0572
Epoch [204/250], Loss: 0.0586
Epoch [205/250], Loss: 0.0581
-- Evaluation after Epoch [205/250], Test Loss: 0.0793
Epoch [206/250], Loss: 0.0569
Epoch [207/250], Loss: 0.0564
Epoch [208/250], Loss: 0.0576
Epoch [209/250], Loss: 0.0604
Epoch [210/250], Loss: 0.0578
-- Evaluation after Epoch [210/250], Test Loss: 0.0773
Epoch [211/250], Loss: 0.0561
Epoch [212/250], Loss: 0.0549
Epoch [213/250], Loss: 0.0548
Epoch [214/250], Loss: 0.0554
Epoch [215/250], Loss: 0.0560
-- Evaluation after Epoch [215/250], Test Loss: 0.0750
Epoch [216/250], Loss: 0.0556
Epoch [217/250], Loss: 0.0565
Epoch [218/250], Loss: 0.0575
Epoch [219/250], Loss: 0.0590
Epoch [220/250], Loss: 0.0572
-- Evaluation after Epoch [220/250], Test Loss: 0.0781
Epoch [221/250], Loss: 0.0550
Epoch [222/250], Loss: 0.0571
Epoch [223/250], Loss: 0.0599
Epoch [224/250], Loss: 0.0570
Epoch [225/250], Loss: 0.0554
-- Evaluation after Epoch [225/250], Test Loss: 0.0813
Epoch [226/250], Loss: 0.0551
Epoch [227/250], Loss: 0.0546
Epoch [228/250], Loss: 0.0536
Epoch [229/250], Loss: 0.0531
Epoch [230/250], Loss: 0.0526
-- Evaluation after Epoch [230/250], Test Loss: 0.0787
Epoch [231/250], Loss: 0.0524
Epoch [232/250], Loss: 0.0516
Epoch [233/250], Loss: 0.0518
Epoch [234/250], Loss: 0.0540
Epoch [235/250], Loss: 0.0554
-- Evaluation after Epoch [235/250], Test Loss: 0.0779
Epoch [236/250], Loss: 0.0526
Epoch [237/250], Loss: 0.0528
Epoch [238/250], Loss: 0.0516
Epoch [239/250], Loss: 0.0519
Epoch [240/250], Loss: 0.0511
-- Evaluation after Epoch [240/250], Test Loss: 0.0785
Epoch [241/250], Loss: 0.0512
Epoch [242/250], Loss: 0.0509
Epoch [243/250], Loss: 0.0508
Epoch [244/250], Loss: 0.0501
Epoch [245/250], Loss: 0.0504
-- Evaluation after Epoch [245/250], Test Loss: 0.0792
Epoch [246/250], Loss: 0.0498
Epoch [247/250], Loss: 0.0500
Epoch [248/250], Loss: 0.0505
Epoch [249/250], Loss: 0.0501
Epoch [250/250], Loss: 0.0512
-- Evaluation after Epoch [250/250], Test Loss: 0.0802
Loading the best model from checkpoint with test loss: 0.0706
Validation Loss (RMSE): 0.0706
Validation results saved to: ./Results\LSTM_Study_20250129_162653\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0706
Avg Test Loss after reverse scaling (RMSE): 52.6598
Test results saved to: ./Results\LSTM_Study_20250129_162653\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
196
64
./Results\LSTM_Study_20250129_163317
./Results\LSTM_Study_20250129_163317\Train
Config file saved to: ./Results\LSTM_Study_20250129_163317\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([840, 196, 11])
Train labels tensor shape: torch.Size([840, 48])
Val data tensor shape: torch.Size([186, 196, 11])
Val labels tensor shape: torch.Size([186, 48])
Test data tensor shape: torch.Size([186, 196, 11])
Test labels tensor shape: torch.Size([186, 48])
C:\Users\BHARGAV BADE\miniconda3\envs\pyda3.9\lib\site-packages\torch\nn\modules\loss.py:538: UserWarning: Using a target size (torch.Size([64, 48])) that is different to the input size (torch.Size([64, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, prepare_data02, params[0m
196
64
./Results\LSTM_Study_20250129_163440
./Results\LSTM_Study_20250129_163440\Train
Config file saved to: ./Results\LSTM_Study_20250129_163440\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([840, 196, 11])
Train labels tensor shape: torch.Size([840, 48])
Val data tensor shape: torch.Size([186, 196, 11])
Val labels tensor shape: torch.Size([186, 48])
Test data tensor shape: torch.Size([186, 196, 11])
Test labels tensor shape: torch.Size([186, 48])
Epoch [1/250], Loss: 0.2152
Epoch [2/250], Loss: 0.1094
Epoch [3/250], Loss: 0.0988
Epoch [4/250], Loss: 0.0942
Epoch [5/250], Loss: 0.0911
-- Evaluation after Epoch [5/250], Test Loss: 0.0740
New best test loss: 0.0740. Saving model.
Epoch [6/250], Loss: 0.0900
Epoch [7/250], Loss: 0.0893
Epoch [8/250], Loss: 0.0890
Epoch [9/250], Loss: 0.0889
Epoch [10/250], Loss: 0.0888
-- Evaluation after Epoch [10/250], Test Loss: 0.0738
New best test loss: 0.0738. Saving model.
Epoch [11/250], Loss: 0.0888
Epoch [12/250], Loss: 0.0888
Epoch [13/250], Loss: 0.0888
Epoch [14/250], Loss: 0.0888
Epoch [15/250], Loss: 0.0888
-- Evaluation after Epoch [15/250], Test Loss: 0.0738
Epoch [16/250], Loss: 0.0888
Epoch [17/250], Loss: 0.0888
Epoch [18/250], Loss: 0.0888
Epoch [19/250], Loss: 0.0888
Epoch [20/250], Loss: 0.0888
-- Evaluation after Epoch [20/250], Test Loss: 0.0738
New best test loss: 0.0738. Saving model.
Epoch [21/250], Loss: 0.0888
Epoch [22/250], Loss: 0.0888
Epoch [23/250], Loss: 0.0888
Epoch [24/250], Loss: 0.0888
Epoch [25/250], Loss: 0.0888
-- Evaluation after Epoch [25/250], Test Loss: 0.0738
Epoch [26/250], Loss: 0.0889
Epoch [27/250], Loss: 0.0889
Epoch [28/250], Loss: 0.0889
Epoch [29/250], Loss: 0.0889
Epoch [30/250], Loss: 0.0889
-- Evaluation after Epoch [30/250], Test Loss: 0.0738
New best test loss: 0.0738. Saving model.
Epoch [31/250], Loss: 0.0889
Epoch [32/250], Loss: 0.0889
Epoch [33/250], Loss: 0.0889
Epoch [34/250], Loss: 0.0889
Epoch [35/250], Loss: 0.0889
-- Evaluation after Epoch [35/250], Test Loss: 0.0737
New best test loss: 0.0737. Saving model.
Epoch [36/250], Loss: 0.0889
Epoch [37/250], Loss: 0.0889
Epoch [38/250], Loss: 0.0889
Epoch [39/250], Loss: 0.0889
Epoch [40/250], Loss: 0.0889
-- Evaluation after Epoch [40/250], Test Loss: 0.0738
Epoch [41/250], Loss: 0.0889
Epoch [42/250], Loss: 0.0889
Epoch [43/250], Loss: 0.0889
Epoch [44/250], Loss: 0.0889
Epoch [45/250], Loss: 0.0889
-- Evaluation after Epoch [45/250], Test Loss: 0.0737
Epoch [46/250], Loss: 0.0889
Epoch [47/250], Loss: 0.0889
Epoch [48/250], Loss: 0.0889
Epoch [49/250], Loss: 0.0889
Epoch [50/250], Loss: 0.0889
-- Evaluation after Epoch [50/250], Test Loss: 0.0738
Epoch [51/250], Loss: 0.0889
Epoch [52/250], Loss: 0.0889
Epoch [53/250], Loss: 0.0889
Epoch [54/250], Loss: 0.0889
Epoch [55/250], Loss: 0.0889
-- Evaluation after Epoch [55/250], Test Loss: 0.0737
New best test loss: 0.0737. Saving model.
Epoch [56/250], Loss: 0.0889
Epoch [57/250], Loss: 0.0889
Epoch [58/250], Loss: 0.0889
Epoch [59/250], Loss: 0.0889
Epoch [60/250], Loss: 0.0889
-- Evaluation after Epoch [60/250], Test Loss: 0.0737
New best test loss: 0.0737. Saving model.
Epoch [61/250], Loss: 0.0889
Epoch [62/250], Loss: 0.0889
Epoch [63/250], Loss: 0.0889
Epoch [64/250], Loss: 0.0889
Epoch [65/250], Loss: 0.0888
-- Evaluation after Epoch [65/250], Test Loss: 0.0734
New best test loss: 0.0734. Saving model.
Epoch [66/250], Loss: 0.0887
Epoch [67/250], Loss: 0.0886
Epoch [68/250], Loss: 0.0887
Epoch [69/250], Loss: 0.0883
Epoch [70/250], Loss: 0.0883
-- Evaluation after Epoch [70/250], Test Loss: 0.0719
New best test loss: 0.0719. Saving model.
Epoch [71/250], Loss: 0.0884
Epoch [72/250], Loss: 0.0882
Epoch [73/250], Loss: 0.0879
Epoch [74/250], Loss: 0.0879
Epoch [75/250], Loss: 0.0882
-- Evaluation after Epoch [75/250], Test Loss: 0.0724
Epoch [76/250], Loss: 0.0877
Epoch [77/250], Loss: 0.0876
Epoch [78/250], Loss: 0.0874
Epoch [79/250], Loss: 0.0874
Epoch [80/250], Loss: 0.0875
-- Evaluation after Epoch [80/250], Test Loss: 0.0712
New best test loss: 0.0712. Saving model.
Epoch [81/250], Loss: 0.0869
Epoch [82/250], Loss: 0.0867
Epoch [83/250], Loss: 0.0868
Epoch [84/250], Loss: 0.0866
Epoch [85/250], Loss: 0.0863
-- Evaluation after Epoch [85/250], Test Loss: 0.0696
New best test loss: 0.0696. Saving model.
Epoch [86/250], Loss: 0.0860
Epoch [87/250], Loss: 0.0861
Epoch [88/250], Loss: 0.0861
Epoch [89/250], Loss: 0.0862
Epoch [90/250], Loss: 0.0858
-- Evaluation after Epoch [90/250], Test Loss: 0.0679
New best test loss: 0.0679. Saving model.
Epoch [91/250], Loss: 0.0854
Epoch [92/250], Loss: 0.0855
Epoch [93/250], Loss: 0.0854
Epoch [94/250], Loss: 0.0852
Epoch [95/250], Loss: 0.0846
-- Evaluation after Epoch [95/250], Test Loss: 0.0671
New best test loss: 0.0671. Saving model.
Epoch [96/250], Loss: 0.0844
Epoch [97/250], Loss: 0.0842
Epoch [98/250], Loss: 0.0841
Epoch [99/250], Loss: 0.0832
Epoch [100/250], Loss: 0.0834
-- Evaluation after Epoch [100/250], Test Loss: 0.0662
New best test loss: 0.0662. Saving model.
Epoch [101/250], Loss: 0.0835
Epoch [102/250], Loss: 0.0826
Epoch [103/250], Loss: 0.0826
Epoch [104/250], Loss: 0.0828
Epoch [105/250], Loss: 0.0819
-- Evaluation after Epoch [105/250], Test Loss: 0.0659
New best test loss: 0.0659. Saving model.
Epoch [106/250], Loss: 0.0821
Epoch [107/250], Loss: 0.0824
Epoch [108/250], Loss: 0.0823
Epoch [109/250], Loss: 0.0824
Epoch [110/250], Loss: 0.0818
-- Evaluation after Epoch [110/250], Test Loss: 0.0653
New best test loss: 0.0653. Saving model.
Epoch [111/250], Loss: 0.0810
Epoch [112/250], Loss: 0.0811
Epoch [113/250], Loss: 0.0814
Epoch [114/250], Loss: 0.0807
Epoch [115/250], Loss: 0.0811
-- Evaluation after Epoch [115/250], Test Loss: 0.0641
New best test loss: 0.0641. Saving model.
Epoch [116/250], Loss: 0.0827
Epoch [117/250], Loss: 0.0825
Epoch [118/250], Loss: 0.0816
Epoch [119/250], Loss: 0.0811
Epoch [120/250], Loss: 0.0809
-- Evaluation after Epoch [120/250], Test Loss: 0.0638
New best test loss: 0.0638. Saving model.
Epoch [121/250], Loss: 0.0806
Epoch [122/250], Loss: 0.0801
Epoch [123/250], Loss: 0.0799
Epoch [124/250], Loss: 0.0805
Epoch [125/250], Loss: 0.0800
-- Evaluation after Epoch [125/250], Test Loss: 0.0647
Epoch [126/250], Loss: 0.0806
Epoch [127/250], Loss: 0.0798
Epoch [128/250], Loss: 0.0794
Epoch [129/250], Loss: 0.0791
Epoch [130/250], Loss: 0.0792
-- Evaluation after Epoch [130/250], Test Loss: 0.0658
Epoch [131/250], Loss: 0.0803
Epoch [132/250], Loss: 0.0793
Epoch [133/250], Loss: 0.0799
Epoch [134/250], Loss: 0.0804
Epoch [135/250], Loss: 0.0798
-- Evaluation after Epoch [135/250], Test Loss: 0.0660
Epoch [136/250], Loss: 0.0794
Epoch [137/250], Loss: 0.0796
Epoch [138/250], Loss: 0.0784
Epoch [139/250], Loss: 0.0777
Epoch [140/250], Loss: 0.0785
-- Evaluation after Epoch [140/250], Test Loss: 0.0686
Epoch [141/250], Loss: 0.0769
Epoch [142/250], Loss: 0.0790
Epoch [143/250], Loss: 0.0767
Epoch [144/250], Loss: 0.0762
Epoch [145/250], Loss: 0.0757
-- Evaluation after Epoch [145/250], Test Loss: 0.0690
Epoch [146/250], Loss: 0.0757
Epoch [147/250], Loss: 0.0757
Epoch [148/250], Loss: 0.0750
Epoch [149/250], Loss: 0.0756
Epoch [150/250], Loss: 0.0746
-- Evaluation after Epoch [150/250], Test Loss: 0.0703
Epoch [151/250], Loss: 0.0741
Epoch [152/250], Loss: 0.0742
Epoch [153/250], Loss: 0.0780
Epoch [154/250], Loss: 0.0773
Epoch [155/250], Loss: 0.0764
-- Evaluation after Epoch [155/250], Test Loss: 0.0699
Epoch [156/250], Loss: 0.0752
Epoch [157/250], Loss: 0.0765
Epoch [158/250], Loss: 0.0779
Epoch [159/250], Loss: 0.0760
Epoch [160/250], Loss: 0.0744
-- Evaluation after Epoch [160/250], Test Loss: 0.0675
Epoch [161/250], Loss: 0.0737
Epoch [162/250], Loss: 0.0737
Epoch [163/250], Loss: 0.0723
Epoch [164/250], Loss: 0.0720
Epoch [165/250], Loss: 0.0714
-- Evaluation after Epoch [165/250], Test Loss: 0.0712
Epoch [166/250], Loss: 0.0723
Epoch [167/250], Loss: 0.0720
Epoch [168/250], Loss: 0.0714
Epoch [169/250], Loss: 0.0711
Epoch [170/250], Loss: 0.0737
-- Evaluation after Epoch [170/250], Test Loss: 0.0713
Epoch [171/250], Loss: 0.0734
Epoch [172/250], Loss: 0.0741
Epoch [173/250], Loss: 0.0737
Epoch [174/250], Loss: 0.0715
Epoch [175/250], Loss: 0.0703
-- Evaluation after Epoch [175/250], Test Loss: 0.0703
Epoch [176/250], Loss: 0.0698
Epoch [177/250], Loss: 0.0703
Epoch [178/250], Loss: 0.0714
Epoch [179/250], Loss: 0.0716
Epoch [180/250], Loss: 0.0703
-- Evaluation after Epoch [180/250], Test Loss: 0.0710
Epoch [181/250], Loss: 0.0690
Epoch [182/250], Loss: 0.0685
Epoch [183/250], Loss: 0.0686
Epoch [184/250], Loss: 0.0696
Epoch [185/250], Loss: 0.0698
-- Evaluation after Epoch [185/250], Test Loss: 0.0710
Epoch [186/250], Loss: 0.0703
Epoch [187/250], Loss: 0.0701
Epoch [188/250], Loss: 0.0725
Epoch [189/250], Loss: 0.0703
Epoch [190/250], Loss: 0.0685
-- Evaluation after Epoch [190/250], Test Loss: 0.0698
Epoch [191/250], Loss: 0.0680
Epoch [192/250], Loss: 0.0676
Epoch [193/250], Loss: 0.0665
Epoch [194/250], Loss: 0.0660
Epoch [195/250], Loss: 0.0661
-- Evaluation after Epoch [195/250], Test Loss: 0.0705
Epoch [196/250], Loss: 0.0671
Epoch [197/250], Loss: 0.0661
Epoch [198/250], Loss: 0.0672
Epoch [199/250], Loss: 0.0675
Epoch [200/250], Loss: 0.0665
-- Evaluation after Epoch [200/250], Test Loss: 0.0739
Epoch [201/250], Loss: 0.0666
Epoch [202/250], Loss: 0.0657
Epoch [203/250], Loss: 0.0663
Epoch [204/250], Loss: 0.0669
Epoch [205/250], Loss: 0.0671
-- Evaluation after Epoch [205/250], Test Loss: 0.0732
Epoch [206/250], Loss: 0.0677
Epoch [207/250], Loss: 0.0668
Epoch [208/250], Loss: 0.0654
Epoch [209/250], Loss: 0.0644
Epoch [210/250], Loss: 0.0627
-- Evaluation after Epoch [210/250], Test Loss: 0.0751
Epoch [211/250], Loss: 0.0628
Epoch [212/250], Loss: 0.0622
Epoch [213/250], Loss: 0.0622
Epoch [214/250], Loss: 0.0628
Epoch [215/250], Loss: 0.0640
-- Evaluation after Epoch [215/250], Test Loss: 0.0743
Epoch [216/250], Loss: 0.0634
Epoch [217/250], Loss: 0.0635
Epoch [218/250], Loss: 0.0622
Epoch [219/250], Loss: 0.0626
Epoch [220/250], Loss: 0.0639
-- Evaluation after Epoch [220/250], Test Loss: 0.0732
Epoch [221/250], Loss: 0.0633
Epoch [222/250], Loss: 0.0616
Epoch [223/250], Loss: 0.0611
Epoch [224/250], Loss: 0.0603
Epoch [225/250], Loss: 0.0596
-- Evaluation after Epoch [225/250], Test Loss: 0.0761
Epoch [226/250], Loss: 0.0598
Epoch [227/250], Loss: 0.0587
Epoch [228/250], Loss: 0.0588
Epoch [229/250], Loss: 0.0586
Epoch [230/250], Loss: 0.0602
-- Evaluation after Epoch [230/250], Test Loss: 0.0754
Epoch [231/250], Loss: 0.0588
Epoch [232/250], Loss: 0.0595
Epoch [233/250], Loss: 0.0596
Epoch [234/250], Loss: 0.0593
Epoch [235/250], Loss: 0.0615
-- Evaluation after Epoch [235/250], Test Loss: 0.0758
Epoch [236/250], Loss: 0.0606
Epoch [237/250], Loss: 0.0593
Epoch [238/250], Loss: 0.0594
Epoch [239/250], Loss: 0.0602
Epoch [240/250], Loss: 0.0604
-- Evaluation after Epoch [240/250], Test Loss: 0.0704
Epoch [241/250], Loss: 0.0599
Epoch [242/250], Loss: 0.0611
Epoch [243/250], Loss: 0.0621
Epoch [244/250], Loss: 0.0625
Epoch [245/250], Loss: 0.0623
-- Evaluation after Epoch [245/250], Test Loss: 0.0754
Epoch [246/250], Loss: 0.0609
Epoch [247/250], Loss: 0.0587
Epoch [248/250], Loss: 0.0579
Epoch [249/250], Loss: 0.0581
Epoch [250/250], Loss: 0.0569
-- Evaluation after Epoch [250/250], Test Loss: 0.0723
Loading the best model from checkpoint with test loss: 0.0638
Validation Loss (RMSE): 0.0638
Validation results saved to: ./Results\LSTM_Study_20250129_163440\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0638
Avg Test Loss after reverse scaling (RMSE): 49.1723
Test results saved to: ./Results\LSTM_Study_20250129_163440\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, params, prepare_data02[0m
196
64
./Results\LSTM_Study_20250129_164147
./Results\LSTM_Study_20250129_164147\Train
Config file saved to: ./Results\LSTM_Study_20250129_164147\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([840, 196, 11])
Train labels tensor shape: torch.Size([840, 48])
Val data tensor shape: torch.Size([186, 196, 11])
Val labels tensor shape: torch.Size([186, 48])
Test data tensor shape: torch.Size([186, 196, 11])
Test labels tensor shape: torch.Size([186, 48])
Epoch [1/250], Loss: 0.2768
Epoch [2/250], Loss: 0.1396
Epoch [3/250], Loss: 0.1058
Epoch [4/250], Loss: 0.0995
Epoch [5/250], Loss: 0.0956
-- Evaluation after Epoch [5/250], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [6/250], Loss: 0.0925
Epoch [7/250], Loss: 0.0909
Epoch [8/250], Loss: 0.0898
Epoch [9/250], Loss: 0.0894
Epoch [10/250], Loss: 0.0891
-- Evaluation after Epoch [10/250], Test Loss: 0.0737
New best test loss: 0.0737. Saving model.
Epoch [11/250], Loss: 0.0891
Epoch [12/250], Loss: 0.0889
Epoch [13/250], Loss: 0.0889
Epoch [14/250], Loss: 0.0889
Epoch [15/250], Loss: 0.0888
-- Evaluation after Epoch [15/250], Test Loss: 0.0738
Epoch [16/250], Loss: 0.0889
Epoch [17/250], Loss: 0.0888
Epoch [18/250], Loss: 0.0889
Epoch [19/250], Loss: 0.0888
Epoch [20/250], Loss: 0.0889
-- Evaluation after Epoch [20/250], Test Loss: 0.0738
Epoch [21/250], Loss: 0.0888
Epoch [22/250], Loss: 0.0888
Epoch [23/250], Loss: 0.0889
Epoch [24/250], Loss: 0.0889
Epoch [25/250], Loss: 0.0889
-- Evaluation after Epoch [25/250], Test Loss: 0.0738
Epoch [26/250], Loss: 0.0889
Epoch [27/250], Loss: 0.0888
Epoch [28/250], Loss: 0.0888
Epoch [29/250], Loss: 0.0889
Epoch [30/250], Loss: 0.0889
-- Evaluation after Epoch [30/250], Test Loss: 0.0738
Epoch [31/250], Loss: 0.0888
Epoch [32/250], Loss: 0.0889
Epoch [33/250], Loss: 0.0888
Epoch [34/250], Loss: 0.0889
Epoch [35/250], Loss: 0.0889
-- Evaluation after Epoch [35/250], Test Loss: 0.0738
Epoch [36/250], Loss: 0.0889
Epoch [37/250], Loss: 0.0889
Epoch [38/250], Loss: 0.0889
Epoch [39/250], Loss: 0.0889
Epoch [40/250], Loss: 0.0888
-- Evaluation after Epoch [40/250], Test Loss: 0.0735
New best test loss: 0.0735. Saving model.
Epoch [41/250], Loss: 0.0888
Epoch [42/250], Loss: 0.0888
Epoch [43/250], Loss: 0.0887
Epoch [44/250], Loss: 0.0884
Epoch [45/250], Loss: 0.0884
-- Evaluation after Epoch [45/250], Test Loss: 0.0727
New best test loss: 0.0727. Saving model.
Epoch [46/250], Loss: 0.0880
Epoch [47/250], Loss: 0.0881
Epoch [48/250], Loss: 0.0882
Epoch [49/250], Loss: 0.0877
Epoch [50/250], Loss: 0.0876
-- Evaluation after Epoch [50/250], Test Loss: 0.0717
New best test loss: 0.0717. Saving model.
Epoch [51/250], Loss: 0.0873
Epoch [52/250], Loss: 0.0868
Epoch [53/250], Loss: 0.0867
Epoch [54/250], Loss: 0.0870
Epoch [55/250], Loss: 0.0872
-- Evaluation after Epoch [55/250], Test Loss: 0.0705
New best test loss: 0.0705. Saving model.
Epoch [56/250], Loss: 0.0869
Epoch [57/250], Loss: 0.0865
Epoch [58/250], Loss: 0.0861
Epoch [59/250], Loss: 0.0863
Epoch [60/250], Loss: 0.0859
-- Evaluation after Epoch [60/250], Test Loss: 0.0692
New best test loss: 0.0692. Saving model.
Epoch [61/250], Loss: 0.0857
Epoch [62/250], Loss: 0.0850
Epoch [63/250], Loss: 0.0850
Epoch [64/250], Loss: 0.0851
Epoch [65/250], Loss: 0.0856
-- Evaluation after Epoch [65/250], Test Loss: 0.0674
New best test loss: 0.0674. Saving model.
Epoch [66/250], Loss: 0.0852
Epoch [67/250], Loss: 0.0852
Epoch [68/250], Loss: 0.0848
Epoch [69/250], Loss: 0.0848
Epoch [70/250], Loss: 0.0845
-- Evaluation after Epoch [70/250], Test Loss: 0.0683
Epoch [71/250], Loss: 0.0839
Epoch [72/250], Loss: 0.0839
Epoch [73/250], Loss: 0.0832
Epoch [74/250], Loss: 0.0835
Epoch [75/250], Loss: 0.0824
-- Evaluation after Epoch [75/250], Test Loss: 0.0673
New best test loss: 0.0673. Saving model.
Epoch [76/250], Loss: 0.0832
Epoch [77/250], Loss: 0.0839
Epoch [78/250], Loss: 0.0827
Epoch [79/250], Loss: 0.0832
Epoch [80/250], Loss: 0.0826
-- Evaluation after Epoch [80/250], Test Loss: 0.0677
Epoch [81/250], Loss: 0.0819
Epoch [82/250], Loss: 0.0828
Epoch [83/250], Loss: 0.0840
Epoch [84/250], Loss: 0.0829
Epoch [85/250], Loss: 0.0827
-- Evaluation after Epoch [85/250], Test Loss: 0.0661
New best test loss: 0.0661. Saving model.
Epoch [86/250], Loss: 0.0819
Epoch [87/250], Loss: 0.0815
Epoch [88/250], Loss: 0.0811
Epoch [89/250], Loss: 0.0806
Epoch [90/250], Loss: 0.0801
-- Evaluation after Epoch [90/250], Test Loss: 0.0677
Epoch [91/250], Loss: 0.0799
Epoch [92/250], Loss: 0.0804
Epoch [93/250], Loss: 0.0812
Epoch [94/250], Loss: 0.0818
Epoch [95/250], Loss: 0.0821
-- Evaluation after Epoch [95/250], Test Loss: 0.0665
Epoch [96/250], Loss: 0.0822
Epoch [97/250], Loss: 0.0810
Epoch [98/250], Loss: 0.0795
Epoch [99/250], Loss: 0.0794
Epoch [100/250], Loss: 0.0799
-- Evaluation after Epoch [100/250], Test Loss: 0.0668
Epoch [101/250], Loss: 0.0798
Epoch [102/250], Loss: 0.0797
Epoch [103/250], Loss: 0.0802
Epoch [104/250], Loss: 0.0807
Epoch [105/250], Loss: 0.0799
-- Evaluation after Epoch [105/250], Test Loss: 0.0664
Epoch [106/250], Loss: 0.0791
Epoch [107/250], Loss: 0.0798
Epoch [108/250], Loss: 0.0790
Epoch [109/250], Loss: 0.0786
Epoch [110/250], Loss: 0.0784
-- Evaluation after Epoch [110/250], Test Loss: 0.0695
Epoch [111/250], Loss: 0.0782
Epoch [112/250], Loss: 0.0784
Epoch [113/250], Loss: 0.0786
Epoch [114/250], Loss: 0.0809
Epoch [115/250], Loss: 0.0788
-- Evaluation after Epoch [115/250], Test Loss: 0.0691
Epoch [116/250], Loss: 0.0783
Epoch [117/250], Loss: 0.0784
Epoch [118/250], Loss: 0.0773
Epoch [119/250], Loss: 0.0766
Epoch [120/250], Loss: 0.0766
-- Evaluation after Epoch [120/250], Test Loss: 0.0709
Epoch [121/250], Loss: 0.0758
Epoch [122/250], Loss: 0.0762
Epoch [123/250], Loss: 0.0767
Epoch [124/250], Loss: 0.0777
Epoch [125/250], Loss: 0.0767
-- Evaluation after Epoch [125/250], Test Loss: 0.0695
Epoch [126/250], Loss: 0.0763
Epoch [127/250], Loss: 0.0766
Epoch [128/250], Loss: 0.0765
Epoch [129/250], Loss: 0.0754
Epoch [130/250], Loss: 0.0750
-- Evaluation after Epoch [130/250], Test Loss: 0.0689
Epoch [131/250], Loss: 0.0744
Epoch [132/250], Loss: 0.0742
Epoch [133/250], Loss: 0.0738
Epoch [134/250], Loss: 0.0738
Epoch [135/250], Loss: 0.0740
-- Evaluation after Epoch [135/250], Test Loss: 0.0700
Epoch [136/250], Loss: 0.0748
Epoch [137/250], Loss: 0.0748
Epoch [138/250], Loss: 0.0736
Epoch [139/250], Loss: 0.0749
Epoch [140/250], Loss: 0.0733
-- Evaluation after Epoch [140/250], Test Loss: 0.0697
Epoch [141/250], Loss: 0.0754
Epoch [142/250], Loss: 0.0763
Epoch [143/250], Loss: 0.0747
Epoch [144/250], Loss: 0.0731
Epoch [145/250], Loss: 0.0720
-- Evaluation after Epoch [145/250], Test Loss: 0.0692
Epoch [146/250], Loss: 0.0741
Epoch [147/250], Loss: 0.0768
Epoch [148/250], Loss: 0.0758
Epoch [149/250], Loss: 0.0740
Epoch [150/250], Loss: 0.0734
-- Evaluation after Epoch [150/250], Test Loss: 0.0700
Epoch [151/250], Loss: 0.0720
Epoch [152/250], Loss: 0.0713
Epoch [153/250], Loss: 0.0718
Epoch [154/250], Loss: 0.0718
Epoch [155/250], Loss: 0.0709
-- Evaluation after Epoch [155/250], Test Loss: 0.0695
Epoch [156/250], Loss: 0.0711
Epoch [157/250], Loss: 0.0718
Epoch [158/250], Loss: 0.0728
Epoch [159/250], Loss: 0.0756
Epoch [160/250], Loss: 0.0740
-- Evaluation after Epoch [160/250], Test Loss: 0.0671
Epoch [161/250], Loss: 0.0745
Epoch [162/250], Loss: 0.0717
Epoch [163/250], Loss: 0.0719
Epoch [164/250], Loss: 0.0717
Epoch [165/250], Loss: 0.0727
-- Evaluation after Epoch [165/250], Test Loss: 0.0694
Epoch [166/250], Loss: 0.0732
Epoch [167/250], Loss: 0.0697
Epoch [168/250], Loss: 0.0699
Epoch [169/250], Loss: 0.0693
Epoch [170/250], Loss: 0.0693
-- Evaluation after Epoch [170/250], Test Loss: 0.0710
Epoch [171/250], Loss: 0.0678
Epoch [172/250], Loss: 0.0678
Epoch [173/250], Loss: 0.0677
Epoch [174/250], Loss: 0.0676
Epoch [175/250], Loss: 0.0667
-- Evaluation after Epoch [175/250], Test Loss: 0.0711
Epoch [176/250], Loss: 0.0664
Epoch [177/250], Loss: 0.0669
Epoch [178/250], Loss: 0.0664
Epoch [179/250], Loss: 0.0657
Epoch [180/250], Loss: 0.0651
-- Evaluation after Epoch [180/250], Test Loss: 0.0736
Epoch [181/250], Loss: 0.0650
Epoch [182/250], Loss: 0.0652
Epoch [183/250], Loss: 0.0667
Epoch [184/250], Loss: 0.0715
Epoch [185/250], Loss: 0.0688
-- Evaluation after Epoch [185/250], Test Loss: 0.0707
Epoch [186/250], Loss: 0.0689
Epoch [187/250], Loss: 0.0681
Epoch [188/250], Loss: 0.0664
Epoch [189/250], Loss: 0.0655
Epoch [190/250], Loss: 0.0650
-- Evaluation after Epoch [190/250], Test Loss: 0.0739
Epoch [191/250], Loss: 0.0661
Epoch [192/250], Loss: 0.0655
Epoch [193/250], Loss: 0.0643
Epoch [194/250], Loss: 0.0637
Epoch [195/250], Loss: 0.0621
-- Evaluation after Epoch [195/250], Test Loss: 0.0747
Epoch [196/250], Loss: 0.0624
Epoch [197/250], Loss: 0.0618
Epoch [198/250], Loss: 0.0639
Epoch [199/250], Loss: 0.0630
Epoch [200/250], Loss: 0.0616
-- Evaluation after Epoch [200/250], Test Loss: 0.0777
Epoch [201/250], Loss: 0.0615
Epoch [202/250], Loss: 0.0611
Epoch [203/250], Loss: 0.0608
Epoch [204/250], Loss: 0.0611
Epoch [205/250], Loss: 0.0612
-- Evaluation after Epoch [205/250], Test Loss: 0.0766
Epoch [206/250], Loss: 0.0620
Epoch [207/250], Loss: 0.0607
Epoch [208/250], Loss: 0.0603
Epoch [209/250], Loss: 0.0601
Epoch [210/250], Loss: 0.0602
-- Evaluation after Epoch [210/250], Test Loss: 0.0791
Epoch [211/250], Loss: 0.0603
Epoch [212/250], Loss: 0.0597
Epoch [213/250], Loss: 0.0597
Epoch [214/250], Loss: 0.0596
Epoch [215/250], Loss: 0.0590
-- Evaluation after Epoch [215/250], Test Loss: 0.0750
Epoch [216/250], Loss: 0.0588
Epoch [217/250], Loss: 0.0590
Epoch [218/250], Loss: 0.0587
Epoch [219/250], Loss: 0.0583
Epoch [220/250], Loss: 0.0576
-- Evaluation after Epoch [220/250], Test Loss: 0.0752
Epoch [221/250], Loss: 0.0574
Epoch [222/250], Loss: 0.0569
Epoch [223/250], Loss: 0.0570
Epoch [224/250], Loss: 0.0577
Epoch [225/250], Loss: 0.0584
-- Evaluation after Epoch [225/250], Test Loss: 0.0794
Epoch [226/250], Loss: 0.0585
Epoch [227/250], Loss: 0.0581
Epoch [228/250], Loss: 0.0587
Epoch [229/250], Loss: 0.0579
Epoch [230/250], Loss: 0.0579
-- Evaluation after Epoch [230/250], Test Loss: 0.0809
Epoch [231/250], Loss: 0.0572
Epoch [232/250], Loss: 0.0578
Epoch [233/250], Loss: 0.0567
Epoch [234/250], Loss: 0.0571
Epoch [235/250], Loss: 0.0558
-- Evaluation after Epoch [235/250], Test Loss: 0.0796
Epoch [236/250], Loss: 0.0553
Epoch [237/250], Loss: 0.0551
Epoch [238/250], Loss: 0.0550
Epoch [239/250], Loss: 0.0545
Epoch [240/250], Loss: 0.0543
-- Evaluation after Epoch [240/250], Test Loss: 0.0790
Epoch [241/250], Loss: 0.0549
Epoch [242/250], Loss: 0.0558
Epoch [243/250], Loss: 0.0554
Epoch [244/250], Loss: 0.0560
Epoch [245/250], Loss: 0.0560
-- Evaluation after Epoch [245/250], Test Loss: 0.0820
Epoch [246/250], Loss: 0.0575
Epoch [247/250], Loss: 0.0569
Epoch [248/250], Loss: 0.0555
Epoch [249/250], Loss: 0.0561
Epoch [250/250], Loss: 0.0559
-- Evaluation after Epoch [250/250], Test Loss: 0.0882
Loading the best model from checkpoint with test loss: 0.0661
Validation Loss (RMSE): 0.0661
Validation results saved to: ./Results\LSTM_Study_20250129_164147\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0661
Avg Test Loss after reverse scaling (RMSE): 52.0805
Test results saved to: ./Results\LSTM_Study_20250129_164147\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, prepare_data02, params[0m
196
64
./Results\LSTM_Study_20250129_164853
./Results\LSTM_Study_20250129_164853\Train
Config file saved to: ./Results\LSTM_Study_20250129_164853\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([840, 196, 11])
Train labels tensor shape: torch.Size([840, 48])
Val data tensor shape: torch.Size([186, 196, 11])
Val labels tensor shape: torch.Size([186, 48])
Test data tensor shape: torch.Size([186, 196, 11])
Test labels tensor shape: torch.Size([186, 48])
Epoch [1/200], Loss: 0.1574
Epoch [2/200], Loss: 0.1198
Epoch [3/200], Loss: 0.1020
Epoch [4/200], Loss: 0.0947
Epoch [5/200], Loss: 0.0929
-- Evaluation after Epoch [5/200], Test Loss: 0.0759
New best test loss: 0.0759. Saving model.
Epoch [6/200], Loss: 0.0916
Epoch [7/200], Loss: 0.0913
Epoch [8/200], Loss: 0.0908
Epoch [9/200], Loss: 0.0908
Epoch [10/200], Loss: 0.0908
-- Evaluation after Epoch [10/200], Test Loss: 0.0749
New best test loss: 0.0749. Saving model.
Epoch [11/200], Loss: 0.0908
Epoch [12/200], Loss: 0.0905
Epoch [13/200], Loss: 0.0906
Epoch [14/200], Loss: 0.0906
Epoch [15/200], Loss: 0.0908
-- Evaluation after Epoch [15/200], Test Loss: 0.0745
New best test loss: 0.0745. Saving model.
Epoch [16/200], Loss: 0.0907
Epoch [17/200], Loss: 0.0906
Epoch [18/200], Loss: 0.0904
Epoch [19/200], Loss: 0.0905
Epoch [20/200], Loss: 0.0903
-- Evaluation after Epoch [20/200], Test Loss: 0.0741
New best test loss: 0.0741. Saving model.
Epoch [21/200], Loss: 0.0903
Epoch [22/200], Loss: 0.0904
Epoch [23/200], Loss: 0.0905
Epoch [24/200], Loss: 0.0904
Epoch [25/200], Loss: 0.0904
-- Evaluation after Epoch [25/200], Test Loss: 0.0753
Epoch [26/200], Loss: 0.0906
Epoch [27/200], Loss: 0.0905
Epoch [28/200], Loss: 0.0905
Epoch [29/200], Loss: 0.0902
Epoch [30/200], Loss: 0.0901
-- Evaluation after Epoch [30/200], Test Loss: 0.0738
New best test loss: 0.0738. Saving model.
Epoch [31/200], Loss: 0.0904
Epoch [32/200], Loss: 0.0902
Epoch [33/200], Loss: 0.0903
Epoch [34/200], Loss: 0.0907
Epoch [35/200], Loss: 0.0906
-- Evaluation after Epoch [35/200], Test Loss: 0.0746
Epoch [36/200], Loss: 0.0904
Epoch [37/200], Loss: 0.0905
Epoch [38/200], Loss: 0.0902
Epoch [39/200], Loss: 0.0901
Epoch [40/200], Loss: 0.0902
-- Evaluation after Epoch [40/200], Test Loss: 0.0742
Epoch [41/200], Loss: 0.0903
Epoch [42/200], Loss: 0.0903
Epoch [43/200], Loss: 0.0906
Epoch [44/200], Loss: 0.0902
Epoch [45/200], Loss: 0.0901
-- Evaluation after Epoch [45/200], Test Loss: 0.0745
Epoch [46/200], Loss: 0.0905
Epoch [47/200], Loss: 0.0901
Epoch [48/200], Loss: 0.0902
Epoch [49/200], Loss: 0.0900
Epoch [50/200], Loss: 0.0906
-- Evaluation after Epoch [50/200], Test Loss: 0.0750
Epoch [51/200], Loss: 0.0901
Epoch [52/200], Loss: 0.0899
Epoch [53/200], Loss: 0.0903
Epoch [54/200], Loss: 0.0906
Epoch [55/200], Loss: 0.0903
-- Evaluation after Epoch [55/200], Test Loss: 0.0741
Epoch [56/200], Loss: 0.0904
Epoch [57/200], Loss: 0.0900
Epoch [58/200], Loss: 0.0902
Epoch [59/200], Loss: 0.0903
Epoch [60/200], Loss: 0.0903
-- Evaluation after Epoch [60/200], Test Loss: 0.0746
Epoch [61/200], Loss: 0.0901
Epoch [62/200], Loss: 0.0901
Epoch [63/200], Loss: 0.0902
Epoch [64/200], Loss: 0.0900
Epoch [65/200], Loss: 0.0899
-- Evaluation after Epoch [65/200], Test Loss: 0.0752
Epoch [66/200], Loss: 0.0900
Epoch [67/200], Loss: 0.0901
Epoch [68/200], Loss: 0.0900
Epoch [69/200], Loss: 0.0902
Epoch [70/200], Loss: 0.0900
-- Evaluation after Epoch [70/200], Test Loss: 0.0741
Epoch [71/200], Loss: 0.0901
Epoch [72/200], Loss: 0.0900
Epoch [73/200], Loss: 0.0901
Epoch [74/200], Loss: 0.0898
Epoch [75/200], Loss: 0.0902
-- Evaluation after Epoch [75/200], Test Loss: 0.0746
Epoch [76/200], Loss: 0.0900
Epoch [77/200], Loss: 0.0900
Epoch [78/200], Loss: 0.0900
Epoch [79/200], Loss: 0.0898
Epoch [80/200], Loss: 0.0900
-- Evaluation after Epoch [80/200], Test Loss: 0.0742
Epoch [81/200], Loss: 0.0899
Epoch [82/200], Loss: 0.0898
Epoch [83/200], Loss: 0.0900
Epoch [84/200], Loss: 0.0899
Epoch [85/200], Loss: 0.0899
-- Evaluation after Epoch [85/200], Test Loss: 0.0750
Epoch [86/200], Loss: 0.0896
Epoch [87/200], Loss: 0.0899
Epoch [88/200], Loss: 0.0900
Epoch [89/200], Loss: 0.0898
Epoch [90/200], Loss: 0.0899
-- Evaluation after Epoch [90/200], Test Loss: 0.0748
Epoch [91/200], Loss: 0.0897
Epoch [92/200], Loss: 0.0897
Epoch [93/200], Loss: 0.0902
Epoch [94/200], Loss: 0.0899
Epoch [95/200], Loss: 0.0899
-- Evaluation after Epoch [95/200], Test Loss: 0.0746
Epoch [96/200], Loss: 0.0898
Epoch [97/200], Loss: 0.0899
Epoch [98/200], Loss: 0.0898
Epoch [99/200], Loss: 0.0898
Epoch [100/200], Loss: 0.0898
-- Evaluation after Epoch [100/200], Test Loss: 0.0742
Epoch [101/200], Loss: 0.0896
Epoch [102/200], Loss: 0.0897
Epoch [103/200], Loss: 0.0899
Epoch [104/200], Loss: 0.0899
Epoch [105/200], Loss: 0.0901
-- Evaluation after Epoch [105/200], Test Loss: 0.0744
Epoch [106/200], Loss: 0.0895
Epoch [107/200], Loss: 0.0896
Epoch [108/200], Loss: 0.0898
Epoch [109/200], Loss: 0.0900
Epoch [110/200], Loss: 0.0899
-- Evaluation after Epoch [110/200], Test Loss: 0.0754
Epoch [111/200], Loss: 0.0897
Epoch [112/200], Loss: 0.0901
Epoch [113/200], Loss: 0.0896
Epoch [114/200], Loss: 0.0900
Epoch [115/200], Loss: 0.0898
-- Evaluation after Epoch [115/200], Test Loss: 0.0741
Epoch [116/200], Loss: 0.0899
Epoch [117/200], Loss: 0.0898
Epoch [118/200], Loss: 0.0897
Epoch [119/200], Loss: 0.0897
Epoch [120/200], Loss: 0.0898
-- Evaluation after Epoch [120/200], Test Loss: 0.0753
Epoch [121/200], Loss: 0.0899
Epoch [122/200], Loss: 0.0899
Epoch [123/200], Loss: 0.0898
Epoch [124/200], Loss: 0.0899
Epoch [125/200], Loss: 0.0895
-- Evaluation after Epoch [125/200], Test Loss: 0.0740
Epoch [126/200], Loss: 0.0899
Epoch [127/200], Loss: 0.0899
Epoch [128/200], Loss: 0.0900
Epoch [129/200], Loss: 0.0896
Epoch [130/200], Loss: 0.0896
-- Evaluation after Epoch [130/200], Test Loss: 0.0743
Epoch [131/200], Loss: 0.0899
Epoch [132/200], Loss: 0.0898
Epoch [133/200], Loss: 0.0898
Epoch [134/200], Loss: 0.0898
Epoch [135/200], Loss: 0.0898
-- Evaluation after Epoch [135/200], Test Loss: 0.0744
Epoch [136/200], Loss: 0.0899
Epoch [137/200], Loss: 0.0899
Epoch [138/200], Loss: 0.0898
Epoch [139/200], Loss: 0.0895
Epoch [140/200], Loss: 0.0897
-- Evaluation after Epoch [140/200], Test Loss: 0.0759
Epoch [141/200], Loss: 0.0899
Epoch [142/200], Loss: 0.0900
Epoch [143/200], Loss: 0.0898
Epoch [144/200], Loss: 0.0897
Epoch [145/200], Loss: 0.0897
-- Evaluation after Epoch [145/200], Test Loss: 0.0740
Epoch [146/200], Loss: 0.0899
Epoch [147/200], Loss: 0.0898
Epoch [148/200], Loss: 0.0895
Epoch [149/200], Loss: 0.0900
Epoch [150/200], Loss: 0.0897
-- Evaluation after Epoch [150/200], Test Loss: 0.0747
Epoch [151/200], Loss: 0.0896
Epoch [152/200], Loss: 0.0897
Epoch [153/200], Loss: 0.0898
Epoch [154/200], Loss: 0.0897
Epoch [155/200], Loss: 0.0897
-- Evaluation after Epoch [155/200], Test Loss: 0.0746
Epoch [156/200], Loss: 0.0895
Epoch [157/200], Loss: 0.0898
Epoch [158/200], Loss: 0.0902
Epoch [159/200], Loss: 0.0896
Epoch [160/200], Loss: 0.0898
-- Evaluation after Epoch [160/200], Test Loss: 0.0747
Epoch [161/200], Loss: 0.0897
Epoch [162/200], Loss: 0.0895
Epoch [163/200], Loss: 0.0899
Epoch [164/200], Loss: 0.0899
Epoch [165/200], Loss: 0.0898
-- Evaluation after Epoch [165/200], Test Loss: 0.0747
Epoch [166/200], Loss: 0.0896
Epoch [167/200], Loss: 0.0900
Epoch [168/200], Loss: 0.0898
Epoch [169/200], Loss: 0.0897
Epoch [170/200], Loss: 0.0901
-- Evaluation after Epoch [170/200], Test Loss: 0.0757
Epoch [171/200], Loss: 0.0895
Epoch [172/200], Loss: 0.0897
Epoch [173/200], Loss: 0.0900
Epoch [174/200], Loss: 0.0895
Epoch [175/200], Loss: 0.0898
-- Evaluation after Epoch [175/200], Test Loss: 0.0746
Epoch [176/200], Loss: 0.0896
Epoch [177/200], Loss: 0.0899
Epoch [178/200], Loss: 0.0899
Epoch [179/200], Loss: 0.0896
Epoch [180/200], Loss: 0.0902
-- Evaluation after Epoch [180/200], Test Loss: 0.0741
Epoch [181/200], Loss: 0.0894
Epoch [182/200], Loss: 0.0897
Epoch [183/200], Loss: 0.0901
Epoch [184/200], Loss: 0.0900
Epoch [185/200], Loss: 0.0901
-- Evaluation after Epoch [185/200], Test Loss: 0.0753
Epoch [186/200], Loss: 0.0897
Epoch [187/200], Loss: 0.0898
Epoch [188/200], Loss: 0.0898
Epoch [189/200], Loss: 0.0897
Epoch [190/200], Loss: 0.0896
-- Evaluation after Epoch [190/200], Test Loss: 0.0745
Epoch [191/200], Loss: 0.0899
Epoch [192/200], Loss: 0.0899
Epoch [193/200], Loss: 0.0899
Epoch [194/200], Loss: 0.0898
Epoch [195/200], Loss: 0.0901
-- Evaluation after Epoch [195/200], Test Loss: 0.0759
Epoch [196/200], Loss: 0.0897
Epoch [197/200], Loss: 0.0903
Epoch [198/200], Loss: 0.0900
Epoch [199/200], Loss: 0.0899
Epoch [200/200], Loss: 0.0899
-- Evaluation after Epoch [200/200], Test Loss: 0.0736
New best test loss: 0.0736. Saving model.
Loading the best model from checkpoint with test loss: 0.0736
Validation Loss (RMSE): 0.0736
Validation results saved to: ./Results\LSTM_Study_20250129_164853\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0736
Avg Test Loss after reverse scaling (RMSE): 59.3236
Test results saved to: ./Results\LSTM_Study_20250129_164853\Test\test_results.txt
[4;33mReloaded modules[24m: Network.lstm_network, study_folder, prepare_data02, params[0m
196
64
./Results\LSTM_Study_20250129_165103
./Results\LSTM_Study_20250129_165103\Train
Config file saved to: ./Results\LSTM_Study_20250129_165103\params.py
./01_PM2.5 Chinese Weather data\BeijingPM20100101_20151231.csv
Range of PM values: 993.0 (Min: 1.0, Max: 994.0)
Avg of PM values: 95.18355527059907
PM index during scaling is: 8
Train data tensor shape: torch.Size([840, 196, 11])
Train labels tensor shape: torch.Size([840, 48])
Val data tensor shape: torch.Size([186, 196, 11])
Val labels tensor shape: torch.Size([186, 48])
Test data tensor shape: torch.Size([186, 196, 11])
Test labels tensor shape: torch.Size([186, 48])
Epoch [1/300], Loss: 0.3288
Epoch [2/300], Loss: 0.3056
Epoch [3/300], Loss: 0.2839
Epoch [4/300], Loss: 0.2627
Epoch [5/300], Loss: 0.2436
-- Evaluation after Epoch [5/300], Test Loss: 0.2210
New best test loss: 0.2210. Saving model.
Epoch [6/300], Loss: 0.2253
Epoch [7/300], Loss: 0.2086
Epoch [8/300], Loss: 0.1934
Epoch [9/300], Loss: 0.1801
Epoch [10/300], Loss: 0.1685
-- Evaluation after Epoch [10/300], Test Loss: 0.1453
New best test loss: 0.1453. Saving model.
Epoch [11/300], Loss: 0.1581
Epoch [12/300], Loss: 0.1482
Epoch [13/300], Loss: 0.1396
Epoch [14/300], Loss: 0.1323
Epoch [15/300], Loss: 0.1260
-- Evaluation after Epoch [15/300], Test Loss: 0.1008
New best test loss: 0.1008. Saving model.
Epoch [16/300], Loss: 0.1200
Epoch [17/300], Loss: 0.1155
Epoch [18/300], Loss: 0.1116
Epoch [19/300], Loss: 0.1083
Epoch [20/300], Loss: 0.1054
-- Evaluation after Epoch [20/300], Test Loss: 0.0804
New best test loss: 0.0804. Saving model.
Epoch [21/300], Loss: 0.1036
Epoch [22/300], Loss: 0.1018
Epoch [23/300], Loss: 0.1005
Epoch [24/300], Loss: 0.0993
Epoch [25/300], Loss: 0.0985
-- Evaluation after Epoch [25/300], Test Loss: 0.0751
New best test loss: 0.0751. Saving model.
Epoch [26/300], Loss: 0.0976
Epoch [27/300], Loss: 0.0975
Epoch [28/300], Loss: 0.0966
Epoch [29/300], Loss: 0.0961
Epoch [30/300], Loss: 0.0958
-- Evaluation after Epoch [30/300], Test Loss: 0.0744
New best test loss: 0.0744. Saving model.
Epoch [31/300], Loss: 0.0952
Epoch [32/300], Loss: 0.0947
Epoch [33/300], Loss: 0.0949
Epoch [34/300], Loss: 0.0945
Epoch [35/300], Loss: 0.0939
-- Evaluation after Epoch [35/300], Test Loss: 0.0741
New best test loss: 0.0741. Saving model.
Epoch [36/300], Loss: 0.0932
Epoch [37/300], Loss: 0.0934
Epoch [38/300], Loss: 0.0927
Epoch [39/300], Loss: 0.0926
Epoch [40/300], Loss: 0.0927
-- Evaluation after Epoch [40/300], Test Loss: 0.0742
Epoch [41/300], Loss: 0.0921
Epoch [42/300], Loss: 0.0917
Epoch [43/300], Loss: 0.0916
Epoch [44/300], Loss: 0.0914
Epoch [45/300], Loss: 0.0912
-- Evaluation after Epoch [45/300], Test Loss: 0.0741
New best test loss: 0.0741. Saving model.
Epoch [46/300], Loss: 0.0912
Epoch [47/300], Loss: 0.0909
Epoch [48/300], Loss: 0.0908
Epoch [49/300], Loss: 0.0906
Epoch [50/300], Loss: 0.0906
-- Evaluation after Epoch [50/300], Test Loss: 0.0740
New best test loss: 0.0740. Saving model.
Epoch [51/300], Loss: 0.0903
Epoch [52/300], Loss: 0.0901
Epoch [53/300], Loss: 0.0900
Epoch [54/300], Loss: 0.0898
Epoch [55/300], Loss: 0.0898
-- Evaluation after Epoch [55/300], Test Loss: 0.0740
Epoch [56/300], Loss: 0.0896
Epoch [57/300], Loss: 0.0895
Epoch [58/300], Loss: 0.0897
Epoch [59/300], Loss: 0.0894
Epoch [60/300], Loss: 0.0894
-- Evaluation after Epoch [60/300], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [61/300], Loss: 0.0894
Epoch [62/300], Loss: 0.0894
Epoch [63/300], Loss: 0.0892
Epoch [64/300], Loss: 0.0891
Epoch [65/300], Loss: 0.0892
-- Evaluation after Epoch [65/300], Test Loss: 0.0740
Epoch [66/300], Loss: 0.0890
Epoch [67/300], Loss: 0.0889
Epoch [68/300], Loss: 0.0890
Epoch [69/300], Loss: 0.0889
Epoch [70/300], Loss: 0.0888
-- Evaluation after Epoch [70/300], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [71/300], Loss: 0.0889
Epoch [72/300], Loss: 0.0888
Epoch [73/300], Loss: 0.0887
Epoch [74/300], Loss: 0.0888
Epoch [75/300], Loss: 0.0887
-- Evaluation after Epoch [75/300], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [76/300], Loss: 0.0887
Epoch [77/300], Loss: 0.0886
Epoch [78/300], Loss: 0.0886
Epoch [79/300], Loss: 0.0887
Epoch [80/300], Loss: 0.0886
-- Evaluation after Epoch [80/300], Test Loss: 0.0739
Epoch [81/300], Loss: 0.0886
Epoch [82/300], Loss: 0.0886
Epoch [83/300], Loss: 0.0886
Epoch [84/300], Loss: 0.0886
Epoch [85/300], Loss: 0.0885
-- Evaluation after Epoch [85/300], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [86/300], Loss: 0.0886
Epoch [87/300], Loss: 0.0885
Epoch [88/300], Loss: 0.0885
Epoch [89/300], Loss: 0.0885
Epoch [90/300], Loss: 0.0885
-- Evaluation after Epoch [90/300], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [91/300], Loss: 0.0885
Epoch [92/300], Loss: 0.0885
Epoch [93/300], Loss: 0.0885
Epoch [94/300], Loss: 0.0885
Epoch [95/300], Loss: 0.0885
-- Evaluation after Epoch [95/300], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [96/300], Loss: 0.0885
Epoch [97/300], Loss: 0.0885
Epoch [98/300], Loss: 0.0884
Epoch [99/300], Loss: 0.0885
Epoch [100/300], Loss: 0.0884
-- Evaluation after Epoch [100/300], Test Loss: 0.0739
New best test loss: 0.0739. Saving model.
Epoch [101/300], Loss: 0.0884
Epoch [102/300], Loss: 0.0884
Epoch [103/300], Loss: 0.0884
Epoch [104/300], Loss: 0.0884
Epoch [105/300], Loss: 0.0885
-- Evaluation after Epoch [105/300], Test Loss: 0.0739
Epoch [106/300], Loss: 0.0884
Epoch [107/300], Loss: 0.0884
Epoch [108/300], Loss: 0.0884
Epoch [109/300], Loss: 0.0884
Epoch [110/300], Loss: 0.0884
-- Evaluation after Epoch [110/300], Test Loss: 0.0739
Epoch [111/300], Loss: 0.0884
Epoch [112/300], Loss: 0.0884
Epoch [113/300], Loss: 0.0884
Epoch [114/300], Loss: 0.0884
Epoch [115/300], Loss: 0.0884
-- Evaluation after Epoch [115/300], Test Loss: 0.0739
Epoch [116/300], Loss: 0.0884
Epoch [117/300], Loss: 0.0884
Epoch [118/300], Loss: 0.0884
Epoch [119/300], Loss: 0.0884
Epoch [120/300], Loss: 0.0884
-- Evaluation after Epoch [120/300], Test Loss: 0.0739
Epoch [121/300], Loss: 0.0884
Epoch [122/300], Loss: 0.0883
Epoch [123/300], Loss: 0.0883
Epoch [124/300], Loss: 0.0883
Epoch [125/300], Loss: 0.0882
-- Evaluation after Epoch [125/300], Test Loss: 0.0736
New best test loss: 0.0736. Saving model.
Epoch [126/300], Loss: 0.0881
Epoch [127/300], Loss: 0.0881
Epoch [128/300], Loss: 0.0881
Epoch [129/300], Loss: 0.0879
Epoch [130/300], Loss: 0.0878
-- Evaluation after Epoch [130/300], Test Loss: 0.0730
New best test loss: 0.0730. Saving model.
Epoch [131/300], Loss: 0.0878
Epoch [132/300], Loss: 0.0877
Epoch [133/300], Loss: 0.0876
Epoch [134/300], Loss: 0.0876
Epoch [135/300], Loss: 0.0876
-- Evaluation after Epoch [135/300], Test Loss: 0.0730
New best test loss: 0.0730. Saving model.
Epoch [136/300], Loss: 0.0875
Epoch [137/300], Loss: 0.0875
Epoch [138/300], Loss: 0.0875
Epoch [139/300], Loss: 0.0875
Epoch [140/300], Loss: 0.0874
-- Evaluation after Epoch [140/300], Test Loss: 0.0726
New best test loss: 0.0726. Saving model.
Epoch [141/300], Loss: 0.0873
Epoch [142/300], Loss: 0.0874
Epoch [143/300], Loss: 0.0873
Epoch [144/300], Loss: 0.0873
Epoch [145/300], Loss: 0.0874
-- Evaluation after Epoch [145/300], Test Loss: 0.0723
New best test loss: 0.0723. Saving model.
Epoch [146/300], Loss: 0.0873
Epoch [147/300], Loss: 0.0872
Epoch [148/300], Loss: 0.0872
Epoch [149/300], Loss: 0.0872
Epoch [150/300], Loss: 0.0872
-- Evaluation after Epoch [150/300], Test Loss: 0.0720
New best test loss: 0.0720. Saving model.
Epoch [151/300], Loss: 0.0872
Epoch [152/300], Loss: 0.0871
Epoch [153/300], Loss: 0.0872
Epoch [154/300], Loss: 0.0871
Epoch [155/300], Loss: 0.0872
-- Evaluation after Epoch [155/300], Test Loss: 0.0721
Epoch [156/300], Loss: 0.0870
Epoch [157/300], Loss: 0.0871
Epoch [158/300], Loss: 0.0872
Epoch [159/300], Loss: 0.0870
Epoch [160/300], Loss: 0.0871
-- Evaluation after Epoch [160/300], Test Loss: 0.0718
New best test loss: 0.0718. Saving model.
Epoch [161/300], Loss: 0.0871
Epoch [162/300], Loss: 0.0870
Epoch [163/300], Loss: 0.0869
Epoch [164/300], Loss: 0.0870
Epoch [165/300], Loss: 0.0869
-- Evaluation after Epoch [165/300], Test Loss: 0.0717
New best test loss: 0.0717. Saving model.
Epoch [166/300], Loss: 0.0869
Epoch [167/300], Loss: 0.0868
Epoch [168/300], Loss: 0.0869
Epoch [169/300], Loss: 0.0869
Epoch [170/300], Loss: 0.0868
-- Evaluation after Epoch [170/300], Test Loss: 0.0717
New best test loss: 0.0717. Saving model.
Epoch [171/300], Loss: 0.0868
Epoch [172/300], Loss: 0.0868
Epoch [173/300], Loss: 0.0868
Epoch [174/300], Loss: 0.0867
Epoch [175/300], Loss: 0.0867
-- Evaluation after Epoch [175/300], Test Loss: 0.0718
Epoch [176/300], Loss: 0.0866
Epoch [177/300], Loss: 0.0866
Epoch [178/300], Loss: 0.0866
Epoch [179/300], Loss: 0.0866
Epoch [180/300], Loss: 0.0866
-- Evaluation after Epoch [180/300], Test Loss: 0.0712
New best test loss: 0.0712. Saving model.
Epoch [181/300], Loss: 0.0865
Epoch [182/300], Loss: 0.0867
Epoch [183/300], Loss: 0.0866
Epoch [184/300], Loss: 0.0865
Epoch [185/300], Loss: 0.0866
-- Evaluation after Epoch [185/300], Test Loss: 0.0716
Epoch [186/300], Loss: 0.0864
Epoch [187/300], Loss: 0.0864
Epoch [188/300], Loss: 0.0864
Epoch [189/300], Loss: 0.0864
Epoch [190/300], Loss: 0.0863
-- Evaluation after Epoch [190/300], Test Loss: 0.0714
Epoch [191/300], Loss: 0.0864
Epoch [192/300], Loss: 0.0863
Epoch [193/300], Loss: 0.0864
Epoch [194/300], Loss: 0.0863
Epoch [195/300], Loss: 0.0863
-- Evaluation after Epoch [195/300], Test Loss: 0.0713
Epoch [196/300], Loss: 0.0861
Epoch [197/300], Loss: 0.0862
Epoch [198/300], Loss: 0.0861
Epoch [199/300], Loss: 0.0863
Epoch [200/300], Loss: 0.0861
-- Evaluation after Epoch [200/300], Test Loss: 0.0710
New best test loss: 0.0710. Saving model.
Epoch [201/300], Loss: 0.0859
Epoch [202/300], Loss: 0.0860
Epoch [203/300], Loss: 0.0859
Epoch [204/300], Loss: 0.0861
Epoch [205/300], Loss: 0.0859
-- Evaluation after Epoch [205/300], Test Loss: 0.0714
Epoch [206/300], Loss: 0.0859
Epoch [207/300], Loss: 0.0857
Epoch [208/300], Loss: 0.0857
Epoch [209/300], Loss: 0.0857
Epoch [210/300], Loss: 0.0858
-- Evaluation after Epoch [210/300], Test Loss: 0.0707
New best test loss: 0.0707. Saving model.
Epoch [211/300], Loss: 0.0855
Epoch [212/300], Loss: 0.0853
Epoch [213/300], Loss: 0.0856
Epoch [214/300], Loss: 0.0855
Epoch [215/300], Loss: 0.0851
-- Evaluation after Epoch [215/300], Test Loss: 0.0703
New best test loss: 0.0703. Saving model.
Epoch [216/300], Loss: 0.0853
Epoch [217/300], Loss: 0.0855
Epoch [218/300], Loss: 0.0853
Epoch [219/300], Loss: 0.0850
Epoch [220/300], Loss: 0.0852
-- Evaluation after Epoch [220/300], Test Loss: 0.0701
New best test loss: 0.0701. Saving model.
Epoch [221/300], Loss: 0.0853
Epoch [222/300], Loss: 0.0849
Epoch [223/300], Loss: 0.0848
Epoch [224/300], Loss: 0.0848
Epoch [225/300], Loss: 0.0849
-- Evaluation after Epoch [225/300], Test Loss: 0.0705
Epoch [226/300], Loss: 0.0847
Epoch [227/300], Loss: 0.0848
Epoch [228/300], Loss: 0.0850
Epoch [229/300], Loss: 0.0847
Epoch [230/300], Loss: 0.0845
-- Evaluation after Epoch [230/300], Test Loss: 0.0696
New best test loss: 0.0696. Saving model.
Epoch [231/300], Loss: 0.0847
Epoch [232/300], Loss: 0.0849
Epoch [233/300], Loss: 0.0844
Epoch [234/300], Loss: 0.0845
Epoch [235/300], Loss: 0.0845
-- Evaluation after Epoch [235/300], Test Loss: 0.0703
Epoch [236/300], Loss: 0.0843
Epoch [237/300], Loss: 0.0844
Epoch [238/300], Loss: 0.0845
Epoch [239/300], Loss: 0.0842
Epoch [240/300], Loss: 0.0843
-- Evaluation after Epoch [240/300], Test Loss: 0.0706
Epoch [241/300], Loss: 0.0841
Epoch [242/300], Loss: 0.0843
Epoch [243/300], Loss: 0.0839
Epoch [244/300], Loss: 0.0844
Epoch [245/300], Loss: 0.0840
-- Evaluation after Epoch [245/300], Test Loss: 0.0701
Epoch [246/300], Loss: 0.0837
Epoch [247/300], Loss: 0.0837
Epoch [248/300], Loss: 0.0835
Epoch [249/300], Loss: 0.0836
Epoch [250/300], Loss: 0.0837
-- Evaluation after Epoch [250/300], Test Loss: 0.0700
Epoch [251/300], Loss: 0.0838
Epoch [252/300], Loss: 0.0832
Epoch [253/300], Loss: 0.0835
Epoch [254/300], Loss: 0.0837
Epoch [255/300], Loss: 0.0842
-- Evaluation after Epoch [255/300], Test Loss: 0.0698
Epoch [256/300], Loss: 0.0835
Epoch [257/300], Loss: 0.0837
Epoch [258/300], Loss: 0.0833
Epoch [259/300], Loss: 0.0836
Epoch [260/300], Loss: 0.0835
-- Evaluation after Epoch [260/300], Test Loss: 0.0699
Epoch [261/300], Loss: 0.0835
Epoch [262/300], Loss: 0.0835
Epoch [263/300], Loss: 0.0831
Epoch [264/300], Loss: 0.0834
Epoch [265/300], Loss: 0.0833
-- Evaluation after Epoch [265/300], Test Loss: 0.0689
New best test loss: 0.0689. Saving model.
Epoch [266/300], Loss: 0.0831
Epoch [267/300], Loss: 0.0830
Epoch [268/300], Loss: 0.0829
Epoch [269/300], Loss: 0.0831
Epoch [270/300], Loss: 0.0829
-- Evaluation after Epoch [270/300], Test Loss: 0.0694
Epoch [271/300], Loss: 0.0830
Epoch [272/300], Loss: 0.0830
Epoch [273/300], Loss: 0.0832
Epoch [274/300], Loss: 0.0825
Epoch [275/300], Loss: 0.0832
-- Evaluation after Epoch [275/300], Test Loss: 0.0699
Epoch [276/300], Loss: 0.0829
Epoch [277/300], Loss: 0.0827
Epoch [278/300], Loss: 0.0829
Epoch [279/300], Loss: 0.0825
Epoch [280/300], Loss: 0.0822
-- Evaluation after Epoch [280/300], Test Loss: 0.0698
Epoch [281/300], Loss: 0.0825
Epoch [282/300], Loss: 0.0825
Epoch [283/300], Loss: 0.0829
Epoch [284/300], Loss: 0.0828
Epoch [285/300], Loss: 0.0827
-- Evaluation after Epoch [285/300], Test Loss: 0.0700
Epoch [286/300], Loss: 0.0823
Epoch [287/300], Loss: 0.0824
Epoch [288/300], Loss: 0.0824
Epoch [289/300], Loss: 0.0823
Epoch [290/300], Loss: 0.0821
-- Evaluation after Epoch [290/300], Test Loss: 0.0690
Epoch [291/300], Loss: 0.0825
Epoch [292/300], Loss: 0.0824
Epoch [293/300], Loss: 0.0822
Epoch [294/300], Loss: 0.0823
Epoch [295/300], Loss: 0.0829
-- Evaluation after Epoch [295/300], Test Loss: 0.0698
Epoch [296/300], Loss: 0.0817
Epoch [297/300], Loss: 0.0817
Epoch [298/300], Loss: 0.0826
Epoch [299/300], Loss: 0.0819
Epoch [300/300], Loss: 0.0820
-- Evaluation after Epoch [300/300], Test Loss: 0.0707
Loading the best model from checkpoint with test loss: 0.0689
Validation Loss (RMSE): 0.0689
Validation results saved to: ./Results\LSTM_Study_20250129_165103\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0689
Avg Test Loss after reverse scaling (RMSE): 55.0647
Test results saved to: ./Results\LSTM_Study_20250129_165103\Test\test_results.txt
