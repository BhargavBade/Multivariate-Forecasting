Epoch [1/200], Loss: 0.3691
Epoch [2/200], Loss: 0.1148
Epoch [3/200], Loss: 0.1047
Epoch [4/200], Loss: 0.0937
Epoch [5/200], Loss: 0.0933
New best training loss: 0.0933. Saving model based on training loss.
-- Evaluation after Epoch [5/200], Test Loss: 0.0806
Epoch [6/200], Loss: 0.1147
Epoch [7/200], Loss: 0.1444
Epoch [8/200], Loss: 0.1072
Epoch [9/200], Loss: 0.1577
Epoch [10/200], Loss: 0.2004
-- Evaluation after Epoch [10/200], Test Loss: 0.1261
Epoch [11/200], Loss: 0.1064
Epoch [12/200], Loss: 0.1373
Epoch [13/200], Loss: 0.1274
Epoch [14/200], Loss: 0.1263
Epoch [15/200], Loss: 0.1163
-- Evaluation after Epoch [15/200], Test Loss: 0.0891
Epoch [16/200], Loss: 0.1238
Epoch [17/200], Loss: 0.1140
Epoch [18/200], Loss: 0.0848
Epoch [19/200], Loss: 0.0804
Epoch [20/200], Loss: 0.0828
New best training loss: 0.0828. Saving model based on training loss.
-- Evaluation after Epoch [20/200], Test Loss: 0.0978
New best test loss: 0.0978. Saving model.
Epoch [21/200], Loss: 0.0880
Epoch [22/200], Loss: 0.1071
Epoch [23/200], Loss: 0.0834
Epoch [24/200], Loss: 0.0841
Epoch [25/200], Loss: 0.0790
New best training loss: 0.0790. Saving model based on training loss.
-- Evaluation after Epoch [25/200], Test Loss: 0.1143
Epoch [26/200], Loss: 0.0780
Epoch [27/200], Loss: 0.1063
Epoch [28/200], Loss: 0.0810
Epoch [29/200], Loss: 0.0741
Epoch [30/200], Loss: 0.0712
New best training loss: 0.0712. Saving model based on training loss.
-- Evaluation after Epoch [30/200], Test Loss: 0.0806
New best test loss: 0.0806. Saving model.
Epoch [31/200], Loss: 0.0696
Epoch [32/200], Loss: 0.0708
Epoch [33/200], Loss: 0.0704
Epoch [34/200], Loss: 0.0754
Epoch [35/200], Loss: 0.0857
-- Evaluation after Epoch [35/200], Test Loss: 0.0947
Epoch [36/200], Loss: 0.0704
Epoch [37/200], Loss: 0.0723
Epoch [38/200], Loss: 0.0916
Epoch [39/200], Loss: 0.0780
Epoch [40/200], Loss: 0.0758
-- Evaluation after Epoch [40/200], Test Loss: 0.1663
Epoch [41/200], Loss: 0.1022
Epoch [42/200], Loss: 0.0952
Epoch [43/200], Loss: 0.0851
Epoch [44/200], Loss: 0.0827
Epoch [45/200], Loss: 0.0782
-- Evaluation after Epoch [45/200], Test Loss: 0.1032
Epoch [46/200], Loss: 0.0786
Epoch [47/200], Loss: 0.0752
Epoch [48/200], Loss: 0.0702
Epoch [49/200], Loss: 0.0692
Epoch [50/200], Loss: 0.0832
-- Evaluation after Epoch [50/200], Test Loss: 0.0843
Epoch [51/200], Loss: 0.0737
Epoch [52/200], Loss: 0.0674
Epoch [53/200], Loss: 0.0670
Epoch [54/200], Loss: 0.0706
Epoch [55/200], Loss: 0.0699
New best training loss: 0.0699. Saving model based on training loss.
-- Evaluation after Epoch [55/200], Test Loss: 0.0827
Epoch [56/200], Loss: 0.0695
Epoch [57/200], Loss: 0.0654
Epoch [58/200], Loss: 0.0660
Epoch [59/200], Loss: 0.0618
Epoch [60/200], Loss: 0.0610
New best training loss: 0.0610. Saving model based on training loss.
-- Evaluation after Epoch [60/200], Test Loss: 0.0758
New best test loss: 0.0758. Saving model.
Epoch [61/200], Loss: 0.0604
Epoch [62/200], Loss: 0.0582
Epoch [63/200], Loss: 0.0601
Epoch [64/200], Loss: 0.0677
Epoch [65/200], Loss: 0.0607
New best training loss: 0.0607. Saving model based on training loss.
-- Evaluation after Epoch [65/200], Test Loss: 0.0817
Epoch [66/200], Loss: 0.0606
Epoch [67/200], Loss: 0.0611
Epoch [68/200], Loss: 0.0657
Epoch [69/200], Loss: 0.0665
Epoch [70/200], Loss: 0.0742
-- Evaluation after Epoch [70/200], Test Loss: 0.0799
Epoch [71/200], Loss: 0.0693
Epoch [72/200], Loss: 0.0753
Epoch [73/200], Loss: 0.0705
Epoch [74/200], Loss: 0.0611
Epoch [75/200], Loss: 0.0577
New best training loss: 0.0577. Saving model based on training loss.
-- Evaluation after Epoch [75/200], Test Loss: 0.0845
Epoch [76/200], Loss: 0.0585
Epoch [77/200], Loss: 0.0625
Epoch [78/200], Loss: 0.0616
Epoch [79/200], Loss: 0.0641
Epoch [80/200], Loss: 0.0699
-- Evaluation after Epoch [80/200], Test Loss: 0.0813
Epoch [81/200], Loss: 0.0597
Epoch [82/200], Loss: 0.0569
Epoch [83/200], Loss: 0.0575
Epoch [84/200], Loss: 0.0638
Epoch [85/200], Loss: 0.0641
-- Evaluation after Epoch [85/200], Test Loss: 0.0762
Epoch [86/200], Loss: 0.0658
Epoch [87/200], Loss: 0.0646
Epoch [88/200], Loss: 0.0663
Epoch [89/200], Loss: 0.0559
Epoch [90/200], Loss: 0.0580
-- Evaluation after Epoch [90/200], Test Loss: 0.0886
Epoch [91/200], Loss: 0.0635
Epoch [92/200], Loss: 0.0628
Epoch [93/200], Loss: 0.0639
Epoch [94/200], Loss: 0.0618
Epoch [95/200], Loss: 0.0572
New best training loss: 0.0572. Saving model based on training loss.
-- Evaluation after Epoch [95/200], Test Loss: 0.0894
Epoch [96/200], Loss: 0.0619
Epoch [97/200], Loss: 0.0643
Epoch [98/200], Loss: 0.0642
Epoch [99/200], Loss: 0.0582
Epoch [100/200], Loss: 0.0561
New best training loss: 0.0561. Saving model based on training loss.
-- Evaluation after Epoch [100/200], Test Loss: 0.0858
Epoch [101/200], Loss: 0.0589
Epoch [102/200], Loss: 0.0572
Epoch [103/200], Loss: 0.0629
Epoch [104/200], Loss: 0.0592
Epoch [105/200], Loss: 0.0608
-- Evaluation after Epoch [105/200], Test Loss: 0.0889
Epoch [106/200], Loss: 0.0590
Epoch [107/200], Loss: 0.0640
Epoch [108/200], Loss: 0.0574
Epoch [109/200], Loss: 0.0580
Epoch [110/200], Loss: 0.0563
-- Evaluation after Epoch [110/200], Test Loss: 0.0972
Epoch [111/200], Loss: 0.0552
Epoch [112/200], Loss: 0.0603
Epoch [113/200], Loss: 0.0574
Epoch [114/200], Loss: 0.0581
Epoch [115/200], Loss: 0.0540
New best training loss: 0.0540. Saving model based on training loss.
-- Evaluation after Epoch [115/200], Test Loss: 0.0977
Epoch [116/200], Loss: 0.0577
Epoch [117/200], Loss: 0.0548
Epoch [118/200], Loss: 0.0557
Epoch [119/200], Loss: 0.0570
Epoch [120/200], Loss: 0.0539
New best training loss: 0.0539. Saving model based on training loss.
-- Evaluation after Epoch [120/200], Test Loss: 0.0854
Epoch [121/200], Loss: 0.0610
Epoch [122/200], Loss: 0.0573
Epoch [123/200], Loss: 0.0542
Epoch [124/200], Loss: 0.0544
Epoch [125/200], Loss: 0.0509
New best training loss: 0.0509. Saving model based on training loss.
-- Evaluation after Epoch [125/200], Test Loss: 0.0801
Epoch [126/200], Loss: 0.0562
Epoch [127/200], Loss: 0.0503
Epoch [128/200], Loss: 0.0532
Epoch [129/200], Loss: 0.0500
Epoch [130/200], Loss: 0.0537
-- Evaluation after Epoch [130/200], Test Loss: 0.1191
Epoch [131/200], Loss: 0.0512
Epoch [132/200], Loss: 0.0544
Epoch [133/200], Loss: 0.0499
Epoch [134/200], Loss: 0.0526
Epoch [135/200], Loss: 0.0491
New best training loss: 0.0491. Saving model based on training loss.
-- Evaluation after Epoch [135/200], Test Loss: 0.0829
Epoch [136/200], Loss: 0.0485
Epoch [137/200], Loss: 0.0517
Epoch [138/200], Loss: 0.0482
Epoch [139/200], Loss: 0.0482
Epoch [140/200], Loss: 0.0487
New best training loss: 0.0487. Saving model based on training loss.
-- Evaluation after Epoch [140/200], Test Loss: 0.0868
Epoch [141/200], Loss: 0.0533
Epoch [142/200], Loss: 0.0538
Epoch [143/200], Loss: 0.0521
Epoch [144/200], Loss: 0.0513
Epoch [145/200], Loss: 0.0587
-- Evaluation after Epoch [145/200], Test Loss: 0.0801
Epoch [146/200], Loss: 0.0487
Epoch [147/200], Loss: 0.0515
Epoch [148/200], Loss: 0.0528
Epoch [149/200], Loss: 0.0495
Epoch [150/200], Loss: 0.0490
-- Evaluation after Epoch [150/200], Test Loss: 0.0961
Epoch [151/200], Loss: 0.0512
Epoch [152/200], Loss: 0.0506
Epoch [153/200], Loss: 0.0473
Epoch [154/200], Loss: 0.0519
Epoch [155/200], Loss: 0.0470
New best training loss: 0.0470. Saving model based on training loss.
-- Evaluation after Epoch [155/200], Test Loss: 0.0787
Epoch [156/200], Loss: 0.0505
Epoch [157/200], Loss: 0.0516
Epoch [158/200], Loss: 0.0477
Epoch [159/200], Loss: 0.0445
Epoch [160/200], Loss: 0.0468
New best training loss: 0.0468. Saving model based on training loss.
-- Evaluation after Epoch [160/200], Test Loss: 0.0813
Epoch [161/200], Loss: 0.0446
Epoch [162/200], Loss: 0.0445
Epoch [163/200], Loss: 0.0458
Epoch [164/200], Loss: 0.0437
Epoch [165/200], Loss: 0.0438
New best training loss: 0.0438. Saving model based on training loss.
-- Evaluation after Epoch [165/200], Test Loss: 0.0843
Epoch [166/200], Loss: 0.0430
Epoch [167/200], Loss: 0.0425
Epoch [168/200], Loss: 0.0418
Epoch [169/200], Loss: 0.0464
Epoch [170/200], Loss: 0.0444
-- Evaluation after Epoch [170/200], Test Loss: 0.0794
Epoch [171/200], Loss: 0.0419
Epoch [172/200], Loss: 0.0456
Epoch [173/200], Loss: 0.0435
Epoch [174/200], Loss: 0.0459
Epoch [175/200], Loss: 0.0418
New best training loss: 0.0418. Saving model based on training loss.
-- Evaluation after Epoch [175/200], Test Loss: 0.0887
Epoch [176/200], Loss: 0.0415
Epoch [177/200], Loss: 0.0424
Epoch [178/200], Loss: 0.0429
Epoch [179/200], Loss: 0.0419
Epoch [180/200], Loss: 0.0428
-- Evaluation after Epoch [180/200], Test Loss: 0.0901
Epoch [181/200], Loss: 0.0426
Epoch [182/200], Loss: 0.0398
Epoch [183/200], Loss: 0.0422
Epoch [184/200], Loss: 0.0429
Epoch [185/200], Loss: 0.0424
-- Evaluation after Epoch [185/200], Test Loss: 0.0872
Epoch [186/200], Loss: 0.0410
Epoch [187/200], Loss: 0.0432
Epoch [188/200], Loss: 0.0396
Epoch [189/200], Loss: 0.0387
Epoch [190/200], Loss: 0.0424
-- Evaluation after Epoch [190/200], Test Loss: 0.0839
Epoch [191/200], Loss: 0.0416
Epoch [192/200], Loss: 0.0419
Epoch [193/200], Loss: 0.0395
Epoch [194/200], Loss: 0.0418
Epoch [195/200], Loss: 0.0419
-- Evaluation after Epoch [195/200], Test Loss: 0.0752
New best test loss: 0.0752. Saving model.
Epoch [196/200], Loss: 0.0391
Epoch [197/200], Loss: 0.0412
Epoch [198/200], Loss: 0.0411
Epoch [199/200], Loss: 0.0418
Epoch [200/200], Loss: 0.0445
-- Evaluation after Epoch [200/200], Test Loss: 0.1273
Loading the best model from checkpoint with train loss: 0.0418
Validation Loss (RMSE): 0.0889
Avg Test Loss after val reverse scaling (RMSE): 67.3858
Validation results saved to: ./Results\Informer_Study_20250203_125700\Val\validation_results.txt
Avg Test Loss before reverse scaling(RMSE): 0.0752
Avg Test Loss after reverse scaling (RMSE): 55.1690
Test results saved to: ./Results\Informer_Study_20250203_125700\Test\test_results.txt
