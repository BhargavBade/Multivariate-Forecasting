{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab1c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63392a7c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from Network.lstm_network import LSTMModel\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eedc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('.')  # Add the parent directory to the Python path\n",
    "import params\n",
    "importlib.reload(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05382293",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(params.window_size)\n",
    "print(params.n_steps_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221801b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "print(params.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc63471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Study Folder creation for saving Results and Plots\n",
    "sys.path.append('./Miscll') \n",
    "import study_folder\n",
    "importlib.reload(study_folder)\n",
    "study_folder, train_folder, test_folder, val_folder = study_folder.create_study_folder()\n",
    "print(study_folder)\n",
    "print(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#saving the params file used for this study in this study folder\n",
    "\n",
    "# Path to the config.py file in your project directory\n",
    "config_file_path = './params.py'\n",
    "\n",
    "# Destination where the config.py file will be copied (inside the study folder)\n",
    "config_destination_path = os.path.join(study_folder, 'params.py')\n",
    "\n",
    "# Copy the config.py file to the study folder\n",
    "shutil.copy(config_file_path, config_destination_path)\n",
    "\n",
    "print(f'Config file saved to: {config_destination_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83257585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('./Data')  # Add the current directory to Python path\n",
    "import prepare_data01\n",
    "importlib.reload(prepare_data01)\n",
    "from prepare_data01 import DataPreparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edef7c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an instance of DataPreparer\n",
    "data_preparer = DataPreparer(data_dir='./01_PM2.5 Chinese Weather data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc39598",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data (loads, cleans, splits, and creates tensors)\n",
    "\n",
    "data_preparer.prepare_data()\n",
    "\n",
    "# Get the tensors\n",
    "(train_data_tensor, train_labels_tensor, val_data_tensor, val_labels_tensor, past_pm25_val, val_pm_dt,\n",
    " test_data_tensor, test_labels_tensor, past_pm25_test, test_pm_dt, scaler, pm_index) = data_preparer.get_tensors()\n",
    "\n",
    "\n",
    "print('PM index during scaling is:',pm_index)\n",
    "# Now you can use these tensors for training in your notebook\n",
    "print(\"Train data tensor shape:\", train_data_tensor.shape)\n",
    "print(\"Train labels tensor shape:\", train_labels_tensor.shape)\n",
    "\n",
    "print(\"Val data tensor shape:\", val_data_tensor.shape)\n",
    "print(\"Val labels tensor shape:\", val_labels_tensor.shape)\n",
    "\n",
    "print(\"Test data tensor shape:\", test_data_tensor.shape)\n",
    "print(\"Test labels tensor shape:\", test_labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2a5ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the datasets and dataloaders\n",
    "\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.to(torch.float32)\n",
    "        self.labels = labels.to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Train Data and Train Labels\n",
    "X_train = train_data_tensor.to(torch.float32)\n",
    "y_train = train_labels_tensor.to(torch.float32)\n",
    "\n",
    "# Train Dataset and Train Dataloader\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=params.batch_size, \n",
    "                                           shuffle = False, drop_last=True)\n",
    "\n",
    "# Val Data and Val Labels\n",
    "X_val = val_data_tensor.to(torch.float32)\n",
    "y_val = val_labels_tensor.to(torch.float32)\n",
    "\n",
    "# Val Dataset and Val Dataloader\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size = params.batch_size, \n",
    "                                         shuffle=False, drop_last=True)\n",
    "\n",
    "# Test Data and Test Labels\n",
    "X_test = test_data_tensor.to(torch.float32)\n",
    "y_test = test_labels_tensor.to(torch.float32)\n",
    "\n",
    "# Test Dataset and Test Dataloader\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=params.batch_size, \n",
    "                                          shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model, loss function, and optimizer\n",
    "\n",
    "input_size = params.input_size  # number of features\n",
    "hidden_size = params.hidden_size  # number of hidden units in LSTM\n",
    "output_size = params.output_size  # output size (1 for regression, could be different for classification)\n",
    "num_layers = params.num_layers  # number of LSTM layers\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loss and optimizer\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss() # Mean Absolute Error (MAE)\n",
    "optimizer = optim.Adam(model.parameters(), lr = params.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272bd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Storing best model based on test loss    \n",
    "best_test_loss = float('inf')  # Initialize best test loss as infinity\n",
    "checkpoint_path = os.path.join(train_folder, 'best_model.pth')  # Path to save the best model    \n",
    "\n",
    "# Path to save the training results\n",
    "train_results_file = os.path.join(train_folder, 'best_test_loss.txt')\n",
    "            \n",
    "####################################################################################################\n",
    "\n",
    "#Train the model\n",
    "\n",
    "num_epochs = params.num_epochs\n",
    "\n",
    "# Initialize list to store loss values\n",
    "loss_values = []\n",
    "test_losses = []   # Store test loss for each 5th epoch\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to('cuda' if torch.cuda.is_available() else 'cpu'), labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute the loss (squeeze the outputs so they match the shape of labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    loss_values.append(avg_loss)  # Store average loss for this epoch\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # -------- Testing Phase (every 5 epochs) --------\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        running_test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculations for testing\n",
    "            for data, labels in test_loader:\n",
    "                data, labels = data.to('cuda' if torch.cuda.is_available() else 'cpu'), labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_test_loss += loss.item()\n",
    "\n",
    "        # Compute average test loss for the epoch\n",
    "        avg_test_loss = running_test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        # Print test loss for the current evaluation\n",
    "        print(f'-- Evaluation after Epoch [{epoch + 1}/{num_epochs}], Test Loss: {avg_test_loss:.4f}')\n",
    "        \n",
    "        # Check if this is the best test loss so far\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            print(f'New best test loss: {best_test_loss:.4f}. Saving model.')\n",
    "            torch.save(model.state_dict(), checkpoint_path)  # Save model\n",
    "        \n",
    "        # Write the validation loss to a text file\n",
    "        with open(train_results_file, 'w') as f:\n",
    "            f.write(f'Best test Loss (MSE) obtained: {best_test_loss:.4f}\\n')\n",
    "        \n",
    "        \n",
    "        model.train()  # Switch back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a373775",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Plot the loss curve after training\n",
    "\n",
    "lossfig_path = os.path.join(train_folder, 'loss_curve.png')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), loss_values, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(lossfig_path) \n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931003c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Valdiation Set Evaluation\n",
    "\n",
    "# Load the best model after training\n",
    "print(f\"Loading the best model from checkpoint with test loss: {best_test_loss:.4f}\")\n",
    "model.load_state_dict(torch.load(checkpoint_path, weights_only = True))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "val_predictions = []\n",
    "val_actual_labels = []\n",
    "\n",
    "# Initialize total loss for calculating MSE over the validation set\n",
    "total_val_loss = 0.0\n",
    "\n",
    "# Path to save the validation results\n",
    "val_results_file = os.path.join(val_folder, 'validation_results.txt')\n",
    "\n",
    "# Make predictions on the validation dataset using the DataLoader\n",
    "with torch.no_grad():\n",
    "    for data, labels in val_loader:\n",
    "        data, labels = data.to('cuda' if torch.cuda.is_available() else 'cpu'), labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        predictions = model(data)\n",
    "        \n",
    "        # Compute the validation loss (MSE)\n",
    "        #val_loss = criterion(predictions.squeeze(-1), labels)\n",
    "        val_loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Accumulate the total validation loss\n",
    "        total_val_loss += val_loss.item()\n",
    "        \n",
    "        # Store predictions and actual labels for later analysis or metrics calculation\n",
    "        val_predictions.append(predictions.cpu().numpy())  # Move predictions to CPU and store\n",
    "        val_actual_labels.append(labels.cpu().numpy())     # Move labels to CPU and store\n",
    "\n",
    "# Calculate the average MSE over the entire validation set\n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "print(f'Validation Loss (MSE): {avg_val_loss:.4f}')\n",
    "\n",
    "# Concatenate all predictions and actual labels into a single array\n",
    "val_predictions = np.concatenate(val_predictions, axis=0)\n",
    "val_actual_labels = np.concatenate(val_actual_labels, axis=0)\n",
    "\n",
    "# Write the validation loss to a text file\n",
    "with open(val_results_file, 'w') as f:\n",
    "    f.write(f'Validation Loss (MSE): {avg_val_loss:.4f}\\n')\n",
    "    f.write(f'Validation Predictions Shape: {val_predictions.shape}\\n')\n",
    "    f.write(f'Validation Actual Labels Shape: {val_actual_labels.shape}\\n')\n",
    "\n",
    "print(f'Validation results saved to: {val_results_file}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the Validation Predictions vs Actual PM2.5 Values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(val_actual_labels, label='Val Actual PM2.5', color='blue', alpha=0.7)\n",
    "plt.plot(val_predictions, label='Val Predicted PM2.5', color='orange', alpha=0.7)\n",
    "plt.title('Val Predicted vs Val Actual PM2.5 Values')\n",
    "plt.xlabel('Date-Time')\n",
    "plt.ylabel('PM2.5 Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to the 'val_folder' with an appropriate file name\n",
    "plot_filename = os.path.join(val_folder, 'val_pred_plot.png')\n",
    "plt.savefig(plot_filename, dpi=600)  # Save figure\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Close the current figure after displaying to avoid overlap\n",
    "plt.close()\n",
    "\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "\n",
    "#TESTING\n",
    "\n",
    "#Evaluation of the TESTING Set\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "test_predictions = []\n",
    "test_actual_labels = []\n",
    "\n",
    "# Initialize total loss for calculating MSE over the validation set\n",
    "total_test_loss = 0.0\n",
    "\n",
    "# Path to save the validation results\n",
    "test_results_file = os.path.join(test_folder, 'test_results.txt')\n",
    "\n",
    "# Make predictions on the validation dataset using the DataLoader\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        #data_tensor, _ = data  # We only need the features for prediction\n",
    "        #predictions = model(data_tensor.to('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        data, labels = data.to('cuda' if torch.cuda.is_available() else 'cpu'), labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        predictions = model(data)\n",
    "        \n",
    "        # Compute the validation loss (MSE)\n",
    "        #val_loss = criterion(predictions.squeeze(-1), labels)\n",
    "        test_loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Accumulate the total validation loss\n",
    "        total_test_loss += test_loss.item()\n",
    "        \n",
    "        # Store predictions and actual labels for later analysis or metrics calculation\n",
    "        test_predictions.append(predictions.cpu().numpy())  # Move predictions to CPU and store\n",
    "        test_actual_labels.append(labels.cpu().numpy())     # Move labels to CPU and store\n",
    "\n",
    "# Calculate the average MSE over the entire validation set\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "print(f'Avg Test Loss (MSE): {avg_test_loss:.4f}')\n",
    "\n",
    "# Concatenate all predictions and actual labels into a single array\n",
    "test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "test_actual_labels = np.concatenate(test_actual_labels, axis=0)\n",
    "\n",
    "# Write the validation loss to a text file\n",
    "with open(test_results_file, 'w') as f:\n",
    "    f.write(f'Test Loss (MSE): {avg_val_loss:.4f}\\n')\n",
    "    f.write(f'Test Predictions Shape: {val_predictions.shape}\\n')\n",
    "    f.write(f'Test Actual Labels Shape: {val_actual_labels.shape}\\n')\n",
    "\n",
    "print(f'Test results saved to: {test_results_file}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02928e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotting All test predictions at a time\n",
    "# Plotting predicted vs actual values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_actual_labels, label='Test Actual PM2.5', color='blue', alpha=0.7)\n",
    "plt.plot(test_predictions, label='Test Predicted PM2.5', color='orange', alpha=0.7)\n",
    "plt.title('Test Predicted vs Test Actual PM2.5 Values')\n",
    "plt.xlabel('Date-Time')\n",
    "plt.ylabel('PM2.5 Value')\n",
    "plt.legend()\n",
    "plt.grid(True)   \n",
    "\n",
    "# Save the plot to the 'val_folder' with an appropriate file name\n",
    "plot_filename = os.path.join(test_folder, 'Test_pred_plot.png')\n",
    "plt.savefig(plot_filename, dpi=600) # Save figure\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Close the current figure after displaying to avoid overlap\n",
    "plt.close()\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "# Plotting TEST SET predicitions as sequences for SINGLE-STEP predictor.\n",
    "\n",
    "\n",
    "# Select 5 random sequences of length 100\n",
    "sequence_length = 500\n",
    "num_sequences = 10\n",
    "total_length = len(test_predictions)\n",
    "\n",
    "# Make sure we can select sequences within the available length\n",
    "if total_length >= sequence_length:\n",
    "    random_indices = random.sample(range(total_length - sequence_length), num_sequences)\n",
    "else:\n",
    "    raise ValueError(\"Not enough data points for the requested sequence length.\")\n",
    "\n",
    "# Plot 5 random sequences of length 100\n",
    "for i, start_idx in enumerate(random_indices):\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    end_idx = start_idx + sequence_length\n",
    "    \n",
    "    plt.plot(range(sequence_length), test_actual_labels[start_idx:end_idx],\n",
    "              label=f'Actual PM2.5 (Seq {i+1})', alpha=0.7)\n",
    "    plt.plot(range(sequence_length), test_predictions[start_idx:end_idx],\n",
    "              label=f'Predicted PM2.5 (Seq {i+1})', alpha=0.7)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'Sequence {i+1}: Predicted vs Actual PM2.5 Values')\n",
    "    plt.xlabel('Date-time')\n",
    "    plt.ylabel('PM2.5 Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save each plot with a different filename\n",
    "    plot_filename = os.path.join(test_folder, f'predicted_vs_actual_sequence_{i+1}.png')\n",
    "    plt.savefig(plot_filename)\n",
    "    \n",
    "    plt.show()\n",
    "    # Close the plot to avoid overlapping figures\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (pyda3.9)",
   "language": "python",
   "name": "pyda3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
